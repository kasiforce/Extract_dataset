[
  {
    "id": "2602.02361",
    "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
    "abstract": "We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.",
    "arxiv_url": "https://arxiv.org/abs/2602.02361",
    "authors": [
      "Mouxiang Chen",
      "Lei Zhang",
      "Yunlong Feng",
      "Xuwu Wang",
      "Wenting Zhao",
      "Ruisheng Cao",
      "Jiaxi Yang",
      "Jiawei Chen",
      "Mingze Li",
      "Zeyao Ma",
      "Hao Ge",
      "Zongmeng Zhang",
      "Zeyu Cui",
      "Dayiheng Liu",
      "Jingren Zhou",
      "Jianling Sun",
      "Junyang Lin",
      "Binyuan Hui"
    ],
    "first_author": "Mouxiang Chen",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.02361v1",
    "published": "2026-02-02",
    "update_time": "2026-02-02",
    "download_time": "2026-02-04 01:10:24"
  },
  {
    "id": "2602.03806",
    "title": "Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation",
    "abstract": "Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.",
    "arxiv_url": "https://arxiv.org/abs/2602.03806",
    "authors": [
      "Ziru Chen",
      "Dongdong Chen",
      "Ruinan Jin",
      "Yingbin Liang",
      "Yujia Xie",
      "Huan Sun"
    ],
    "first_author": "Ziru Chen",
    "primary_category": "cs.LG",
    "tag": [
      "Code Reinforcement Learning"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.03806v1",
    "published": "2026-02-03",
    "update_time": "2026-02-03",
    "download_time": "2026-02-05 01:10:34"
  },
  {
    "id": "2602.04726",
    "title": "Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation",
    "abstract": "The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tasks. First, we developed a solution for automatic test scenario generation from a detailed requirements description. This approach relies on specialized worker agents forming a star topology with the supervisor agent in the middle. We demonstrate its capabilities on a real-world example. Second, we developed an agentic AI solution for the document retrieval task in the context of software engineering documents. Our solution enables performing various use cases on a body of documents related to the development of a single software, including search, question answering, tracking changes, and large document summarization. In this case, each use case is handled by a dedicated LLM-based agent, which performs all subtasks related to the corresponding use case. We conclude by hinting at the future perspectives of our line of research.",
    "arxiv_url": "https://arxiv.org/abs/2602.04726",
    "authors": [
      "Marian Kica",
      "Lukas Radosky",
      "David Slivka",
      "Karin Kubinova",
      "Daniel Dovhun",
      "Tomas Uhercik",
      "Erik Bircak",
      "Ivan Polasek"
    ],
    "first_author": "Marian Kica",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": "International Conference on Artificial Intelligence",
    "pdf_url": "https://arxiv.org/pdf/2602.04726v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-06 01:10:48"
  },
  {
    "id": "2602.05891",
    "title": "When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models",
    "abstract": "As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.",
    "arxiv_url": "https://arxiv.org/abs/2602.05891",
    "authors": [
      "Shenyu Zheng",
      "Ximing Dong",
      "Xiaoshuang Liu",
      "Gustavo Oliva",
      "Chong Chun Yong",
      "Dayi Lin",
      "Boyuan Chen",
      "Shaowei Wang",
      "Ahmed E. Hassan"
    ],
    "first_author": "Shenyu Zheng",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.05891v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-08 01:45:38"
  }
]