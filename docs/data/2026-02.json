[
  {
    "id": "2602.02361",
    "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
    "abstract": "We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.",
    "arxiv_url": "https://arxiv.org/abs/2602.02361",
    "authors": [
      "Mouxiang Chen",
      "Lei Zhang",
      "Yunlong Feng",
      "Xuwu Wang",
      "Wenting Zhao",
      "Ruisheng Cao",
      "Jiaxi Yang",
      "Jiawei Chen",
      "Mingze Li",
      "Zeyao Ma",
      "Hao Ge",
      "Zongmeng Zhang",
      "Zeyu Cui",
      "Dayiheng Liu",
      "Jingren Zhou",
      "Jianling Sun",
      "Junyang Lin",
      "Binyuan Hui"
    ],
    "first_author": "Mouxiang Chen",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.02361v1",
    "published": "2026-02-02",
    "update_time": "2026-02-02",
    "download_time": "2026-02-04 01:10:24"
  },
  {
    "id": "2602.03806",
    "title": "Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation",
    "abstract": "Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.",
    "arxiv_url": "https://arxiv.org/abs/2602.03806",
    "authors": [
      "Ziru Chen",
      "Dongdong Chen",
      "Ruinan Jin",
      "Yingbin Liang",
      "Yujia Xie",
      "Huan Sun"
    ],
    "first_author": "Ziru Chen",
    "primary_category": "cs.LG",
    "tag": [
      "Code Reinforcement Learning"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.03806v1",
    "published": "2026-02-03",
    "update_time": "2026-02-03",
    "download_time": "2026-02-05 01:10:34"
  },
  {
    "id": "2602.04726",
    "title": "Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation",
    "abstract": "The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tasks. First, we developed a solution for automatic test scenario generation from a detailed requirements description. This approach relies on specialized worker agents forming a star topology with the supervisor agent in the middle. We demonstrate its capabilities on a real-world example. Second, we developed an agentic AI solution for the document retrieval task in the context of software engineering documents. Our solution enables performing various use cases on a body of documents related to the development of a single software, including search, question answering, tracking changes, and large document summarization. In this case, each use case is handled by a dedicated LLM-based agent, which performs all subtasks related to the corresponding use case. We conclude by hinting at the future perspectives of our line of research.",
    "arxiv_url": "https://arxiv.org/abs/2602.04726",
    "authors": [
      "Marian Kica",
      "Lukas Radosky",
      "David Slivka",
      "Karin Kubinova",
      "Daniel Dovhun",
      "Tomas Uhercik",
      "Erik Bircak",
      "Ivan Polasek"
    ],
    "first_author": "Marian Kica",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": "International Conference on Artificial Intelligence",
    "pdf_url": "https://arxiv.org/pdf/2602.04726v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-06 01:10:48"
  },
  {
    "id": "2602.05891",
    "title": "When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models",
    "abstract": "As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.",
    "arxiv_url": "https://arxiv.org/abs/2602.05891",
    "authors": [
      "Shenyu Zheng",
      "Ximing Dong",
      "Xiaoshuang Liu",
      "Gustavo Oliva",
      "Chong Chun Yong",
      "Dayi Lin",
      "Boyuan Chen",
      "Shaowei Wang",
      "Ahmed E. Hassan"
    ],
    "first_author": "Shenyu Zheng",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.05891v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-08 01:45:38"
  },
  {
    "id": "2602.05780",
    "title": "Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes",
    "abstract": "Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.",
    "arxiv_url": "https://arxiv.org/abs/2602.05780",
    "authors": [
      "Ulrich Finkler",
      "Irene Manotas",
      "Wei Zhang",
      "Geert Janssen",
      "Octavian Popescu",
      "Shyam Ramji"
    ],
    "first_author": "Ulrich Finkler",
    "primary_category": "cs.SE",
    "tag": [
      "Code Completion"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.05780v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-09 01:16:38"
  },
  {
    "id": "2602.06875",
    "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code",
    "abstract": "Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2602.06875",
    "authors": [
      "Jiangping Huang",
      "Wenguang Ye",
      "Weisong Sun",
      "Jian Zhang",
      "Mingyue Zhang",
      "Yang Liu"
    ],
    "first_author": "Jiangping Huang",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.06875v1",
    "published": "2026-02-06",
    "update_time": "2026-02-06",
    "download_time": "2026-02-10 01:25:15"
  },
  {
    "id": "2602.08915",
    "title": "Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance",
    "abstract": "The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).",
    "arxiv_url": "https://arxiv.org/abs/2602.08915",
    "authors": [
      "Giovanni Pinna",
      "Jingzhi Gong",
      "David Williams",
      "Federica Sarro"
    ],
    "first_author": "Giovanni Pinna",
    "primary_category": "cs.SE",
    "tag": [
      "Code Editing"
    ],
    "benchmark": false,
    "conference": "MSR'26 Mining Challenge Track",
    "pdf_url": "https://arxiv.org/pdf/2602.08915v1",
    "published": "2026-02-09",
    "update_time": "2026-02-09",
    "download_time": "2026-02-11 01:23:20"
  },
  {
    "id": "2602.10046",
    "title": "Artisan: Agentic Artifact Evaluation",
    "abstract": "Artifact evaluation has become standard practice in the software engineering community to ensure the reproducibility of research results. However, the current manual process is labor-intensive, and hence, done only as a one-time assessment for a subset of all papers. To support the artifact evaluation effort, we present Artisan, an automated LLM agent for reproducing research results given a paper and its artifact. The approach is enabled by two key contributions: First, we frame the reproduction problem as a code generation task where the goal is to generate a reproduction script that, when executed, reproduces the results reported in a paper. Unlike prior work on automatically reproducing research results in other domains, this formulation allows for running the script independently of the agent and for assessing the reproduction process at a fine-grained level. Second, we design automated judging mechanism that guides the agent toward the expected results without revealing them and that prevent trivial solutions, such as simply copying checked-in results. To evaluate Artisan, we introduce Artisan-Bench, the first benchmark assessing the ability to generate reproduction scripts and the first benchmark for automated artifact evaluation in software engineering. Artisan-Bench comprises 60 tasks derived from 23 software engineering papers, covering different research areas and programming languages. We validate all tasks in Artisan-Bench for reproducibility to ensure that the tasks are feasible. Our experiments show that Artisan is effective, producing 44/60 reproduction scripts and outperforming the best available baseline, a vanilla LLM agent (mini-swe-agent), by 3.14$\\times$ in terms of reproduction scripts generated while taking $0.45 and 48 minutes, on average per task. Artisan also helped uncover 20 new errors in either the paper or artifact.",
    "arxiv_url": "https://arxiv.org/abs/2602.10046",
    "authors": [
      "Doehyun Baek",
      "Michael Pradel"
    ],
    "first_author": "Doehyun Baek",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.10046v1",
    "published": "2026-02-10",
    "update_time": "2026-02-10",
    "download_time": "2026-02-12 01:16:12"
  },
  {
    "id": "2602.11114",
    "title": "Learning to Compose for Cross-domain Agentic Workflow Generation",
    "abstract": "Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily on the task distribution and the available operators. Under domain shift, current systems typically rely on iterative workflow refinement to discover a feasible workflow from a large workflow space, incurring high iteration costs and yielding unstable, domain-specific behavior. In response, we internalize a decompose-recompose-decide mechanism into an open-source LLM for cross-domain workflow generation. To decompose, we learn a compact set of reusable workflow capabilities across diverse domains. To recompose, we map each input task to a sparse composition over these bases to generate a task-specific workflow in a single pass. To decide, we attribute the success or failure of workflow generation to counterfactual contributions from learned capabilities, thereby capturing which capabilities actually drive success by their marginal effects. Across stringent multi-domain, cross-domain, and unseen-domain evaluations, our 1-pass generator surpasses SOTA refinement baselines that consume 20 iterations, while substantially reducing generation latency and cost.",
    "arxiv_url": "https://arxiv.org/abs/2602.11114",
    "authors": [
      "Jialiang Wang",
      "Shengxiang Xu",
      "Hanmo Liu",
      "Jiachuan Wang",
      "Yuyu Luo",
      "Shimin Di",
      "Min-Ling Zhang",
      "Lei Chen"
    ],
    "first_author": "Jialiang Wang",
    "primary_category": "cs.MA",
    "tag": [
      "Agentic Workflow Generation"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.11114v1",
    "published": "2026-02-11",
    "update_time": "2026-02-11",
    "download_time": "2026-02-13 01:19:07"
  },
  {
    "id": "2602.12256",
    "title": "Automated Test Suite Enhancement Using Large Language Models with Few-shot Prompting",
    "abstract": "Unit testing is essential for verifying the functional correctness of code modules (e.g., classes, methods), but manually writing unit tests is often labor-intensive and time-consuming. Unit tests generated by tools that employ traditional approaches, such as search-based software testing (SBST), lack readability, naturalness, and practical usability. LLMs have recently provided promising results and become integral to developers' daily practices. Consequently, software repositories now include a mix of human-written tests, LLM-generated tests, and those from tools employing traditional approaches such as SBST. While LLMs' zero-shot capabilities have been widely studied, their few-shot learning potential for unit test generation remains underexplored. Few-shot prompting enables LLMs to learn from examples in the prompt, and automatically retrieving such examples could enhance test suites. This paper empirically investigates how few-shot prompting with different test artifact sources, comprising human, SBST, or LLM, affects the quality of LLM-generated unit tests as program comprehension artifacts and their contribution to improving existing test suites by evaluating not only correctness and coverage but also readability, cognitive complexity, and maintainability in hybrid human-AI codebases. We conducted experiments on HumanEval and ClassEval datasets using GPT-4o, which is integrated into GitHub Copilot and widely used among developers. We also assessed retrieval-based methods for selecting relevant examples. Our results show that LLMs can generate high-quality tests via few-shot prompting, with human-written examples producing the best coverage and correctness. Additionally, selecting examples based on the combined similarity of problem description and code consistently yields the most effective few-shot prompts.",
    "arxiv_url": "https://arxiv.org/abs/2602.12256",
    "authors": [
      "Alex Chudic",
      "Gül Çalıklı"
    ],
    "first_author": "Alex Chudic",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": "ICPC 2026 (34th International Conference on Program Comprehension)",
    "pdf_url": "https://arxiv.org/pdf/2602.12256v1",
    "published": "2026-02-12",
    "update_time": "2026-02-12",
    "download_time": "2026-02-14 01:12:12"
  },
  {
    "id": "2602.12144",
    "title": "On the Adoption of AI Coding Agents in Open-source Android and iOS Development",
    "abstract": "AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We find that Android projects have received 2x more AI-authored PRs and have achieved higher PR acceptance rate (71%) than iOS (63%), with significant agent-level variation on Android. Across task categories, PRs with routine tasks (feature, fix, and ui) achieve the highest acceptance, while structural changes like refactor and build achieve lower success and longer resolution times. Furthermore, our evolution analysis shows improvement in PR resolution time on Android through mid-2025 before it declined again. Our findings offer the first evidence-based characterization of AI agents effects on OSS mobile projects and establish empirical baselines for evaluating agent-generated contributions to design platform aware agentic systems.",
    "arxiv_url": "https://arxiv.org/abs/2602.12144",
    "authors": [
      "Muhammad Ahmad Khan",
      "Hasnain Ali",
      "Muneeb Rana",
      "Muhammad Saqib Ilyas",
      "Abdul Ali Bangash"
    ],
    "first_author": "Muhammad Ahmad Khan",
    "primary_category": "cs.SE",
    "tag": [
      "Empirical Evaluation of AI Coding Agents"
    ],
    "benchmark": false,
    "conference": "MSR 2026 Mining Challenge track",
    "pdf_url": "https://arxiv.org/pdf/2602.12144v1",
    "published": "2026-02-12",
    "update_time": "2026-02-12",
    "download_time": "2026-02-15 01:18:46"
  },
  {
    "id": "2602.12058",
    "title": "ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair",
    "abstract": "Model checking in TLA+ provides strong correctness guarantees, yet practitioners continue to face significant challenges in interpreting counterexamples, understanding large state-transition graphs, and repairing faulty models. These difficulties stem from the limited explainability of raw model-checker output and the substantial manual effort required to trace violations back to source specifications. Although the TLA+ Toolbox includes a state diagram viewer, it offers only a static, fully expanded graph without folding, color highlighting, or semantic explanations, which limits its scalability and interpretability. We present ModelWisdom, an interactive environment that uses visualization and large language models to make TLA+ model checking more interpretable and actionable. ModelWisdom offers: (i) Model Visualization, with colorized violation highlighting, click-through links from transitions to TLA+ code, and mapping between violating states and broken properties; (ii) Graph Optimization, including tree-based structuring and node/edge folding to manage large models; (iii) Model Digest, which summarizes and explains subgraphs via large language models (LLMs) and performs preprocessing and partial explanations; and (iv) Model Repair, which extracts error information and supports iterative debugging. Together, these capabilities turn raw model-checker output into an interactive, explainable workflow, improving understanding and reducing debugging effort for nontrivial TLA+ specifications. The website to ModelWisdom is available: https://model-wisdom.pages.dev. A demonstrative video can be found at https://www.youtube.com/watch?v=plyZo30VShA.",
    "arxiv_url": "https://arxiv.org/abs/2602.12058",
    "authors": [
      "Zhiyong Chen",
      "Jialun Cao",
      "Chang Xu",
      "Shing-Chi Cheung"
    ],
    "first_author": "Zhiyong Chen",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": "FM 2026 Research Track (Tool)",
    "pdf_url": "https://arxiv.org/pdf/2602.12058v1",
    "published": "2026-02-12",
    "update_time": "2026-02-12",
    "download_time": "2026-02-16 01:16:04"
  },
  {
    "id": "2602.13072",
    "title": "Automated Testing of Task-based Chatbots: How Far Are We?",
    "abstract": "Task-based chatbots are software, typically embedded in real-world applications, that assist users in completing tasks through a conversational interface. As chatbots are gaining popularity, effectively assessing their quality has become crucial. Whereas traditional testing techniques fail to systematically exercise the conversational space of chatbots, several approaches specifically targeting chatbots have emerged from both industry and research. Although these techniques have shown advancements over the years, they still exhibit limitations, such as simplicity of the generated test scenarios and weakness in implemented oracles. In this paper, we conduct a confirmatory study to investigate such limitations by evaluating the effectiveness of state-of-the-art chatbot testing techniques on a curated selection of task-based chatbots from GitHub, developed using the most popular commercial and open-source platforms.",
    "arxiv_url": "https://arxiv.org/abs/2602.13072",
    "authors": [
      "Diego Clerissi",
      "Elena Masserini",
      "Daniela Micucci",
      "Leonardo Mariani"
    ],
    "first_author": "Diego Clerissi",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": "23rd International Conference on Mining Software Repositories (MSR) 2026 - Registered Reports",
    "pdf_url": "https://arxiv.org/pdf/2602.13072v1",
    "published": "2026-02-13",
    "update_time": "2026-02-13",
    "download_time": "2026-02-17 01:14:51"
  },
  {
    "id": "2602.14922",
    "title": "ReusStdFlow: A Standardized Reusability Framework for Dynamic Workflow Construction in Agentic AI",
    "abstract": "To address the ``reusability dilemma'' and structural hallucinations in enterprise Agentic AI,this paper proposes ReusStdFlow, a framework centered on a novel ``Extraction-Storage-Construction'' paradigm. The framework deconstructs heterogeneous, platform-specific Domain Specific Languages (DSLs) into standardized, modular workflow segments. It employs a dual knowledge architecture-integrating graph and vector databases-to facilitate synergistic retrieval of both topological structures and functional semantics. Finally, workflows are intelligently assembled using a retrieval-augmented generation (RAG) strategy. Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction. This framework provides a standardized solution for the automated reorganization and efficient reuse of enterprise digital assets.",
    "arxiv_url": "https://arxiv.org/abs/2602.14922",
    "authors": [
      "Gaoyang Zhang",
      "Shanghong Zou",
      "Yafang Wang",
      "He Zhang",
      "Ruohua Xu",
      "Feng Zhao"
    ],
    "first_author": "Gaoyang Zhang",
    "primary_category": "cs.AI",
    "tag": [
      "Code Translation"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.14922v1",
    "published": "2026-02-16",
    "update_time": "2026-02-16",
    "download_time": "2026-02-18 01:17:15"
  },
  {
    "id": "2602.15761",
    "title": "A Differential Fuzzing-Based Evaluation of Functional Equivalence in LLM-Generated Code Refactorings",
    "abstract": "With the rapid adoption of large language models (LLMs) in automated code refactoring, assessing and ensuring functional equivalence between LLM-generated refactoring and the original implementation becomes critical. While prior work typically relies on predefined test cases to evaluate correctness, in this work, we leverage differential fuzzing to check functional equivalence in LLM-generated code refactorings. Unlike test-based evaluation, a differential fuzzing-based equivalence checker needs no predefined test cases and can explore a much larger input space by executing and comparing thousands of automatically generated test inputs. In a large-scale evaluation of six LLMs (CodeLlama, Codestral, StarChat2, Qwen-2.5, Olmo-3, and GPT-4o) across three datasets and two refactoring types, we find that LLMs show a non-trivial tendency to alter program semantics, producing 19-35% functionally non-equivalent refactorings. Our experiments further demonstrate that about 21% of these non-equivalent refactorings remain undetected by the existing test suites of the three evaluated datasets. Collectively, the findings of this study imply that reliance on existing tests might overestimate functional equivalence in LLM-generated code refactorings, which remain prone to semantic divergence.",
    "arxiv_url": "https://arxiv.org/abs/2602.15761",
    "authors": [
      "Simantika Bhattacharjee Dristi",
      "Matthew B. Dwyer"
    ],
    "first_author": "Simantika Bhattacharjee Dristi",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.15761v1",
    "published": "2026-02-17",
    "update_time": "2026-02-17",
    "download_time": "2026-02-19 01:16:11"
  },
  {
    "id": "2602.16671",
    "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
    "abstract": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",
    "arxiv_url": "https://arxiv.org/abs/2602.16671",
    "authors": [
      "Jaid Monwar Chowdhury",
      "Chi-An Fu",
      "Reyhaneh Jabbarvand"
    ],
    "first_author": "Jaid Monwar Chowdhury",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.16671v1",
    "published": "2026-02-18",
    "update_time": "2026-02-18",
    "download_time": "2026-02-20 01:11:46"
  },
  {
    "id": "2602.17622",
    "title": "What Makes a Good LLM Agent for Real-world Penetration Testing?",
    "abstract": "LLM-based agents show promise for automating penetration testing, yet reported performance varies widely across systems and benchmarks. We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks of increasing complexity. Our analysis reveals two distinct failure modes: Type A failures stem from capability gaps (missing tools, inadequate prompts) that engineering readily addresses, while Type B failures persist regardless of tooling due to planning and state management limitations. We show that Type B failures share a root cause that is largely invariant to the underlying LLM: agents lack real-time task difficulty estimation. As a result, agents misallocate effort, over-commit to low-value branches, and exhaust context before completing attack chains.   Based on this insight, we present Excalibur, a penetration testing agent that couples strong tooling with difficulty-aware planning. A Tool and Skill Layer eliminates Type A failures through typed interfaces and retrieval-augmented knowledge. A Task Difficulty Assessment (TDA) mechanism addresses Type B failures by estimating tractability through four measurable dimensions (horizon estimation, evidence confidence, context load, and historical success) and uses these estimates to guide exploration-exploitation decisions within an Evidence-Guided Attack Tree Search (EGATS) framework. Excalibur achieves up to 91% task completion on CTF benchmarks with frontier models (39 to 49% relative improvement over baselines) and compromises 4 of 5 hosts on the GOAD Active Directory environment versus 2 by prior systems. These results show that difficulty-aware planning yields consistent end-to-end gains across models and addresses a limitation that model scaling alone does not eliminate.",
    "arxiv_url": "https://arxiv.org/abs/2602.17622",
    "authors": [
      "Gelei Deng",
      "Yi Liu",
      "Yuekang Li",
      "Ruozhao Yang",
      "Xiaofei Xie",
      "Jie Zhang",
      "Han Qiu",
      "Tianwei Zhang"
    ],
    "first_author": "Gelei Deng",
    "primary_category": "cs.CR",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.17622v1",
    "published": "2026-02-19",
    "update_time": "2026-02-19",
    "download_time": "2026-02-21 01:09:54"
  },
  {
    "id": "2602.17183",
    "title": "Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering",
    "abstract": "Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.",
    "arxiv_url": "https://arxiv.org/abs/2602.17183",
    "authors": [
      "Kishan Maharaj",
      "Nandakishore Menon",
      "Ashita Saxena",
      "Srikanth Tamilselvam"
    ],
    "first_author": "Kishan Maharaj",
    "primary_category": "cs.SE",
    "tag": [
      "Code Reasoning"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.17183v1",
    "published": "2026-02-19",
    "update_time": "2026-02-19",
    "download_time": "2026-02-22 01:16:06"
  },
  {
    "id": "2602.17091",
    "title": "What to Cut? Predicting Unnecessary Methods in Agentic Code Generation",
    "abstract": "Agentic Coding, powered by autonomous agents such as GitHub Copilot and Cursor, enables developers to generate code, tests, and pull requests from natural language instructions alone. While this accelerates implementation, it produces larger volumes of code per pull request, shifting the burden from implementers to reviewers. In practice, a notable portion of AI-generated code is eventually deleted during review, yet reviewers must still examine such code before deciding to remove it. No prior work has explored methods to help reviewers efficiently identify code that will be removed.In this paper, we propose a prediction model that identifies functions likely to be deleted during PR review. Our results show that functions deleted for different reasons exhibit distinct characteristics, and our model achieves an AUC of 87.1%. These findings suggest that predictive approaches can help reviewers prioritize their efforts on essential code.",
    "arxiv_url": "https://arxiv.org/abs/2602.17091",
    "authors": [
      "Kan Watanabe",
      "Tatsuya Shirai",
      "Yutaro Kashiwa",
      "Hajimu Iida"
    ],
    "first_author": "Kan Watanabe",
    "primary_category": "cs.SE",
    "tag": [
      "Code Review"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.17091v1",
    "published": "2026-02-19",
    "update_time": "2026-02-19",
    "download_time": "2026-02-23 01:15:40"
  },
  {
    "id": "2602.18307",
    "title": "VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean",
    "abstract": "Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.",
    "arxiv_url": "https://arxiv.org/abs/2602.18307",
    "authors": [
      "Yutong Xin",
      "Qiaochu Chen",
      "Greg Durrett",
      "Işil Dillig"
    ],
    "first_author": "Yutong Xin",
    "primary_category": "cs.SE",
    "tag": [
      "Formal Verification"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.18307v1",
    "published": "2026-02-20",
    "update_time": "2026-02-20",
    "download_time": "2026-02-24 01:13:27"
  },
  {
    "id": "2602.20048",
    "title": "CodeCompass: Navigating the Navigation Paradox in Agentic Code Intelligence",
    "abstract": "Modern code intelligence agents operate in contexts exceeding 1 million tokens--far beyond the scale where humans manually locate relevant files. Yet agents consistently fail to discover architecturally critical files when solving real-world coding tasks. We identify the Navigation Paradox: agents perform poorly not due to context limits, but because navigation and retrieval are fundamentally distinct problems. Through 258 automated trials across 30 benchmark tasks on a production FastAPI repository, we demonstrate that graph-based structural navigation via CodeCompass--a Model Context Protocol server exposing dependency graphs--achieves 99.4% task completion on hidden-dependency tasks, a 23.2 percentage-point improvement over vanilla agents (76.2%) and 21.2 points over BM25 retrieval (78.2%).However, we uncover a critical adoption gap: 58% of trials with graph access made zero tool calls, and agents required explicit prompt engineering to adopt the tool consistently. Our findings reveal that the bottleneck is not tool availability but behavioral alignment--agents must be explicitly guided to leverage structural context over lexical heuristics. We contribute: (1) a task taxonomy distinguishing semantic-search, structural, and hidden-dependency scenarios; (2) empirical evidence that graph navigation outperforms retrieval when dependencies lack lexical overlap; and (3) open-source infrastructure for reproducible evaluation of navigation tools.",
    "arxiv_url": "https://arxiv.org/abs/2602.20048",
    "authors": [
      "Tarakanath Paipuru"
    ],
    "first_author": "Tarakanath Paipuru",
    "primary_category": "cs.AI",
    "tag": [
      "Code Navigation"
    ],
    "benchmark": true,
    "conference": "",
    "pdf_url": "https://arxiv.org/pdf/2602.20048v1",
    "published": "2026-02-23",
    "update_time": "2026-02-23",
    "download_time": "2026-02-25 01:18:25"
  },
  {
    "id": "2602.20979",
    "title": "Toward an Agentic Infused Software Ecosystem",
    "abstract": "Fully leveraging the capabilities of AI agents in software development requires a rethinking of the software ecosystem itself. To this end, this paper outlines the creation of an Agentic Infused Software Ecosystem (AISE), that rests on three pillars. The first, of course, is the AI agents themselves, which in the past 5 years have moved from simple code completion and toward sophisticated independent development tasks, a trend which will only continue. The second pillar is the programming language and APIs (or tools) that these agents use to accomplish tasks, and increasingly, serve as the communication substrate that humans and AI agents interact and collaborate through. The final pillar is the runtime environment and ecosystem that agents operate within, and which provide the capabilities that programmatic agents use to interface with (and effect actions in) the external world. To realize the vision of AISE, all three pillars must be advanced in a holistic manner, and critically, in a manner that is synergistic for AI agents as they exist today, those that will exist in the future, and for the human developers that work alongside them.",
    "arxiv_url": "https://arxiv.org/abs/2602.20979",
    "authors": [
      "Mark Marron"
    ],
    "first_author": "Mark Marron",
    "primary_category": "cs.SE",
    "tag": [
      "Agentic Software Ecosystem"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.20979v1",
    "published": "2026-02-24",
    "update_time": "2026-02-24",
    "download_time": "2026-02-26 01:12:19"
  },
  {
    "id": "2602.22124",
    "title": "SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents",
    "abstract": "Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Protégé, a post-training framework that reframes software repair as an expert-protégé collaboration problem. In SWE-Protégé, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).",
    "arxiv_url": "https://arxiv.org/abs/2602.22124",
    "authors": [
      "Patrick Tser Jern Kon",
      "Archana Pradeep",
      "Ang Chen",
      "Alexander P. Ellis",
      "Warren Hunt",
      "Zijian Wang",
      "John Yang",
      "Samuel Thompson"
    ],
    "first_author": "Patrick Tser Jern Kon",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.22124v1",
    "published": "2026-02-25",
    "update_time": "2026-02-25",
    "download_time": "2026-02-27 01:11:02"
  },
  {
    "id": "2602.23331",
    "title": "Utilizing LLMs for Industrial Process Automation",
    "abstract": "A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.",
    "arxiv_url": "https://arxiv.org/abs/2602.23331",
    "authors": [
      "Salim Fares"
    ],
    "first_author": "Salim Fares",
    "primary_category": "cs.SE",
    "tag": [
      "Code Prompting"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.23331v1",
    "published": "2026-02-26",
    "update_time": "2026-02-26",
    "download_time": "2026-02-28 01:05:38"
  },
  {
    "id": "2602.23065",
    "title": "LLM-Powered Silent Bug Fuzzing in Deep Learning Libraries via Versatile and Controlled Bug Transfer",
    "abstract": "Deep learning (DL) libraries are widely used in critical applications, where even subtle silent bugs can lead to serious consequences. While existing DL fuzzing techniques have made progress in detecting crashes, they inherently struggle to detect silent bugs due to the lack of effective test programs and corresponding oracles.   Building on the observation that historical bug reports contain rich, underutilized information about silent bugs, we leverage large language models (LLMs) to perform versatile yet controlled bug transfer for silent bug fuzzing. Specifically, our approach uses LLMs to extract context-aware bug patterns from historical issues, match semantically related Application Programming Interfaces (APIs) using functionality-based embeddings, and synthesize test cases with customized oracles. This enables proactive detection of silent bugs by transferring high-risk contexts and oracle designs from known buggy APIs to functionally similar target APIs. To ensure the reliability of our context-aware bug transfer, we introduce an LLM-powered self-validation module that systematically evaluates the validity of each transferred bug instance. We implement this methodology in a tool named TransFuzz and evaluate it on three mainstream DL libraries: PyTorch, TensorFlow, and MindSpore. TransFuzz successfully discovers 79 previously unknown bugs (12 confirmed as Common Vulnerabilities and Exposures (CVEs)) in 10 bug types, demonstrating its effectiveness and generalizability in migrating DL library bug discovery capabilities.",
    "arxiv_url": "https://arxiv.org/abs/2602.23065",
    "authors": [
      "Kunpeng Zhang",
      "Dongwei Xiao",
      "Daoyuan Wu",
      "Jiali Zhao",
      "Yuanyi Lin",
      "Tongtong Xu",
      "Shaohua Wang",
      "Shuai Wang"
    ],
    "first_author": "Kunpeng Zhang",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.23065v1",
    "published": "2026-02-26",
    "update_time": "2026-02-26",
    "download_time": "2026-03-01 01:21:18"
  }
]