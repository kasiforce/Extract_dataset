[
  {
    "id": "2602.02361",
    "title": "SWE-Universe: Scale Real-World Verifiable Environments to Millions",
    "abstract": "We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.",
    "arxiv_url": "https://arxiv.org/abs/2602.02361",
    "authors": [
      "Mouxiang Chen",
      "Lei Zhang",
      "Yunlong Feng",
      "Xuwu Wang",
      "Wenting Zhao",
      "Ruisheng Cao",
      "Jiaxi Yang",
      "Jiawei Chen",
      "Mingze Li",
      "Zeyao Ma",
      "Hao Ge",
      "Zongmeng Zhang",
      "Zeyu Cui",
      "Dayiheng Liu",
      "Jingren Zhou",
      "Jianling Sun",
      "Junyang Lin",
      "Binyuan Hui"
    ],
    "first_author": "Mouxiang Chen",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.02361v1",
    "published": "2026-02-02",
    "update_time": "2026-02-02",
    "download_time": "2026-02-04 01:10:24"
  },
  {
    "id": "2602.03806",
    "title": "Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation",
    "abstract": "Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.",
    "arxiv_url": "https://arxiv.org/abs/2602.03806",
    "authors": [
      "Ziru Chen",
      "Dongdong Chen",
      "Ruinan Jin",
      "Yingbin Liang",
      "Yujia Xie",
      "Huan Sun"
    ],
    "first_author": "Ziru Chen",
    "primary_category": "cs.LG",
    "tag": [
      "Code Reinforcement Learning"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.03806v1",
    "published": "2026-02-03",
    "update_time": "2026-02-03",
    "download_time": "2026-02-05 01:10:34"
  },
  {
    "id": "2602.04726",
    "title": "Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation",
    "abstract": "The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tasks. First, we developed a solution for automatic test scenario generation from a detailed requirements description. This approach relies on specialized worker agents forming a star topology with the supervisor agent in the middle. We demonstrate its capabilities on a real-world example. Second, we developed an agentic AI solution for the document retrieval task in the context of software engineering documents. Our solution enables performing various use cases on a body of documents related to the development of a single software, including search, question answering, tracking changes, and large document summarization. In this case, each use case is handled by a dedicated LLM-based agent, which performs all subtasks related to the corresponding use case. We conclude by hinting at the future perspectives of our line of research.",
    "arxiv_url": "https://arxiv.org/abs/2602.04726",
    "authors": [
      "Marian Kica",
      "Lukas Radosky",
      "David Slivka",
      "Karin Kubinova",
      "Daniel Dovhun",
      "Tomas Uhercik",
      "Erik Bircak",
      "Ivan Polasek"
    ],
    "first_author": "Marian Kica",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": "International Conference on Artificial Intelligence",
    "pdf_url": "https://arxiv.org/pdf/2602.04726v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-06 01:10:48"
  },
  {
    "id": "2602.05891",
    "title": "When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models",
    "abstract": "As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.",
    "arxiv_url": "https://arxiv.org/abs/2602.05891",
    "authors": [
      "Shenyu Zheng",
      "Ximing Dong",
      "Xiaoshuang Liu",
      "Gustavo Oliva",
      "Chong Chun Yong",
      "Dayi Lin",
      "Boyuan Chen",
      "Shaowei Wang",
      "Ahmed E. Hassan"
    ],
    "first_author": "Shenyu Zheng",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.05891v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-08 01:45:38"
  },
  {
    "id": "2602.05780",
    "title": "Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes",
    "abstract": "Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.",
    "arxiv_url": "https://arxiv.org/abs/2602.05780",
    "authors": [
      "Ulrich Finkler",
      "Irene Manotas",
      "Wei Zhang",
      "Geert Janssen",
      "Octavian Popescu",
      "Shyam Ramji"
    ],
    "first_author": "Ulrich Finkler",
    "primary_category": "cs.SE",
    "tag": [
      "Code Completion"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.05780v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-09 01:16:38"
  },
  {
    "id": "2602.06875",
    "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code",
    "abstract": "Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2602.06875",
    "authors": [
      "Jiangping Huang",
      "Wenguang Ye",
      "Weisong Sun",
      "Jian Zhang",
      "Mingyue Zhang",
      "Yang Liu"
    ],
    "first_author": "Jiangping Huang",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.06875v1",
    "published": "2026-02-06",
    "update_time": "2026-02-06",
    "download_time": "2026-02-10 01:25:15"
  },
  {
    "id": "2602.08915",
    "title": "Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance",
    "abstract": "The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).",
    "arxiv_url": "https://arxiv.org/abs/2602.08915",
    "authors": [
      "Giovanni Pinna",
      "Jingzhi Gong",
      "David Williams",
      "Federica Sarro"
    ],
    "first_author": "Giovanni Pinna",
    "primary_category": "cs.SE",
    "tag": [
      "Code Editing"
    ],
    "benchmark": false,
    "conference": "MSR'26 Mining Challenge Track",
    "pdf_url": "https://arxiv.org/pdf/2602.08915v1",
    "published": "2026-02-09",
    "update_time": "2026-02-09",
    "download_time": "2026-02-11 01:23:20"
  },
  {
    "id": "2602.10046",
    "title": "Artisan: Agentic Artifact Evaluation",
    "abstract": "Artifact evaluation has become standard practice in the software engineering community to ensure the reproducibility of research results. However, the current manual process is labor-intensive, and hence, done only as a one-time assessment for a subset of all papers. To support the artifact evaluation effort, we present Artisan, an automated LLM agent for reproducing research results given a paper and its artifact. The approach is enabled by two key contributions: First, we frame the reproduction problem as a code generation task where the goal is to generate a reproduction script that, when executed, reproduces the results reported in a paper. Unlike prior work on automatically reproducing research results in other domains, this formulation allows for running the script independently of the agent and for assessing the reproduction process at a fine-grained level. Second, we design automated judging mechanism that guides the agent toward the expected results without revealing them and that prevent trivial solutions, such as simply copying checked-in results. To evaluate Artisan, we introduce Artisan-Bench, the first benchmark assessing the ability to generate reproduction scripts and the first benchmark for automated artifact evaluation in software engineering. Artisan-Bench comprises 60 tasks derived from 23 software engineering papers, covering different research areas and programming languages. We validate all tasks in Artisan-Bench for reproducibility to ensure that the tasks are feasible. Our experiments show that Artisan is effective, producing 44/60 reproduction scripts and outperforming the best available baseline, a vanilla LLM agent (mini-swe-agent), by 3.14$\\times$ in terms of reproduction scripts generated while taking $0.45 and 48 minutes, on average per task. Artisan also helped uncover 20 new errors in either the paper or artifact.",
    "arxiv_url": "https://arxiv.org/abs/2602.10046",
    "authors": [
      "Doehyun Baek",
      "Michael Pradel"
    ],
    "first_author": "Doehyun Baek",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.10046v1",
    "published": "2026-02-10",
    "update_time": "2026-02-10",
    "download_time": "2026-02-12 01:16:12"
  },
  {
    "id": "2602.11114",
    "title": "Learning to Compose for Cross-domain Agentic Workflow Generation",
    "abstract": "Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily on the task distribution and the available operators. Under domain shift, current systems typically rely on iterative workflow refinement to discover a feasible workflow from a large workflow space, incurring high iteration costs and yielding unstable, domain-specific behavior. In response, we internalize a decompose-recompose-decide mechanism into an open-source LLM for cross-domain workflow generation. To decompose, we learn a compact set of reusable workflow capabilities across diverse domains. To recompose, we map each input task to a sparse composition over these bases to generate a task-specific workflow in a single pass. To decide, we attribute the success or failure of workflow generation to counterfactual contributions from learned capabilities, thereby capturing which capabilities actually drive success by their marginal effects. Across stringent multi-domain, cross-domain, and unseen-domain evaluations, our 1-pass generator surpasses SOTA refinement baselines that consume 20 iterations, while substantially reducing generation latency and cost.",
    "arxiv_url": "https://arxiv.org/abs/2602.11114",
    "authors": [
      "Jialiang Wang",
      "Shengxiang Xu",
      "Hanmo Liu",
      "Jiachuan Wang",
      "Yuyu Luo",
      "Shimin Di",
      "Min-Ling Zhang",
      "Lei Chen"
    ],
    "first_author": "Jialiang Wang",
    "primary_category": "cs.MA",
    "tag": [
      "Agentic Workflow Generation"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.11114v1",
    "published": "2026-02-11",
    "update_time": "2026-02-11",
    "download_time": "2026-02-13 01:19:07"
  },
  {
    "id": "2602.12256",
    "title": "Automated Test Suite Enhancement Using Large Language Models with Few-shot Prompting",
    "abstract": "Unit testing is essential for verifying the functional correctness of code modules (e.g., classes, methods), but manually writing unit tests is often labor-intensive and time-consuming. Unit tests generated by tools that employ traditional approaches, such as search-based software testing (SBST), lack readability, naturalness, and practical usability. LLMs have recently provided promising results and become integral to developers' daily practices. Consequently, software repositories now include a mix of human-written tests, LLM-generated tests, and those from tools employing traditional approaches such as SBST. While LLMs' zero-shot capabilities have been widely studied, their few-shot learning potential for unit test generation remains underexplored. Few-shot prompting enables LLMs to learn from examples in the prompt, and automatically retrieving such examples could enhance test suites. This paper empirically investigates how few-shot prompting with different test artifact sources, comprising human, SBST, or LLM, affects the quality of LLM-generated unit tests as program comprehension artifacts and their contribution to improving existing test suites by evaluating not only correctness and coverage but also readability, cognitive complexity, and maintainability in hybrid human-AI codebases. We conducted experiments on HumanEval and ClassEval datasets using GPT-4o, which is integrated into GitHub Copilot and widely used among developers. We also assessed retrieval-based methods for selecting relevant examples. Our results show that LLMs can generate high-quality tests via few-shot prompting, with human-written examples producing the best coverage and correctness. Additionally, selecting examples based on the combined similarity of problem description and code consistently yields the most effective few-shot prompts.",
    "arxiv_url": "https://arxiv.org/abs/2602.12256",
    "authors": [
      "Alex Chudic",
      "Gül Çalıklı"
    ],
    "first_author": "Alex Chudic",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": "ICPC 2026 (34th International Conference on Program Comprehension)",
    "pdf_url": "https://arxiv.org/pdf/2602.12256v1",
    "published": "2026-02-12",
    "update_time": "2026-02-12",
    "download_time": "2026-02-14 01:12:12"
  },
  {
    "id": "2602.12144",
    "title": "On the Adoption of AI Coding Agents in Open-source Android and iOS Development",
    "abstract": "AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We find that Android projects have received 2x more AI-authored PRs and have achieved higher PR acceptance rate (71%) than iOS (63%), with significant agent-level variation on Android. Across task categories, PRs with routine tasks (feature, fix, and ui) achieve the highest acceptance, while structural changes like refactor and build achieve lower success and longer resolution times. Furthermore, our evolution analysis shows improvement in PR resolution time on Android through mid-2025 before it declined again. Our findings offer the first evidence-based characterization of AI agents effects on OSS mobile projects and establish empirical baselines for evaluating agent-generated contributions to design platform aware agentic systems.",
    "arxiv_url": "https://arxiv.org/abs/2602.12144",
    "authors": [
      "Muhammad Ahmad Khan",
      "Hasnain Ali",
      "Muneeb Rana",
      "Muhammad Saqib Ilyas",
      "Abdul Ali Bangash"
    ],
    "first_author": "Muhammad Ahmad Khan",
    "primary_category": "cs.SE",
    "tag": [
      "Empirical Evaluation of AI Coding Agents"
    ],
    "benchmark": false,
    "conference": "MSR 2026 Mining Challenge track",
    "pdf_url": "https://arxiv.org/pdf/2602.12144v1",
    "published": "2026-02-12",
    "update_time": "2026-02-12",
    "download_time": "2026-02-15 01:18:46"
  },
  {
    "id": "2602.12058",
    "title": "ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair",
    "abstract": "Model checking in TLA+ provides strong correctness guarantees, yet practitioners continue to face significant challenges in interpreting counterexamples, understanding large state-transition graphs, and repairing faulty models. These difficulties stem from the limited explainability of raw model-checker output and the substantial manual effort required to trace violations back to source specifications. Although the TLA+ Toolbox includes a state diagram viewer, it offers only a static, fully expanded graph without folding, color highlighting, or semantic explanations, which limits its scalability and interpretability. We present ModelWisdom, an interactive environment that uses visualization and large language models to make TLA+ model checking more interpretable and actionable. ModelWisdom offers: (i) Model Visualization, with colorized violation highlighting, click-through links from transitions to TLA+ code, and mapping between violating states and broken properties; (ii) Graph Optimization, including tree-based structuring and node/edge folding to manage large models; (iii) Model Digest, which summarizes and explains subgraphs via large language models (LLMs) and performs preprocessing and partial explanations; and (iv) Model Repair, which extracts error information and supports iterative debugging. Together, these capabilities turn raw model-checker output into an interactive, explainable workflow, improving understanding and reducing debugging effort for nontrivial TLA+ specifications. The website to ModelWisdom is available: https://model-wisdom.pages.dev. A demonstrative video can be found at https://www.youtube.com/watch?v=plyZo30VShA.",
    "arxiv_url": "https://arxiv.org/abs/2602.12058",
    "authors": [
      "Zhiyong Chen",
      "Jialun Cao",
      "Chang Xu",
      "Shing-Chi Cheung"
    ],
    "first_author": "Zhiyong Chen",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": "FM 2026 Research Track (Tool)",
    "pdf_url": "https://arxiv.org/pdf/2602.12058v1",
    "published": "2026-02-12",
    "update_time": "2026-02-12",
    "download_time": "2026-02-16 01:16:04"
  }
]