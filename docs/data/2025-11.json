[
  {
    "id": "2511.17368",
    "title": "Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software",
    "abstract": "Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.",
    "arxiv_url": "https://arxiv.org/abs/2511.17368",
    "authors": [
      "Eric L. Melin",
      "Ahmed Musa Awon",
      "Nasir U. Eisty",
      "Neil A. Ernst",
      "Shurui Zhou"
    ],
    "first_author": "Eric L. Melin",
    "primary_category": "cs.SE",
    "tag": [
      "Code Quality Analysis"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.17368v1",
    "published": "2025-11-21",
    "update_time": "2025-11-21",
    "download_time": "2025-11-24 16:01:06"
  },
  {
    "id": "2511.17330",
    "title": "Agentic Program Verification",
    "abstract": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.   In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.   Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.",
    "arxiv_url": "https://arxiv.org/abs/2511.17330",
    "authors": [
      "Haoxin Tu",
      "Huan Zhao",
      "Yahui Song",
      "Mehtab Zafar",
      "Ruijie Meng",
      "Abhik Roychoudhury"
    ],
    "first_author": "Haoxin Tu",
    "primary_category": "cs.SE",
    "tag": [
      "Program Verification"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.17330v1",
    "published": "2025-11-21",
    "update_time": "2025-11-21",
    "download_time": "2025-11-24 16:01:18"
  },
  {
    "id": "2511.17262",
    "title": "SlsReuse: LLM-Powered Serverless Function Reuse",
    "abstract": "Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.   This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.",
    "arxiv_url": "https://arxiv.org/abs/2511.17262",
    "authors": [
      "Jinfeng Wen",
      "Yuehan Sun"
    ],
    "first_author": "Jinfeng Wen",
    "primary_category": "cs.SE",
    "tag": [
      "Code Retrieval and Reuse"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.17262v1",
    "published": "2025-11-21",
    "update_time": "2025-11-21",
    "download_time": "2025-11-24 16:01:47"
  },
  {
    "id": "2511.17027",
    "title": "ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting",
    "abstract": "Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.",
    "arxiv_url": "https://arxiv.org/abs/2511.17027",
    "authors": [
      "Zhijie Chen",
      "Xiang Chen",
      "Ziming Li",
      "Jiacheng Xue",
      "Chaoyang Gao"
    ],
    "first_author": "Zhijie Chen",
    "primary_category": "cs.SE",
    "tag": [
      "Code Prompting"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.17027v1",
    "published": "2025-11-21",
    "update_time": "2025-11-21",
    "download_time": "2025-11-24 16:02:30"
  },
  {
    "id": "2511.16858",
    "title": "Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair",
    "abstract": "Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.",
    "arxiv_url": "https://arxiv.org/abs/2511.16858",
    "authors": [
      "Toufique Ahmed",
      "Jatin Ganhotra",
      "Avraham Shinnar",
      "Martin Hirzel"
    ],
    "first_author": "Toufique Ahmed",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16858v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-11-24 16:02:47"
  },
  {
    "id": "2511.16787",
    "title": "NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction to Python Code Generation",
    "abstract": "This paper presents JGU Mainz's winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions. We propose a multi-agent-based pipeline. First, a code-generation agent produces an initial solution from the input instruction. The candidate program is then executed against the provided unit tests (pytest-style, assert-based). Only the failing cases are forwarded to a debugger agent, which reruns the tests, extracts error traces, and, conditioning on the error messages, the current program, and the relevant test cases, generates a revised solution. Using this approach, our submission achieved first place in the shared task with a $Pass@1$ score of 95.4. We also make our code public.",
    "arxiv_url": "https://arxiv.org/abs/2511.16787",
    "authors": [
      "Hossain Shaikh Saadi",
      "Faria Alam",
      "Mario Sanz-Guerrero",
      "Minh Duc Bui",
      "Manuel Mager",
      "Katharina von der Wense"
    ],
    "first_author": "Hossain Shaikh Saadi",
    "primary_category": "cs.CL",
    "tag": [
      "Code Translation"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16787v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-11-24 16:02:59"
  },
  {
    "id": "2511.16395",
    "title": "CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs). However, their inherent tendency toward hallucination often introduces functional errors into the generated HDL designs. To address this issue, we propose the framework CorrectHDL that leverages high-level synthesis (HLS) results as functional references to correct potential errors in LLM-generated HDL designs.The input to the proposed framework is a C/C++ program that specifies the target circuit's functionality. The program is provided to an LLM to directly generate an HDL design, whose syntax errors are repaired using a Retrieval-Augmented Generation (RAG) mechanism. The functional correctness of the LLM-generated circuit is iteratively improved by comparing its simulated behavior with an HLS reference design produced by conventional HLS tools, which ensures the functional correctness of the result but can lead to suboptimal area and power efficiency. Experimental results demonstrate that circuits generated by the proposed framework achieve significantly better area and power efficiency than conventional HLS designs and approach the quality of human-engineered circuits. Meanwhile, the correctness of the resulting HDL implementation is maintained, highlighting the effectiveness and potential of agentic HDL design leveraging the generative capabilities of LLMs and the rigor of traditional correctness-driven IC design flows.",
    "arxiv_url": "https://arxiv.org/abs/2511.16395",
    "authors": [
      "Kangwei Xu",
      "Grace Li Zhang",
      "Ulf Schlichtmann",
      "Bing Li"
    ],
    "first_author": "Kangwei Xu",
    "primary_category": "cs.AI",
    "tag": [
      "Code Translation"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16395v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-11-24 16:03:33"
  },
  {
    "id": "2511.16383",
    "title": "An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models",
    "abstract": "Recently, using Large Language Models (LLMs) to generate optimization models from natural language descriptions has became increasingly popular. However, a major open question is how to validate that the generated models are correct and satisfy the requirements defined in the natural language description. In this work, we propose a novel agent-based method for automatic validation of optimization models that builds upon and extends methods from software testing to address optimization modeling . This method consists of several agents that initially generate a problem-level testing API, then generate tests utilizing this API, and, lastly, generate mutations specific to the optimization model (a well-known software testing technique assessing the fault detection power of the test suite). In this work, we detail this validation framework and show, through experiments, the high quality of validation provided by this agent ensemble in terms of the well-known software testing measure called mutation coverage.",
    "arxiv_url": "https://arxiv.org/abs/2511.16383",
    "authors": [
      "Alexander Zadorojniy",
      "Segev Wasserkrug",
      "Eitan Farchi"
    ],
    "first_author": "Alexander Zadorojniy",
    "primary_category": "cs.AI",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16383v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-11-24 16:03:44"
  },
  {
    "id": "2511.16224",
    "title": "Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts",
    "abstract": "Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.",
    "arxiv_url": "https://arxiv.org/abs/2511.16224",
    "authors": [
      "Francesco Salzano",
      "Simone Scalabrino",
      "Rocco Oliveto",
      "Remo Pareschi"
    ],
    "first_author": "Francesco Salzano",
    "primary_category": "cs.SE",
    "tag": [
      "Code Completion"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16224v2",
    "published": "2025-11-20",
    "update_time": "2025-11-21",
    "download_time": "2025-11-24 16:03:59"
  },
  {
    "id": "2511.16123",
    "title": "Domain-constrained Synthesis of Inconsistent Key Aspects in Textual Vulnerability Descriptions",
    "abstract": "Textual Vulnerability Descriptions (TVDs) are crucial for security analysts to understand and address software vulnerabilities. However, the key aspect inconsistencies in TVDs from different repositories pose challenges for achieving a comprehensive understanding of vulnerabilities. Existing approaches aim to mitigate inconsistencies by aligning TVDs with external knowledge bases, but they often discard valuable information and fail to synthesize comprehensive representations. In this paper, we propose a domain-constrained LLM-based synthesis framework for unifying key aspects of TVDs. Our framework consists of three stages: 1) Extraction, guided by rule-based templates to ensure all critical details are captured; 2) Self-evaluation, using domain-specific anchor words to assess semantic variability across sources; and 3) Fusion, leveraging information entropy to reconcile inconsistencies and prioritize relevant details. This framework improves synthesis performance, increasing the F1 score for key aspect augmentation from 0.82 to 0.87, while enhancing comprehension and efficiency by over 30\\%. We further develop Digest Labels, a practical tool for visualizing TVDs, which human evaluations show significantly boosts usability.",
    "arxiv_url": "https://arxiv.org/abs/2511.16123",
    "authors": [
      "Linyi Han",
      "Shidong Pan",
      "Zhenchang Xing",
      "Sofonias Yitagesu",
      "Xiaowang Zhang",
      "Zhiyong Feng",
      "Jiamou Sun",
      "Qing Huang"
    ],
    "first_author": "Linyi Han",
    "primary_category": "cs.SE",
    "tag": [
      "Code Summarization"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16123v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-11-24 16:04:14"
  },
  {
    "id": "2511.16092",
    "title": "The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report",
    "abstract": "Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report",
    "arxiv_url": "https://arxiv.org/abs/2511.16092",
    "authors": [
      "Xing Hu",
      "Raula Gaikovina Kula",
      "Christoph Treude"
    ],
    "first_author": "Xing Hu",
    "primary_category": "cs.SE",
    "tag": [
      "AI-Assisted Development Environments"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16092v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-11-24 16:04:23"
  },
  {
    "id": "2511.16708",
    "title": "Multi-Agent Code Verification with Compound Vulnerability Detection",
    "abstract": "LLMs generate buggy code: 29.6% of SWE-bench \"solved\" patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, confirmed by measuring agent correlation of p = 0.05--0.25. We also show that multiple vulnerabilities in the same code create exponentially more risk than previously thought--SQL injection plus exposed credentials creates 15x more danger (risk 300 vs. 20) than traditional models predict. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution. We tested 15 different agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with gains of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4. The best two-agent combination reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.",
    "arxiv_url": "https://arxiv.org/abs/2511.16708",
    "authors": [
      "Shreshth Rajan"
    ],
    "first_author": "Shreshth Rajan",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16708v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-11-24 16:04:36"
  },
  {
    "id": "2511.16005",
    "title": "InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution",
    "abstract": "Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \\texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.",
    "arxiv_url": "https://arxiv.org/abs/2511.16005",
    "authors": [
      "Qingao Dong",
      "Mengfei Wang",
      "Hengzhi Zhang",
      "Zhichao Li",
      "Yuan Yuan",
      "Mu Li",
      "Xiang Gao",
      "Hailong Sun",
      "Chunming Hu",
      "Weifeng Lv"
    ],
    "first_author": "Qingao Dong",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16005v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-11-24 16:04:48"
  },
  {
    "id": "2511.16004",
    "title": "InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution",
    "abstract": "Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.",
    "arxiv_url": "https://arxiv.org/abs/2511.16004",
    "authors": [
      "KeFan Li",
      "Mengfei Wang",
      "Hengzhi Zhang",
      "Zhichao Li",
      "Yuan Yuan",
      "Mu Li",
      "Xiang Gao",
      "Hailong Sun",
      "Chunming Hu",
      "Weifeng Lv"
    ],
    "first_author": "KeFan Li",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16004v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-11-24 16:04:59"
  },
  {
    "id": "2511.15817",
    "title": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code",
    "abstract": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.   This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.",
    "arxiv_url": "https://arxiv.org/abs/2511.15817",
    "authors": [
      "Alejandro Velasco",
      "Daniel Rodriguez-Cardenas",
      "Dipin Khati",
      "David N. Palacio",
      "Luftar Rahman Alif",
      "Denys Poshyvanyk"
    ],
    "first_author": "Alejandro Velasco",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.15817v2",
    "published": "2025-11-19",
    "update_time": "2025-11-21",
    "download_time": "2025-11-24 16:05:36"
  },
  {
    "id": "2511.15665",
    "title": "Quantum-Guided Test Case Minimization for LLM-Based Code Generation",
    "abstract": "Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.",
    "arxiv_url": "https://arxiv.org/abs/2511.15665",
    "authors": [
      "Huixiang Zhang",
      "Mahzabeen Emu"
    ],
    "first_author": "Huixiang Zhang",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": "This is a preprint version, full paper has been accepted in IEEE CASCON 2025 and...",
    "pdf_url": "https://arxiv.org/pdf/2511.15665v1",
    "published": "2025-11-19",
    "update_time": "2025-11-19",
    "download_time": "2025-11-24 16:05:46"
  },
  {
    "id": "2511.15403",
    "title": "MutDafny: A Mutation-Based Approach to Assess Dafny Specifications",
    "abstract": "This paper explores the use of mutation testing to reveal weaknesses in formal specifications written in Dafny. In verification-aware programming languages, such as Dafny, despite their critical role, specifications are as prone to errors as implementations. Flaws in specs can result in formally verified programs that deviate from the intended behavior.   We present MutDafny, a tool that increases the reliability of Dafny specifications by automatically signaling potential weaknesses. Using a mutation testing approach, we introduce faults (mutations) into the code and rely on formal specifications for detecting them. If a program with a mutant verifies, this may indicate a weakness in the specification. We extensively analyze mutation operators from popular tools, identifying the ones applicable to Dafny. In addition, we synthesize new operators tailored for Dafny from bugfix commits in publicly available Dafny projects on GitHub. Drawing from both, we equipped our tool with a total of 32 mutation operators. We evaluate MutDafny's effectiveness and efficiency in a dataset of 794 real-world Dafny programs and we manually analyze a subset of the resulting undetected mutants, identifying five weak real-world specifications (on average, one at every 241 lines of code) that would benefit from strengthening.",
    "arxiv_url": "https://arxiv.org/abs/2511.15403",
    "authors": [
      "Isabel Amaral",
      "Alexandra Mendes",
      "Jos√© Campos"
    ],
    "first_author": "Isabel Amaral",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.15403v1",
    "published": "2025-11-19",
    "update_time": "2025-11-19",
    "download_time": "2025-11-24 16:06:15"
  },
  {
    "id": "2511.15293",
    "title": "A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development",
    "abstract": "Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.",
    "arxiv_url": "https://arxiv.org/abs/2511.15293",
    "authors": [
      "Jia Li",
      "Zhi Jin",
      "Kechi Zhang",
      "Huangzhao Zhang",
      "Jiaru Qian",
      "Tiankuo Zhao"
    ],
    "first_author": "Jia Li",
    "primary_category": "cs.SE",
    "tag": [
      "Automated Software Development"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.15293v1",
    "published": "2025-11-19",
    "update_time": "2025-11-19",
    "download_time": "2025-11-24 16:06:36"
  },
  {
    "id": "2511.15757",
    "title": "Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym",
    "abstract": "Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on userspace applications and overlook the complexities of kernel-space debugging and repair. The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions. Prior efforts such as KGym and CrashFixer have highlighted the difficulty of APR in this domain, reporting low success rates or relying on costly and complex pipelines and pricey cloud infrastructure. In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. Built on RGym, we propose a simple yet effective APR pipeline leveraging specialized localization techniques (e.g., call stacks and blamed commits) to overcome the unrealistic usage of oracles in KGym. We test on a filtered and verified dataset of 143 bugs. Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug. We further conduct an ablation study to analyze contributions from our proposed localization strategy, prompt structure, and model choice, and demonstrate that feedback-based retries can significantly enhance success rates.",
    "arxiv_url": "https://arxiv.org/abs/2511.15757",
    "authors": [
      "Kareem Shehada",
      "Yifan Wu",
      "Wyatt D. Feng",
      "Adithya Iyer",
      "Gryphon Kumfert",
      "Yangruibo Ding",
      "Zhiyun Qian"
    ],
    "first_author": "Kareem Shehada",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": true,
    "conference": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.15757v1",
    "published": "2025-11-19",
    "update_time": "2025-11-19",
    "download_time": "2025-11-24 16:06:58"
  },
  {
    "id": "2511.15229",
    "title": "From Code Smells to Best Practices: Tackling Resource Leaks in PyTorch, TensorFlow, and Keras",
    "abstract": "Much of the existing ML research focuses on model performance metrics, leaving limited attention to the long-term sustainability and resource efficiency of ML applications. While high performance is essential, ensuring efficient resource management is equally critical for robust deployment. This study addresses this gap by systematically identifying code smells that lead to resource leaks in ML applications. We conducted an empirical investigation of developer discussions and real-world code snippets from PyTorch, TensorFlow, and Keras. The analysis identified 30 PyTorch-related smells and 16 TensorFlow/Keras smells linked to resource leaks. These smells were categorized in two ways: (1) based on their root causes, and (2) as general ML smells with framework-specific characteristics. For each smell, we derived at least one best practice, resulting in 50 recommended coding patterns aimed at reducing resource leakage and improving efficiency. To ensure the validity of our findings, we employed a three-phase validation process involving independent analysis by three authors followed by consensus discussions. This is the first comprehensive study to examine resource-leak-inducing code smells across major ML frameworks and to present actionable best practices for mitigating them. The contributions support developers in building more efficient and sustainable ML applications and offer a structured view of the underlying causes of resource leaks.",
    "arxiv_url": "https://arxiv.org/abs/2511.15229",
    "authors": [
      "Bashar Abdallah",
      "Martyna E. Wojciechowska",
      "Gustavo Santos",
      "Edmand Yu",
      "Maxime Lamothe",
      "Alain Abran",
      "Mohammad Hamdaqa"
    ],
    "first_author": "Bashar Abdallah",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.15229v1",
    "published": "2025-11-19",
    "update_time": "2025-11-19",
    "download_time": "2025-11-24 16:07:09"
  },
  {
    "id": "2511.17417",
    "title": "CREST: Improving Interpretability and Effectiveness of Troubleshooting at Ericsson through Criterion-Specific Trouble Report Retrieval",
    "abstract": "The rapid evolution of the telecommunication industry necessitates efficient troubleshooting processes to maintain network reliability, software maintainability, and service quality. Trouble Reports (TRs), which document issues in Ericsson's production system, play a critical role in facilitating the timely resolution of software faults. However, the complexity and volume of TR data, along with the presence of diverse criteria that reflect different aspects of each fault, present challenges for retrieval systems. Building on prior work at Ericsson, which utilized a two-stage workflow, comprising Initial Retrieval (IR) and Re-Ranking (RR) stages, this study investigates different TR observation criteria and their impact on the performance of retrieval models. We propose \\textbf{CREST} (\\textbf{C}riteria-specific \\textbf{R}etrieval via \\textbf{E}nsemble of \\textbf{S}pecialized \\textbf{T}R models), a criterion-driven retrieval approach that leverages specialized models for different TR fields to improve both effectiveness and interpretability, thereby enabling quicker fault resolution and supporting software maintenance. CREST utilizes specialized models trained on specific TR criteria and aggregates their outputs to capture diverse and complementary signals. This approach leads to enhanced retrieval accuracy, better calibration of predicted scores, and improved interpretability by providing relevance scores for each criterion, helping users understand why specific TRs were retrieved. Using a subset of Ericsson's internal TRs, this research demonstrates that criterion-specific models significantly outperform a single model approach across key evaluation metrics. This highlights the importance of all targeted criteria used in this study for optimizing the performance of retrieval systems.",
    "arxiv_url": "https://arxiv.org/abs/2511.17417",
    "authors": [
      "Soroush Javdan",
      "Pragash Krishnamoorthy",
      "Olga Baysal"
    ],
    "first_author": "Soroush Javdan",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.17417v1",
    "published": "2025-11-21",
    "update_time": "2025-11-21",
    "download_time": "2025-11-24 09:54:12"
  },
  {
    "id": "2511.17131",
    "title": "UI-CUBE: Enterprise-Grade Computer Use Agent Benchmarking Beyond Task Accuracy to Operational Reliability",
    "abstract": "While current Computer Use Agent (CUA) benchmarks measure task completion effectively, they provide limited assessment of enterprise deployment readiness, emphasizing functional correctness over the operational reliability required for production systems. We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs. Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks), with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state. Evaluation of five state-of-the-art models reveals a sharp capability cliff rather than gradual performance degradation. Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%. Human evaluators with no prior application experience achieve only 61.2% on complex tasks despite near-perfect performance on simple tasks, establishing realistic performance ceilings. This discontinuous performance pattern -- where agents achieve 68-87% of human performance on simple tasks but only 15-32% on complex workflows -- indicates fundamental architectural limitations in memory management, hierarchical planning, and state coordination rather than incremental capability gaps addressable through better training or prompting. UI-CUBE functions as an enterprise-readiness diagnostic, revealing that while current CUAs can manipulate individual interface elements, they cannot yet function as reliable workflow automation tools. These findings provide architectural insights essential for developing production-ready CUAs capable of managing complex, multi-step enterprise processes.",
    "arxiv_url": "https://arxiv.org/abs/2511.17131",
    "authors": [
      "Horia Cristescu",
      "Charles Park",
      "Trong Canh Nguyen",
      "Sergiu Talmacel",
      "Alexandru-Gabriel Ilie",
      "Stefan Adam"
    ],
    "first_author": "Horia Cristescu",
    "primary_category": "cs.SE",
    "tag": [
      "Agentic UI Automation Evaluation"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.17131v1",
    "published": "2025-11-21",
    "update_time": "2025-11-21",
    "download_time": "2025-11-24 09:55:38"
  },
  {
    "id": "2511.15168",
    "title": "Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework",
    "abstract": "Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.",
    "arxiv_url": "https://arxiv.org/abs/2511.15168",
    "authors": [
      "Nguyen-Khang Le",
      "Hiep Nguyen",
      "Ngoc-Minh Nguyen",
      "Son T. Luu",
      "Trung Vo",
      "Quan Minh Bui",
      "Shoshin Nomura",
      "Le-Minh Nguyen"
    ],
    "first_author": "Nguyen-Khang Le",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": true,
    "conference": "Proceedings of KSE 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.15168v2",
    "published": "2025-11-19",
    "update_time": "2025-11-20",
    "download_time": "2025-11-25 01:01:11"
  },
  {
    "id": "2511.15755",
    "title": "Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response",
    "abstract": "Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.",
    "arxiv_url": "https://arxiv.org/abs/2511.15755",
    "authors": [
      "Philip Drammeh"
    ],
    "first_author": "Philip Drammeh",
    "primary_category": "cs.AI",
    "tag": [
      "Code Debug"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.15755v1",
    "published": "2025-11-19",
    "update_time": "2025-11-19",
    "download_time": "2025-11-25 01:01:22"
  }
]