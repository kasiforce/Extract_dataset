[
  {
    "id": "2512.01939",
    "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks",
    "abstract": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.",
    "arxiv_url": "https://arxiv.org/abs/2512.01939",
    "authors": [
      "Yanlin Wang",
      "Xinyi Xu",
      "Jiachi Chen",
      "Tingting Bi",
      "Wenchao Gu",
      "Zibin Zheng"
    ],
    "first_author": "Yanlin Wang",
    "primary_category": "cs.SE",
    "tag": [
      "Agent Frameworks & Developer Practices"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.01939v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-03 00:57:19"
  },
  {
    "id": "2512.02953",
    "title": "The Evolutionary Ecology of Software: Constraints, Innovation, and the AI Disruption",
    "abstract": "This chapter investigates the evolutionary ecology of software, focusing on the symbiotic relationship between software and innovation. An interplay between constraints, tinkering, and frequency-dependent selection drives the complex evolutionary trajectories of these socio-technological systems. Our approach integrates agent-based modeling and case studies, drawing on complex network analysis and evolutionary theory to explore how software evolves under the competing forces of novelty generation and imitation. By examining the evolution of programming languages and their impact on developer practices, we illustrate how technological artifacts co-evolve with and shape societal norms, cultural dynamics, and human interactions. This ecological perspective also informs our analysis of the emerging role of AI-driven development tools in software evolution. While large language models (LLMs) provide unprecedented access to information, their widespread adoption introduces new evolutionary pressures that may contribute to cultural stagnation, much like the decline of diversity in past software ecosystems. Understanding the evolutionary pressures introduced by AI-mediated software production is critical for anticipating broader patterns of cultural change, technological adaptation, and the future of software innovation.",
    "arxiv_url": "https://arxiv.org/abs/2512.02953",
    "authors": [
      "Sergi Valverde",
      "Blai Vidiella",
      "Salva Duran-Nebreda"
    ],
    "first_author": "Sergi Valverde",
    "primary_category": "cs.SE",
    "tag": [
      "AI-mediated Software Evolution"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.02953v1",
    "published": "2025-12-02",
    "update_time": "2025-12-02",
    "download_time": "2025-12-04 00:57:36"
  },
  {
    "id": "2512.03421",
    "title": "Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization",
    "abstract": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.",
    "arxiv_url": "https://arxiv.org/abs/2512.03421",
    "authors": [
      "Hexiang Xu",
      "Hengyuan Liu",
      "Yonghao Wu",
      "Xiaolan Kang",
      "Xiang Chen",
      "Yong Liu"
    ],
    "first_author": "Hexiang Xu",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": true,
    "conference": "publication in The Journal of Systems & Software",
    "pdf_url": "https://arxiv.org/pdf/2512.03421v1",
    "published": "2025-12-03",
    "update_time": "2025-12-03",
    "download_time": "2025-12-05 00:58:23"
  },
  {
    "id": "2512.05073",
    "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?",
    "abstract": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.05073",
    "authors": [
      "Shashwat Shankar",
      "Subhranshu Pandey",
      "Innocent Dengkhw Mochahari",
      "Bhabesh Mali",
      "Animesh Basak Chowdhury",
      "Sukanta Bhattacharjee",
      "Chandan Karfa"
    ],
    "first_author": "Shashwat Shankar",
    "primary_category": "cs.LG",
    "tag": [
      "Code Prompting"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.05073v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-06 00:55:11"
  },
  {
    "id": "2512.04680",
    "title": "Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap",
    "abstract": "Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.",
    "arxiv_url": "https://arxiv.org/abs/2512.04680",
    "authors": [
      "Jialong Li",
      "Mingyue Zhang",
      "Nianyu Li",
      "Danny Weyns",
      "Zhi Jin",
      "Kenji Tei"
    ],
    "first_author": "Jialong Li",
    "primary_category": "cs.SE",
    "tag": [
      "GenAI for Self-Adaptive Systems"
    ],
    "benchmark": false,
    "conference": "ACM Transactions on Autonomous and Adaptive Systems",
    "pdf_url": "https://arxiv.org/pdf/2512.04680v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-07 01:03:43"
  },
  {
    "id": "2512.04673",
    "title": "Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models",
    "abstract": "Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.",
    "arxiv_url": "https://arxiv.org/abs/2512.04673",
    "authors": [
      "Gunjan Das",
      "Paheli Bhattacharya",
      "Rishabh Gupta"
    ],
    "first_author": "Gunjan Das",
    "primary_category": "cs.SE",
    "tag": [
      "Cross-Task Benchmarking and Evaluation"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04673v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-08 00:58:57"
  },
  {
    "id": "2512.05908",
    "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures",
    "abstract": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.",
    "arxiv_url": "https://arxiv.org/abs/2512.05908",
    "authors": [
      "Amirkia Rafiei Oskooei",
      "S. Selcan Yukcu",
      "Mehmet Cevheri Bozoglan",
      "Mehmet S. Aktas"
    ],
    "first_author": "Amirkia Rafiei Oskooei",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": "LLM4Code Workshop",
    "pdf_url": "https://arxiv.org/pdf/2512.05908v1",
    "published": "2025-12-05",
    "update_time": "2025-12-05",
    "download_time": "2025-12-09 00:58:03"
  },
  {
    "id": "2512.07814",
    "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach",
    "abstract": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.",
    "arxiv_url": "https://arxiv.org/abs/2512.07814",
    "authors": [
      "Hua Yang",
      "Alejandro Velasco",
      "Sen Fang",
      "Bowen Xu",
      "Denys Poshyvanyk"
    ],
    "first_author": "Hua Yang",
    "primary_category": "cs.SE",
    "tag": [
      "Code Pre-Training"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.07814v1",
    "published": "2025-12-08",
    "update_time": "2025-12-08",
    "download_time": "2025-12-10 00:59:23"
  },
  {
    "id": "2512.08867",
    "title": "SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA",
    "abstract": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.",
    "arxiv_url": "https://arxiv.org/abs/2512.08867",
    "authors": [
      "Jing Zhang",
      "Lianghong Guo",
      "Yanlin Wang",
      "Mingwei Liu",
      "Jiachi Chen",
      "Yuchi Ma",
      "Ensheng Shi",
      "Terry Yue Zhuo",
      "Hongyu Zhang",
      "Zibin Zheng"
    ],
    "first_author": "Jing Zhang",
    "primary_category": "cs.SE",
    "tag": [
      "Development Knowledge QA"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.08867v1",
    "published": "2025-12-09",
    "update_time": "2025-12-09",
    "download_time": "2025-12-11 00:59:49"
  },
  {
    "id": "2512.09679",
    "title": "Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis",
    "abstract": "Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \\emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.",
    "arxiv_url": "https://arxiv.org/abs/2512.09679",
    "authors": [
      "Naizhu Jin",
      "Zhong Li",
      "Guang Yang",
      "Tian Zhang",
      "Qingkai Zeng"
    ],
    "first_author": "Naizhu Jin",
    "primary_category": "cs.SE",
    "tag": [
      "Code Prompting"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.09679v1",
    "published": "2025-12-10",
    "update_time": "2025-12-10",
    "download_time": "2025-12-12 00:59:30"
  },
  {
    "id": "2512.10713",
    "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code",
    "abstract": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.10713",
    "authors": [
      "Itay Dreyfuss",
      "Antonio Abu Nassar",
      "Samuel Ackerman",
      "Axel Ben David",
      "Rami Katan",
      "Orna Raz",
      "Marcel Zalmanovici"
    ],
    "first_author": "Itay Dreyfuss",
    "primary_category": "cs.SE",
    "tag": [
      "Code Evaluation/Benchmarking"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10713v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-13 00:56:08"
  },
  {
    "id": "2512.10493",
    "title": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild",
    "abstract": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.",
    "arxiv_url": "https://arxiv.org/abs/2512.10493",
    "authors": [
      "Binquan Zhang",
      "Li Zhang",
      "Haoyuan Zhang",
      "Fang Liu",
      "Song Wang",
      "Bo Shen",
      "An Fu",
      "Lin Shi"
    ],
    "first_author": "Binquan Zhang",
    "primary_category": "cs.SE",
    "tag": [
      "Code Prompting"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10493v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-14 01:03:59"
  },
  {
    "id": "2512.10485",
    "title": "From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection",
    "abstract": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.",
    "arxiv_url": "https://arxiv.org/abs/2512.10485",
    "authors": [
      "Chaomeng Lu",
      "Bert Lagaisse"
    ],
    "first_author": "Chaomeng Lu",
    "primary_category": "cs.CR",
    "tag": [
      "Code Testing"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10485v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-15 01:02:58"
  },
  {
    "id": "2512.11589",
    "title": "A Study of Library Usage in Agent-Authored Pull Requests",
    "abstract": "Coding agents are becoming increasingly capable of completing end-to-end software engineering workflows that previously required a human developer, including raising pull requests (PRs) to propose their changes. However, we still know little about how these agents use libraries when generating code, a core part of real-world software development. To fill this gap, we study 26,760 agent-authored PRs from the AIDev dataset to examine three questions: how often do agents import libraries, how often do they introduce new dependencies (and with what versioning), and which specific libraries do they choose? We find that agents often import libraries (29.5% of PRs) but rarely add new dependencies (1.3% of PRs); and when they do, they follow strong versioning practices (75.0% specify a version), an improvement on direct LLM usage where versions are rarely mentioned. Generally, agents draw from a surprisingly diverse set of external libraries, contrasting with the limited \"library preferences\" seen in prior non-agentic LLM studies. Our results offer an early empirical view into how AI coding agents interact with today's software ecosystems.",
    "arxiv_url": "https://arxiv.org/abs/2512.11589",
    "authors": [
      "Lukas Twist"
    ],
    "first_author": "Lukas Twist",
    "primary_category": "cs.SE",
    "tag": [
      "Agentic Code Generation"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.11589v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 01:00:01"
  },
  {
    "id": "2512.13515",
    "title": "Fine-tuned LLM-based Code Migration Framework",
    "abstract": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2512.13515",
    "authors": [
      "Oleg Grynets",
      "Vasyl Lyashkevych",
      "Dmytro Baran",
      "Maksym Orliansky",
      "Taras Zelenyy",
      "Markiian Leshchyshyn"
    ],
    "first_author": "Oleg Grynets",
    "primary_category": "cs.SE",
    "tag": [
      "Code Translation"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.13515v1",
    "published": "2025-12-15",
    "update_time": "2025-12-15",
    "download_time": "2025-12-17 00:55:40"
  },
  {
    "id": "2512.14475",
    "title": "Teralizer: Semantics-Based Test Generalization from Conventional Unit Tests to Property-Based Tests",
    "abstract": "Conventional unit tests validate single input-output pairs, leaving most inputs of an execution path untested. Property-based testing addresses this shortcoming by generating multiple inputs satisfying properties but requires significant manual effort to define properties and their constraints. We propose a semantics-based approach that automatically transforms unit tests into property-based tests by extracting specifications from implementations via single-path symbolic analysis. We demonstrate this approach through Teralizer, a prototype for Java that transforms JUnit tests into property-based jqwik tests. Unlike prior work that generalizes from input-output examples, Teralizer derives specifications from program semantics.   We evaluated Teralizer on three progressively challenging datasets. On EvoSuite-generated tests for EqBench and Apache Commons utilities, Teralizer improved mutation scores by 1-4 percentage points. Generalization of mature developer-written tests from Apache Commons utilities showed only 0.05-0.07 percentage points improvement. Analysis of 632 real-world Java projects from RepoReapers highlights applicability barriers: only 1.7% of projects completed the generalization pipeline, with failures primarily due to type support limitations in symbolic analysis and static analysis limitations in our prototype. Based on the results, we provide a roadmap for future work, identifying research and engineering challenges that need to be tackled to advance the field of test generalization.   Artifacts available at: https://doi.org/10.5281/zenodo.17950381",
    "arxiv_url": "https://arxiv.org/abs/2512.14475",
    "authors": [
      "Johann Glock",
      "Clemens Bauer",
      "Martin Pinzger"
    ],
    "first_author": "Johann Glock",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.14475v1",
    "published": "2025-12-16",
    "update_time": "2025-12-16",
    "download_time": "2025-12-18 00:56:41"
  },
  {
    "id": "2512.15699",
    "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
    "abstract": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.",
    "arxiv_url": "https://arxiv.org/abs/2512.15699",
    "authors": [
      "Qiuyang Mang",
      "Wenhao Chai",
      "Zhifei Li",
      "Huanzhi Mao",
      "Shang Zhou",
      "Alexander Du",
      "Hanchen Li",
      "Shu Liu",
      "Edwin Chen",
      "Yichuan Wang",
      "Xieting Chu",
      "Zerui Cheng",
      "Yuan Xu",
      "Tian Xia",
      "Zirui Wang",
      "Tianneng Shi",
      "Jianzhu Yao",
      "Yilong Zhao",
      "Qizheng Zhang",
      "Charlie Ruan",
      "Zeyu Shen",
      "Kaiyuan Liu",
      "Runyuan He",
      "Dong Xing",
      "Zerui Li",
      "Zirong Zeng",
      "Yige Jiang",
      "Lufeng Cheng",
      "Ziyi Zhao",
      "Youran Sun",
      "Wesley Zheng",
      "Meiyuwang Zhang",
      "Ruyi Ji",
      "Xuechang Tu",
      "Zihan Zheng",
      "Zexing Chen",
      "Kangyang Zhou",
      "Zhaozi Wang",
      "Jingbang Chen",
      "Aleksandra Korolova",
      "Peter Henderson",
      "Pramod Viswanath",
      "Vijay Ganesh",
      "Saining Xie",
      "Zhuang Liu",
      "Dawn Song",
      "Sewon Min",
      "Ion Stoica",
      "Joseph E. Gonzalez",
      "Jingbo Shang",
      "Alvin Cheung"
    ],
    "first_author": "Qiuyang Mang",
    "primary_category": "cs.LG",
    "tag": [
      "Code Benchmarking"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.15699v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-19 00:59:23"
  },
  {
    "id": "2512.16790",
    "title": "Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse",
    "abstract": "While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.",
    "arxiv_url": "https://arxiv.org/abs/2512.16790",
    "authors": [
      "Aaron Imani",
      "Mohammad Moshirpour",
      "Iftekhar Ahmed"
    ],
    "first_author": "Aaron Imani",
    "primary_category": "cs.SE",
    "tag": [
      "Code Interpretability"
    ],
    "benchmark": false,
    "conference": "ICSE",
    "pdf_url": "https://arxiv.org/pdf/2512.16790v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-20 00:56:12"
  },
  {
    "id": "2512.16741",
    "title": "An Empirical Study of the Realism of Mutants in Deep Learning",
    "abstract": "Mutation analysis is a well-established technique for assessing test quality in the traditional software development paradigm by injecting artificial faults into programs. Its application to deep learning (DL) has expanded beyond classical testing to support tasks such as fault localization, repair, data generation, and model robustness evaluation. The core assumption is that mutants behave similarly to real faults, an assumption well established in traditional software systems but largely unverified for DL.   This study presents the first empirical comparison of pre-training and post-training mutation approaches in DL with respect to realism. We introduce a statistical framework to quantify their coupling strength and behavioral similarity to real faults using publicly available bugs datasets: CleanML, DeepFD, DeepLocalize, and defect4ML. Mutants are generated using state-of-the-art tools representing both approaches.   Results show that pre-training mutants exhibit consistently stronger coupling and higher behavioral similarity to real faults than post-training mutants, indicating greater realism. However, the substantial computational cost of pre-training mutation underscores the need for more effective post-training operators that match or exceed the realism demonstrated by pre-training mutants.",
    "arxiv_url": "https://arxiv.org/abs/2512.16741",
    "authors": [
      "Zaheed Ahmed",
      "Philip Makedonski",
      "Jens Grabowski"
    ],
    "first_author": "Zaheed Ahmed",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16741v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-21 01:03:33"
  },
  {
    "id": "2512.16272",
    "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls",
    "abstract": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.   To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.   Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.",
    "arxiv_url": "https://arxiv.org/abs/2512.16272",
    "authors": [
      "Ora Nova Fandina",
      "Eitan Farchi",
      "Shmulik Froimovich",
      "Raviv Gal",
      "Wesam Ibraheem",
      "Rami Katan",
      "Alice Podolsky"
    ],
    "first_author": "Ora Nova Fandina",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16272v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-22 01:02:53"
  },
  {
    "id": "2512.17814",
    "title": "LLM-based Behaviour Driven Development for Hardware Design",
    "abstract": "Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.   Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.",
    "arxiv_url": "https://arxiv.org/abs/2512.17814",
    "authors": [
      "Rolf Drechsler",
      "Qian Liu"
    ],
    "first_author": "Rolf Drechsler",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.17814v1",
    "published": "2025-12-19",
    "update_time": "2025-12-19",
    "download_time": "2025-12-23 00:59:13"
  },
  {
    "id": "2512.19644",
    "title": "More code, less validation: Risk factors for over-reliance on AI coding tools among scientists",
    "abstract": "Programming is essential to modern scientific research, yet most scientists report inadequate training for the software development their work demands. Generative AI tools capable of code generation may support scientific programmers, but user studies indicate risks of over-reliance, particularly among inexperienced users. We surveyed 868 scientists who program, examining adoption patterns, tool preferences, and factors associated with perceived productivity. Adoption is highest among students and less experienced programmers, with variation across fields. Scientific programmers overwhelmingly prefer general-purpose conversational interfaces like ChatGPT over developer-specific tools. Both inexperience and limited use of development practices (like testing, code review, and version control) are associated with greater perceived productivity-but these factors interact, suggesting formal practices may partially compensate for inexperience. The strongest predictor of perceived productivity is the number of lines of generated code typically accepted at once. These findings suggest scientific programmers using generative AI may gauge productivity by code generation rather than validation, raising concerns about research code integrity.",
    "arxiv_url": "https://arxiv.org/abs/2512.19644",
    "authors": [
      "Gabrielle O'Brien",
      "Alexis Parker",
      "Nasir Eisty",
      "Jeffrey Carver"
    ],
    "first_author": "Gabrielle O'Brien",
    "primary_category": "cs.SE",
    "tag": [
      "Code Testing"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.19644v1",
    "published": "2025-12-22",
    "update_time": "2025-12-22",
    "download_time": "2025-12-24 00:58:43"
  },
  {
    "id": "2512.20482",
    "title": "SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization",
    "abstract": "Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.",
    "arxiv_url": "https://arxiv.org/abs/2512.20482",
    "authors": [
      "Revanth Gangi Reddy",
      "Ye Liu",
      "Wenting Zhao",
      "JaeHyeok Doo",
      "Tarun Suresh",
      "Daniel Lee",
      "Caiming Xiong",
      "Yingbo Zhou",
      "Semih Yavuz",
      "Shafiq Joty"
    ],
    "first_author": "Revanth Gangi Reddy",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.20482v1",
    "published": "2025-12-23",
    "update_time": "2025-12-23",
    "download_time": "2025-12-25 00:59:23"
  },
  {
    "id": "2512.21238",
    "title": "Assessing the Software Security Comprehension of Large Language Models",
    "abstract": "Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.",
    "arxiv_url": "https://arxiv.org/abs/2512.21238",
    "authors": [
      "Mohammed Latif Siddiq",
      "Natalie Sekerak",
      "Antonio Karam",
      "Maria Leal",
      "Arvin Islam-Gomes",
      "Joanna C. S. Santos"
    ],
    "first_author": "Mohammed Latif Siddiq",
    "primary_category": "cs.SE",
    "tag": [
      "Code Security Evaluation"
    ],
    "benchmark": false,
    "conference": "Submitted to Empirical Software Engineering (EMSE) journal",
    "pdf_url": "https://arxiv.org/pdf/2512.21238v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-26 00:59:49"
  },
  {
    "id": "2512.21236",
    "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking",
    "abstract": "Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.",
    "arxiv_url": "https://arxiv.org/abs/2512.21236",
    "authors": [
      "Yifan Huang",
      "Xiaojun Jia",
      "Wenbo Guo",
      "Yuqiang Sun",
      "Yihao Huang",
      "Chong Wang",
      "Yang Liu"
    ],
    "first_author": "Yifan Huang",
    "primary_category": "cs.CR",
    "tag": [
      "Code Alignment"
    ],
    "benchmark": true,
    "conference": "FSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.21236v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-27 00:58:08"
  },
  {
    "id": "2512.21028",
    "title": "Artificial or Just Artful? Do LLMs Bend the Rules in Programming?",
    "abstract": "Large Language Models (LLMs) are widely used for automated code generation, yet their apparent successes often mask a tension between pretraining objectives and alignment choices. While pretraining encourages models to exploit all available signals to maximize success, alignment, whether through fine-tuning or prompting, may restrict their use. This conflict is especially salient in agentic AI settings, for instance when an agent has access to unit tests that, although intended for validation, act as strong contextual signals that can be leveraged regardless of explicit prohibitions. In this paper, we investigate how LLMs adapt their code generation strategies when exposed to test cases under different prompting conditions. Using the BigCodeBench (Hard) dataset, we design five prompting conditions that manipulate test visibility and impose explicit or implicit restrictions on their use. We evaluate five LLMs (four open-source and one closed-source) across correctness, code similarity, program size, and code churn, and analyze cross-model consistency to identify recurring adaptation strategies. Our results show that test visibility dramatically alters performance, correctness nearly doubles for some models, while explicit restrictions or partial exposure only partially mitigate this effect. Beyond raw performance, we identify four recurring adaptation strategies, with test-driven refinement emerging as the most frequent. These results highlight how LLMs adapt their behavior when exposed to contextual signals that conflict with explicit instructions, providing useful insight into how models reconcile pretraining objectives with alignment constraints.",
    "arxiv_url": "https://arxiv.org/abs/2512.21028",
    "authors": [
      "Oussama Ben Sghaier",
      "Kevin Delcourt",
      "Houari Sahraoui"
    ],
    "first_author": "Oussama Ben Sghaier",
    "primary_category": "cs.SE",
    "tag": [
      "Code Prompting"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.21028v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-28 01:07:08"
  },
  {
    "id": "2512.20957",
    "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents",
    "abstract": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",
    "arxiv_url": "https://arxiv.org/abs/2512.20957",
    "authors": [
      "Zhaoxi Zhang",
      "Yitong Duan",
      "Yanzhi Zhang",
      "Yiming Xu",
      "Jiyan He",
      "Yunfang Wu"
    ],
    "first_author": "Zhaoxi Zhang",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.20957v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-29 01:04:48"
  },
  {
    "id": "2512.22113",
    "title": "Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications",
    "abstract": "Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.",
    "arxiv_url": "https://arxiv.org/abs/2512.22113",
    "authors": [
      "Shengkun Cui",
      "Rahul Krishna",
      "Saurabh Jha",
      "Ravishankar K. Iyer"
    ],
    "first_author": "Shengkun Cui",
    "primary_category": "cs.DC",
    "tag": [
      "Code Debug"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.22113v1",
    "published": "2025-12-26",
    "update_time": "2025-12-26",
    "download_time": "2025-12-30 00:59:44"
  },
  {
    "id": "2512.23327",
    "title": "An Empirical Study of Generative AI Adoption in Software Engineering",
    "abstract": "Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.",
    "arxiv_url": "https://arxiv.org/abs/2512.23327",
    "authors": [
      "Grkem Giray",
      "Onur Demirrs",
      "Marcos Kalinowski",
      "Daniel Mendez"
    ],
    "first_author": "Grkem Giray",
    "primary_category": "cs.SE",
    "tag": [
      "Human Factors in LLM4SE"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.23327v1",
    "published": "2025-12-29",
    "update_time": "2025-12-29",
    "download_time": "2025-12-31 01:07:00"
  },
  {
    "id": "2512.23214",
    "title": "Anka: A Domain-Specific Language for Reliable LLM Code Generation",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.",
    "arxiv_url": "https://arxiv.org/abs/2512.23214",
    "authors": [
      "Saif Khalfan Saif Al Mazrouei"
    ],
    "first_author": "Saif Khalfan Saif Al Mazrouei",
    "primary_category": "cs.CL",
    "tag": [
      "Code Prompting"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.23214v1",
    "published": "2025-12-29",
    "update_time": "2025-12-29",
    "download_time": "2026-01-01 01:08:37"
  },
  {
    "id": "2512.24858",
    "title": "Feature Slice Matching for Precise Bug Detection",
    "abstract": "Measuring the function similarity to detect bugs is effective, but the statements unrelated to the bugs can impede the performance due to the noise interference. Suppressing the noise interference in existing works does not manage the tough job, i.e., eliminating the noise in the targets. In this paper, we propose MATUS to mitigate the target noise for precise bug detection based on similarity measurement. Feature slices are extracted from both the buggy query and the targets to represent the semantic feature of (potential) bug logics. In particular, MATUS guides the target slicing with the prior knowledge from the buggy code, in an end-to-end way to pinpoint the slicing criterion in the targets. All feature slices are embedded and compared based on the vector similarity. Buggy candidates are audited to confirm unknown bugs in the targets. Experiments show that MATUS holds advantages in bug detection for real-world projects with acceptable efficiency. In total, MATUS has spotted 31 unknown bugs in the Linux kernel. All of them have been confirmed by the kernel developers, and 11 have been assigned CVEs.",
    "arxiv_url": "https://arxiv.org/abs/2512.24858",
    "authors": [
      "Ke Ma",
      "Jianjun Huang",
      "Wei You",
      "Bin Liang",
      "Jingzheng Wu",
      "Yanjun Wu",
      "Yuanjun Gong"
    ],
    "first_author": "Ke Ma",
    "primary_category": "cs.SE",
    "tag": [
      "Code Debug"
    ],
    "benchmark": false,
    "conference": "FSE2026",
    "pdf_url": "https://arxiv.org/pdf/2512.24858v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-02 01:01:14"
  }
]