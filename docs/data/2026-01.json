[
  {
    "id": "2601.00753",
    "title": "Early-Stage Prediction of Review Effort in AI-Generated Pull Requests",
    "abstract": "As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?   Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).   We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.   Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.",
    "arxiv_url": "https://arxiv.org/abs/2601.00753",
    "authors": [
      "Dao Sy Duy Minh",
      "Huynh Trung Kiet",
      "Tran Chi Nguyen",
      "Nguyen Lam Phu Quy",
      "Phu Hoa Pham",
      "Nguyen Dinh Ha Duong",
      "Truong Bao Tran"
    ],
    "first_author": "Dao Sy Duy Minh",
    "primary_category": "cs.SE",
    "tag": [
      "Code Review Triage"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00753v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:00:51"
  },
  {
    "id": "2601.02345",
    "title": "Question Answering for Multi-Release Systems: A Case Study at Ciena",
    "abstract": "Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.",
    "arxiv_url": "https://arxiv.org/abs/2601.02345",
    "authors": [
      "Parham Khamsepour",
      "Mark Cole",
      "Ish Ashraf",
      "Sandeep Puri",
      "Mehrdad Sabetzadeh",
      "Shiva Nejati"
    ],
    "first_author": "Parham Khamsepour",
    "primary_category": "cs.SE",
    "tag": [
      "Code Prompting"
    ],
    "benchmark": true,
    "conference": "publication in SANER 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.02345v1",
    "published": "2026-01-05",
    "update_time": "2026-01-05",
    "download_time": "2026-01-07 01:01:26"
  },
  {
    "id": "2601.02971",
    "title": "Few-shot learning for security bug report identification",
    "abstract": "Security bug reports require prompt identification to minimize the window of vulnerability in software systems. Traditional machine learning (ML) techniques for classifying bug reports to identify security bug reports rely heavily on large amounts of labeled data. However, datasets for security bug reports are often scarce in practice, leading to poor model performance and limited applicability in real-world settings. In this study, we propose a few-shot learning-based technique to effectively identify security bug reports using limited labeled data. We employ SetFit, a state-of-the-art few-shot learning framework that combines sentence transformers with contrastive learning and parameter-efficient fine-tuning. The model is trained on a small labeled dataset of bug reports and is evaluated on its ability to classify these reports as either security-related or non-security-related. Our approach achieves an AUC of 0.865, at best, outperforming traditional ML techniques (baselines) for all of the evaluated datasets. This highlights the potential of SetFit to effectively identify security bug reports. SetFit-based few-shot learning offers a promising alternative to traditional ML techniques to identify security bug reports. The approach enables efficient model development with minimal annotation effort, making it highly suitable for scenarios where labeled data is scarce.",
    "arxiv_url": "https://arxiv.org/abs/2601.02971",
    "authors": [
      "Muhammad Laiq"
    ],
    "first_author": "Muhammad Laiq",
    "primary_category": "cs.SE",
    "tag": [
      "Bug Report Classification"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.02971v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 01:01:06"
  }
]