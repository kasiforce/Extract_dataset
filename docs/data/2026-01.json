[
  {
    "id": "2601.00753",
    "title": "Early-Stage Prediction of Review Effort in AI-Generated Pull Requests",
    "abstract": "As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?   Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).   We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.   Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.",
    "arxiv_url": "https://arxiv.org/abs/2601.00753",
    "authors": [
      "Dao Sy Duy Minh",
      "Huynh Trung Kiet",
      "Tran Chi Nguyen",
      "Nguyen Lam Phu Quy",
      "Phu Hoa Pham",
      "Nguyen Dinh Ha Duong",
      "Truong Bao Tran"
    ],
    "first_author": "Dao Sy Duy Minh",
    "primary_category": "cs.SE",
    "tag": [
      "Code Review Triage"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00753v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:00:51"
  },
  {
    "id": "2601.02345",
    "title": "Question Answering for Multi-Release Systems: A Case Study at Ciena",
    "abstract": "Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.",
    "arxiv_url": "https://arxiv.org/abs/2601.02345",
    "authors": [
      "Parham Khamsepour",
      "Mark Cole",
      "Ish Ashraf",
      "Sandeep Puri",
      "Mehrdad Sabetzadeh",
      "Shiva Nejati"
    ],
    "first_author": "Parham Khamsepour",
    "primary_category": "cs.SE",
    "tag": [
      "Code Prompting"
    ],
    "benchmark": true,
    "conference": "publication in SANER 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.02345v1",
    "published": "2026-01-05",
    "update_time": "2026-01-05",
    "download_time": "2026-01-07 01:01:26"
  },
  {
    "id": "2601.02971",
    "title": "Few-shot learning for security bug report identification",
    "abstract": "Security bug reports require prompt identification to minimize the window of vulnerability in software systems. Traditional machine learning (ML) techniques for classifying bug reports to identify security bug reports rely heavily on large amounts of labeled data. However, datasets for security bug reports are often scarce in practice, leading to poor model performance and limited applicability in real-world settings. In this study, we propose a few-shot learning-based technique to effectively identify security bug reports using limited labeled data. We employ SetFit, a state-of-the-art few-shot learning framework that combines sentence transformers with contrastive learning and parameter-efficient fine-tuning. The model is trained on a small labeled dataset of bug reports and is evaluated on its ability to classify these reports as either security-related or non-security-related. Our approach achieves an AUC of 0.865, at best, outperforming traditional ML techniques (baselines) for all of the evaluated datasets. This highlights the potential of SetFit to effectively identify security bug reports. SetFit-based few-shot learning offers a promising alternative to traditional ML techniques to identify security bug reports. The approach enables efficient model development with minimal annotation effort, making it highly suitable for scenarios where labeled data is scarce.",
    "arxiv_url": "https://arxiv.org/abs/2601.02971",
    "authors": [
      "Muhammad Laiq"
    ],
    "first_author": "Muhammad Laiq",
    "primary_category": "cs.SE",
    "tag": [
      "Bug Report Classification"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.02971v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 01:01:06"
  },
  {
    "id": "2601.03988",
    "title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures",
    "abstract": "Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.   Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.   Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.",
    "arxiv_url": "https://arxiv.org/abs/2601.03988",
    "authors": [
      "Nicolas Lacroix",
      "Mireille Blay-Fornarino",
      "SÃ©bastien Mosser",
      "Frederic Precioso"
    ],
    "first_author": "Nicolas Lacroix",
    "primary_category": "cs.SE",
    "tag": [
      "Code Summarization"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.03988v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 01:02:06"
  },
  {
    "id": "2601.04886",
    "title": "Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests",
    "abstract": "Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that descriptions claiming unimplemented changes was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5x longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.",
    "arxiv_url": "https://arxiv.org/abs/2601.04886",
    "authors": [
      "Jingzhi Gong",
      "Giovanni Pinna",
      "Yixin Bian",
      "Jie M. Zhang"
    ],
    "first_author": "Jingzhi Gong",
    "primary_category": "cs.SE",
    "tag": [
      "Code Alignment"
    ],
    "benchmark": true,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04886v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-10 00:59:31"
  },
  {
    "id": "2601.04540",
    "title": "AdaptEval: A Benchmark for Evaluating Large Language Models on Code Snippet Adaptation",
    "abstract": "Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMs' performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation. Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, Practical Context. Tasks in AdaptEval are derived from developers' practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, Multi-granularity Annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, Fine-grained Evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs' performance across various individual adaptations. Based on AdaptEval, we conduct the first empirical study to evaluate six instruction-tuned LLMs and especially three reasoning LLMs on code snippet adaptation. Experimental results demonstrate that AdaptEval enables the assessment of LLMs' adaptation capabilities from various perspectives. It also provides critical insights into their current limitations, particularly their struggle to follow explicit instructions. We hope AdaptEval can facilitate further investigation and enhancement of LLMs' capabilities in code snippet adaptation, supporting their real-world applications.",
    "arxiv_url": "https://arxiv.org/abs/2601.04540",
    "authors": [
      "Tanghaoran Zhang",
      "Xinjun Mao",
      "Shangwen Wang",
      "Yuxin Zhao",
      "Yao Lu",
      "Jin Zhang",
      "Zhang Zhang",
      "Kang Yang",
      "Yue Yu"
    ],
    "first_author": "Tanghaoran Zhang",
    "primary_category": "cs.SE",
    "tag": [
      "Code Editing"
    ],
    "benchmark": true,
    "conference": "ASE 2025",
    "pdf_url": "https://arxiv.org/pdf/2601.04540v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-11 01:07:14"
  },
  {
    "id": "2601.04556",
    "title": "4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering",
    "abstract": "We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked \"Why is completion rate 80%?\", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.",
    "arxiv_url": "https://arxiv.org/abs/2601.04556",
    "authors": [
      "Bo Yu",
      "Lei Zhao"
    ],
    "first_author": "Bo Yu",
    "primary_category": "cs.SE",
    "tag": [
      "Code Prompting"
    ],
    "benchmark": false,
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04556v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-12 01:04:09"
  }
]