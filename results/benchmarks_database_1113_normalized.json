[
  {
    "source_paper": "content.md",
    "benchmark_name": "HumanEval-x, CodefuseEval",
    "benchmark_name_quote": "Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodefuseEval for Chinese prompts.",
    "dataset_url": "https://github.com/codefuse-ai/codefuse-evaluation",
    "dataset_url_quote": "Moreover, we developed and open-sourced a more comprehensive benchmark, named CodefuseEval3, to support for a broader range of programming scenarios involving Chinese inputs.",
    "task_description": "代码生成、代码翻译、代码注释、测试用例生成等多语言代码相关任务",
    "task_description_quote": "In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",
    "dimension": "多语言代码理解能力、功能正确性",
    "dimension_quote": "However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied.",
    "evaluation_method": "pass@1指标、人工反馈评估",
    "evaluation_method_quote": "The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "软件工程、编程开发",
    "problem_domain_quote": "Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "支持40多种编程语言，包括Java、Python、C++、JavaScript等",
    "language_quote": "It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages.",
    "data_size": "HumanEval基准测试，包含164个Python编程问题",
    "data_size_quote": "The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%",
    "source_type": "行业标准基准测试和专门设计的中文提示评估集",
    "source_type_quote": "Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodefuseEval for Chinese prompts.",
    "last_updated": 2024,
    "last_updated_quote": "ICSE-SEIP '24, April 14–20, 2024, Lisbon, Portugal",
    "build_type": "官方自建",
    "build_type_quote": "Moreover, we developed and open-sourced a more comprehensive benchmark, named CodefuseEval3, to support for a broader range of programming scenarios involving Chinese inputs.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "代码生成、代码翻译、代码注释、测试用例生成",
    "task_granularity_quote": "In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%",
    "input_modality": "自然语言（英文和中文提示）",
    "input_modality_quote": "It is specifically designed for code-related tasks with both English and Chinese prompts",
    "output_modality": "代码",
    "output_modality_quote": "In practical scenarios, such as code generation, code translation, code comments, and testcase generation",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "专门针对中文提示优化的多语言代码评估基准，支持40多种编程语言",
    "unique_features_quote": "It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages.",
    "data_size_quantity": 164,
    "data_size_unit": "个问题",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java', 'Python', 'C++', 'JavaScript', '多语言']",
    "dimension_normalized": "['多语言代码理解能力', '功能正确性']",
    "evaluation_method_normalized": "['pass@1指标', '人工反馈评估']",
    "problem_domain_normalized": "['软件工程', '编程开发']",
    "source_type_normalized": "['行业标准基准测试', '专门设计的中文提示评估集']",
    "problem_difficulty_normalized": "未知",
    "context_dependency_normalized": "未知",
    "task_granularity_normalized": "代码生成",
    "input_modality_normalized": "自然语言",
    "output_modality_normalized": "代码",
    "execution_environment_normalized": "未知",
    "dataset_license_normalized": "未知",
    "contamination_status_normalized": "未知"
  },
  {
    "source_paper": "2102.04664v2_output/content.md",
    "benchmark_name": "CodeXGLUE",
    "benchmark_name_quote": "In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation.",
    "dataset_url": "https://github.com/microsoft/CodeXGLUE",
    "dataset_url_quote": "CodeXGLUE is publicly available at https://github.com/microsoft/CodeXGLUE.",
    "task_description": "程序理解和生成的机器学习基准数据集，包含10个任务和14个数据集，用于促进编程语言任务的机器学习研究",
    "task_description_quote": "CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison.",
    "dimension": "代码理解与生成的多个维度，包括代码相似性、缺陷检测、代码补全、代码翻译、代码搜索等",
    "dimension_quote": "CodeXGLUE supports the following tasks: code-code (clone detection, defect detection, cloze test, code completion, code repair, and code-to-code translation), text-code (natural language code search, text-to-code generation), code-text (code summarization), text-text (documentation translation)",
    "evaluation_method": "提供评估平台和基线系统进行模型评估和比较",
    "evaluation_method_quote": "a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models",
    "context_dependency": "多种上下文范围，从单行代码补全到完整函数理解",
    "context_dependency_quote": "The task is to complete an unfinished line. Models should be capable of predicting code sequences of arbitrary token types and code structures.",
    "problem_domain": "软件工程和编程语言处理，涵盖多种编程任务",
    "problem_domain_quote": "program understanding, machine learning, naturalness of software",
    "problem_difficulty": "多样化难度，从基础代码补全到复杂缺陷检测",
    "problem_difficulty_quote": NaN,
    "language": "支持多种编程语言，包括Python、Java、C/C++、PHP、JavaScript、Ruby、Go等",
    "language_quote": "CT-all and CT-maxmin datasets for six programming languages, i.e., Go, Java, JavaScript (JS), PHP, Python and Ruby.",
    "data_size": "大规模数据集，包含多个子数据集，总数据量达数百万样本",
    "data_size_quote": "BigCloneBench is a widely used large code clone benchmark that contains over 6,000,000 true clone pairs and 260,000 false clone pairs",
    "source_type": "混合来源，包括现有公开数据集和新构建的数据集",
    "source_type_quote": "CodeXGLUE includes eight previously proposed datasets — BigCloneBench [71], POJ-104 [52], Devign [99], PY150 [62], Github Java Corpus [4], Bugs2Fix [75], CONCODE [38], and CodeSearchNet [35]— but also newly introduced datasets",
    "last_updated": "2021",
    "last_updated_quote": "arXiv:2102.04664v2  [cs.SE]  16 Mar 2021",
    "build_type": "官方自建，由微软等机构研究人员构建",
    "build_type_quote": "Authors are from Peking University, Sun Yat-sen University, Beihang University, and Microsoft",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "多种粒度任务，包括代码克隆检测、缺陷检测、完形填空测试、代码补全、代码修复、代码翻译、代码搜索、文本到代码生成、代码摘要、文档翻译",
    "task_granularity_quote": "Clone detection, Defect detection, Cloze test, Code completion, Code translation, Code search, Code repair, Text-to-code generation, Code summarization, Documentation translation",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "多种输入模式，包括代码、自然语言、代码与自然语言组合",
    "input_modality_quote": "code-code, text-code, code-text, text-text",
    "output_modality": "多种输出模式，包括代码、自然语言、分类标签",
    "output_modality_quote": "code completion, code generation, code summarization, classification tasks",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "综合性基准套件，覆盖广泛的编程语言任务，提供多种基线模型和评估平台",
    "unique_features_quote": "a benchmark dataset for program understanding and generation research that includes 14 datasets, a collection of 10 diversified programming language understanding and generation tasks, and a platform for model evaluation and comparison",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python', 'Java', 'C/C++', 'PHP', 'JavaScript', 'Ruby', 'Go']",
    "dimension_normalized": "['代码相似性', '缺陷检测', '代码补全', '代码翻译', '代码搜索']",
    "evaluation_method_normalized": "['提供评估平台和基线系统进行模型评估和比较']",
    "problem_domain_normalized": "['软件工程', '编程语言处理']",
    "source_type_normalized": "['混合来源', '现有公开数据集', '新构建的数据集']",
    "problem_difficulty_normalized": "未知",
    "context_dependency_normalized": "未知",
    "task_granularity_normalized": "未知",
    "input_modality_normalized": "未知",
    "output_modality_normalized": "未知",
    "execution_environment_normalized": "未知",
    "dataset_license_normalized": "未知",
    "contamination_status_normalized": "未知"
  },
  {
    "source_paper": "2302.06527_output/content.md",
    "benchmark_name": "TESTPILOT",
    "benchmark_name_quote": "We implement our approach in TESTPILOT, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API.",
    "dataset_url": "https://github.com/githubnext/testpilot",
    "dataset_url_quote": "An implementation of this technique for JavaScript in a tool called TESTPILOT, which is available as open-source software at https://github.com/githubnext/testpilot.",
    "task_description": "自动化单元测试生成，使用大型语言模型为JavaScript项目中的API函数生成单元测试",
    "task_description_quote": "automatically generates unit tests for the methods in a given project's API.",
    "dimension": "测试生成质量、代码覆盖率、测试有效性",
    "dimension_quote": "We evaluate the coverage achieved by the generated tests and their quality in terms of success rate, reasons for failure, and whether or not they contain assertions that actually exercise functionality from the target package.",
    "evaluation_method": "语句覆盖率、分支覆盖率、测试成功率、非平凡断言比例",
    "evaluation_method_quote": "TESTPILOT's generated tests achieve a median statement coverage of 70.2%, and branch coverage of 52.8%. We find that a median 61.4% of the generated tests contain non-trivial assertions.",
    "context_dependency": "单函数测试，基于函数签名、文档和实现",
    "context_dependency_quote": "our prompts contain (1) the signature of the function under test; (2) its documentation comment, if any; (3) usage examples for the function mined from documentation, if available; (4) its source code.",
    "problem_domain": "软件测试、JavaScript开发",
    "problem_domain_quote": "automated unit test generation for JavaScript",
    "problem_difficulty": "实际工程级难度，涵盖25个npm包",
    "problem_difficulty_quote": "We evaluate our approach on 25 npm packages from various domains hosted on both GitHub and GitLab, with varying levels of popularity and amounts of available documentation.",
    "language": "JavaScript",
    "language_quote": "an LLM-based test generator for JavaScript",
    "data_size": "25个npm包，共1,684个API函数",
    "data_size_quote": "We evaluate TESTPILOT using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API functions.",
    "source_type": "实际npm包中的API函数",
    "source_type_quote": "25 npm packages from various domains hosted on both GitHub and GitLab",
    "last_updated": "2023",
    "last_updated_quote": "arXiv:2302.06527v4 [cs.SE] 11 Dec 2023",
    "build_type": "官方自建",
    "build_type_quote": "We implement our approach in TESTPILOT",
    "contamination_status": "抗污染设计，测试与训练集相似度低",
    "contamination_status_quote": "we find that 60.0% of the tests generated using the gpt3.5-turbo LLM have ≤40% similarity to existing tests and 92.8% have ≤50% similarity, with none of the tests being exact copies.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "单元测试生成",
    "task_granularity_quote": "automated generation of unit tests",
    "evaluation_metrics": "语句覆盖率、分支覆盖率、测试成功率",
    "evaluation_metrics_quote": "median statement coverage of 70.2%, and branch coverage of 52.8%",
    "input_modality": "代码与自然语言（函数签名、文档、实现）",
    "input_modality_quote": "prompts contain (1) the signature of the function under test; (2) its documentation comment, if any; (3) usage examples for the function mined from documentation, if available; (4) its source code.",
    "output_modality": "JavaScript测试代码",
    "output_modality_quote": "generates tests using the popular JavaScript testing framework Mocha",
    "execution_environment": "Node.js环境，使用Mocha测试框架和assert模块",
    "execution_environment_quote": "TESTPILOT generates tests using the popular JavaScript testing framework Mocha [50] with its BDD-style syntax... Assertions are checked using the built-in Node.js assert module.",
    "unique_features": "自适应测试生成，包含失败测试修复机制；无需额外训练或few-shot学习；支持多种LLM模型",
    "unique_features_quote": "Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We explore the feasibility of automatically generating unit tests using off-the-shelf LLMs, with no additional training and as little pre-processing as possible.",
    "data_size_quantity": 1684,
    "data_size_unit": "个API函数",
    "last_updated_year": 2023,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['JavaScript']",
    "dimension_normalized": "['测试生成质量', '代码覆盖率', '测试有效性']",
    "evaluation_method_normalized": "['语句覆盖率', '分支覆盖率', '测试成功率', '非平凡断言比例']",
    "problem_domain_normalized": "['软件测试', 'JavaScript开发']",
    "source_type_normalized": "['实际npm包中的API函数']",
    "problem_difficulty_normalized": "工程级",
    "context_dependency_normalized": "单函数",
    "task_granularity_normalized": "代码生成",
    "input_modality_normalized": "代码与自然语言",
    "output_modality_normalized": "代码",
    "execution_environment_normalized": "需要特定依赖",
    "dataset_license_normalized": "未知",
    "contamination_status_normalized": "抗污染设计"
  },
  {
    "source_paper": "2304.12244_output/content.md",
    "benchmark_name": "WizardEval",
    "benchmark_name_quote": "Due to the low proportion of difficult instructions in the previous instruction-following test dataset, we manually created a new difficulty-balanced test dataset, named WizardEval.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "评估大型语言模型遵循复杂指令的能力，涵盖推理、代码、数学、通用对话等多个领域",
    "task_description_quote": "We evaluate Alpaca, Vicuna, ChatGPT, and WizardLM on a wide range of LLM benchmarks (covering reasoning, code, mathematics, general conversation, etc.).",
    "dimension": "指令遵循能力、复杂任务处理能力、多领域适应性",
    "dimension_quote": "Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions.",
    "evaluation_method": "自动评估和人工评估相结合",
    "evaluation_method_quote": "Both automatic and human evaluations consistently indicate that WizardLM outperforms baselines such as Alpaca (trained from Self-Instruct) and Vicuna (trained from human-created instructions).",
    "context_dependency": "开放领域指令，具有不同的输入和任务",
    "context_dependency_quote": "Our work focuses on open-domain instruction data, where instructions have varying inputs and tasks without a clear distinction between the instruction part and the input.",
    "problem_domain": "多领域综合评估，包括推理、代码、数学、通用对话等",
    "problem_domain_quote": "We evaluate Alpaca, Vicuna, ChatGPT, and WizardLM on a wide range of LLM benchmarks (covering reasoning, code, mathematics, general conversation, etc.).",
    "problem_difficulty": "难度平衡的测试集，包含从简单到复杂的各种难度级别",
    "problem_difficulty_quote": "Due to the low proportion of difficult instructions in the previous instruction-following test dataset, we manually created a new difficulty-balanced test dataset, named WizardEval.",
    "language": "自然语言指令，主要涉及英语",
    "language_quote": "Training large language models (LLMs) with open-domain instruction following data brings colossal success.",
    "data_size": "从250k指令中采样70k用于训练，具体测试集规模未明确说明",
    "data_size_quote": "We execute four epochs of evolution using OpenAI ChatGPT API and finally obtain 250k instructions. To ensure a fair comparison with Vicuna's 70k real user data, we sampled 70k from the full 250k data and fine-tuned the LLaMA 13B model.",
    "source_type": "通过Evol-Instruct方法自动生成，基于Alpaca数据的演化",
    "source_type_quote": "We evolve the instructions from Aplaca data (created by machine), fine-tune the LLaMA model, and comprehensively compare the fine-tuned model WizardLM with Vicuna trained on ShareGPT (instructions are created by human).",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2304.12244v3 [cs.CL] 27 May 2025",
    "build_type": "研究团队自建",
    "build_type_quote": "Due to the low proportion of difficult instructions in the previous instruction-following test dataset, we manually created a new difficulty-balanced test dataset, named WizardEval.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "复杂指令遵循，包括多步骤推理和多样化任务",
    "task_granularity_quote": "The In-depth Evolving includes five types of operations: add constraints, deepening, concretizing, increase reasoning steps, and complicate input.",
    "evaluation_metrics": "性能比较基准测试，包括GPT-4评估和人工评估",
    "evaluation_metrics_quote": "Notably, WizardLM outperforms baselines by a substantial margin in terms of code, math, GPT-4 and human evaluations.",
    "input_modality": "自然语言指令，可能包含表格、代码等非文本部分",
    "input_modality_quote": "Your rewriting cannot omit the non-text parts such as the table and code in #Given Prompt#:. Also, please do not omit the input in #Given Prompt#.",
    "output_modality": "自然语言响应",
    "output_modality_quote": "In each evolution, we upgrade all the I(t) in D(t) to I(t+1) by prompting a LLM with Evol-Instruct prompt, and then use the LLM to generate corresponding responses Rt+1 for the newly evolved It+1.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "难度平衡的测试集设计，专门针对复杂指令评估，通过Evol-Instruct方法生成多样化难度级别的指令",
    "unique_features_quote": "Due to the low proportion of difficult instructions in the previous instruction-following test dataset, we manually created a new difficulty-balanced test dataset, named WizardEval. In this work, we introduce Evol-Instruct, a novel method using LLMs instead of humans to automatically mass-produce open-domain instructions of various difficulty levels, to improve the performance of LLMs.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['自然语言指令', '英语']",
    "dimension_normalized": "['指令遵循能力', '复杂任务处理能力', '多领域适应性']",
    "evaluation_method_normalized": "['自动评估', '人工评估']",
    "problem_domain_normalized": "['多领域综合评估', '推理', '代码', '数学', '通用对话']",
    "source_type_normalized": "['Evol-Instruct方法自动生成', 'Alpaca数据的演化']",
    "problem_difficulty_normalized": "未知",
    "context_dependency_normalized": "未知",
    "task_granularity_normalized": "未知",
    "input_modality_normalized": "自然语言",
    "output_modality_normalized": "自然语言",
    "execution_environment_normalized": "未知",
    "dataset_license_normalized": "未知",
    "contamination_status_normalized": "未知"
  },
  {
    "source_paper": "2305.04764_output/content.md",
    "benchmark_name": "ChatUniTest",
    "benchmark_name_quote": "This paper presents ChatUniTest, an LLM-based automated unit test generation framework.",
    "dataset_url": "https://github.com/ZJU-ACES-ISE/ChatUniTest",
    "dataset_url_quote": "ChatUniTest is available at https://github.com/ZJU-ACES-ISE/ChatUniTest",
    "task_description": "基于大语言模型的自动化单元测试生成",
    "task_description_quote": "ChatUniTest, an LLM-based automated unit test generation framework.",
    "dimension": "单元测试生成的有效性和覆盖率",
    "dimension_quote": "Our effectiveness evaluation reveals that ChatUniTest outperforms TestSpark and EvoSuite in half of the evaluated projects, achieving the highest overall line coverage.",
    "evaluation_method": "行覆盖率评估和用户研究",
    "evaluation_method_quote": "achieving the highest overall line coverage. Furthermore, insights from our user study affirm that ChatUniTest delivers substantial value to various stakeholders",
    "context_dependency": "自适应焦点上下文机制，包含焦点方法和类的相关信息",
    "context_dependency_quote": "ChatUniTest employs an adaptive focal context generation mechanism to encompass valuable context in prompts",
    "problem_domain": "软件测试，单元测试生成",
    "problem_domain_quote": "Unit testing is an essential yet frequently arduous task. Various automated unit test generation tools have been introduced to mitigate this challenge.",
    "problem_difficulty": "工程级软件测试任务",
    "problem_difficulty_quote": "Unit testing is a critical practice in software development, ensuring the quality of software applications. However, manually writing and maintaining high-quality unit tests can be a daunting task.",
    "language": "Java",
    "language_quote": "In this stage, ChatUniTest scans the project folder to identify all Java files. Each Java file is then converted into an Abstract Syntax Tree (AST).",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "实际Java项目",
    "source_type_quote": "ChatUniTest scans the project folder to identify all Java files.",
    "last_updated": "2024",
    "last_updated_quote": "FSE Companion '24, July 15–19, 2024, Porto de Galinhas, Brazil",
    "build_type": "官方自建框架",
    "build_type_quote": "we have developed the ChatUniTest Core to assist researchers and tool builders.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "ACM版权许可",
    "dataset_license_quote": "Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted.",
    "task_granularity": "单元测试生成",
    "task_granularity_quote": "automated unit test generation",
    "evaluation_metrics": "行覆盖率",
    "evaluation_metrics_quote": "achieving the highest overall line coverage",
    "input_modality": "Java代码和上下文信息",
    "input_modality_quote": "ChatUniTest scans the project folder to identify all Java files. Each Java file is then converted into an Abstract Syntax Tree (AST).",
    "output_modality": "Java单元测试代码",
    "output_modality_quote": "generation of unit tests",
    "execution_environment": "Java编译和执行环境",
    "execution_environment_quote": "ChatUniTest proceeds to compile the test. This step not only checks for syntax errors but also examines certain semantic issues in the code.",
    "unique_features": "自适应焦点上下文机制和生成-验证-修复机制，包含基于规则和基于LLM的修复策略",
    "unique_features_quote": "ChatUniTest incorporates an adaptive focal context mechanism to encompass valuable context in prompts and adheres to a generation-validation-repair mechanism to rectify errors in generated unit tests. There are two types of repair strategies: rule-based repair and LLM-based repair.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['单元测试生成的有效性和覆盖率']",
    "evaluation_method_normalized": "['行覆盖率评估和用户研究']",
    "problem_domain_normalized": "['软件测试', '单元测试生成']",
    "source_type_normalized": "['实际Java项目']",
    "problem_difficulty_normalized": "工程级",
    "context_dependency_normalized": "未知",
    "task_granularity_normalized": "代码生成",
    "input_modality_normalized": "代码与自然语言",
    "output_modality_normalized": "代码",
    "execution_environment_normalized": "需要特定依赖",
    "dataset_license_normalized": "未知",
    "contamination_status_normalized": "未知"
  },
  {
    "source_paper": "2310.04951_output/content.md",
    "benchmark_name": "CodeTransOcean",
    "benchmark_name_quote": "we construct CodeTransOcean, a large-scale comprehensive benchmark that supports the largest variety of programming languages for code translation.",
    "dataset_url": "https://github.com/WeixiangYAN/CodeTransOcean",
    "dataset_url_quote": "The CodeTransOcean datasets and code are publicly available at https://github.com/WeixiangYAN/CodeTransOcean.",
    "task_description": "代码翻译，将源代码从一种编程语言转换为另一种编程语言",
    "task_description_quote": "The code translation task aims to convert source code from one programming language to another and is of great value in industry.",
    "dimension": "多语言代码翻译能力、跨框架代码翻译能力、代码可执行性",
    "dimension_quote": "CodeTransOcean consists of three novel multilingual datasets, namely, MultilingualTrans supporting translations between multiple popular programming languages, NicheTrans for translating between niche programming languages and popular ones, and LLMTrans for evaluating executability of translated code by large language models (LLMs).",
    "evaluation_method": "Debugging Success Rate@K (DSR@K)、模糊执行评估、匹配度指标、执行指标",
    "evaluation_method_quote": "We also propose a novel evaluation metric Debugging Success Rate@K for program-level code translation. Since our AutoTransExecuter still cannot cover arbitrary programming languages, we also propose a novel metric fuzzy execution, attempting to address the limitations of existing evaluation metrics for code translation.",
    "context_dependency": "程序级别",
    "context_dependency_quote": "Note that all samples in all CodeTransOcean datasets are constructed at the program-level.",
    "problem_domain": "软件工程、深度学习框架、多语言编程环境",
    "problem_domain_quote": "CodeTransOcean also includes a novel cross-framework dataset, DLTrans, for translating deep learning code across different frameworks. With the increasing need to unify the language variety when implementing system integration or extensions with multilingual programming environments",
    "problem_difficulty": "工业级应用难度，涵盖高资源和低资源语言对",
    "problem_difficulty_quote": "Experimental results demonstrate that multilingual modeling significantly improves translation quality for both high-resource and low-resource language pairs",
    "language": "Python, C, C++, Visual Basic, Go, PHP, Java, C#, Swift, R, Rust, Fortran, Ada, Perl, COBOL, Lua等45种编程语言",
    "language_quote": "covering 45 programming languages for multilingual code translation and 4 deep learning frameworks for cross-framework code translation",
    "data_size": "总计270,507个样本，包含超过200K单元测试",
    "data_size_quote": "Overall, CodeTransOcean consists of 270,507 samples (over 200K unit tests)",
    "source_type": "基于TIOBE编程语言社区指数的流行和利基编程语言分类构建",
    "source_type_quote": "We define popular and niche programming languages based on the TIOBE Programming Community Index, which is a metric of the popularity of programming languages.",
    "last_updated": "2023.10",
    "last_updated_quote": "arXiv:2310.04951v2 [cs.AI] 25 Oct 2023",
    "build_type": "官方自建，由阿里巴巴集团语音实验室支持",
    "build_type_quote": "Work is supported by Speech Lab, Alibaba Group.",
    "contamination_status": "无重叠设计，与现有代码翻译数据集无重叠",
    "contamination_status_quote": "There is no overlap between CodeTransOcean datasets and existing code translation datasets.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "程序级别代码翻译",
    "task_granularity_quote": "Note that all samples in all CodeTransOcean datasets are constructed at the program-level.",
    "evaluation_metrics": "Debugging Success Rate@K (DSR@K)、模糊执行评估",
    "evaluation_metrics_quote": "We also propose a novel evaluation metric Debugging Success Rate@K for program-level code translation. We also propose a novel metric fuzzy execution",
    "input_modality": "源代码",
    "input_modality_quote": "The code translation task aims to convert source code from one programming language to another",
    "output_modality": "翻译后的目标编程语言代码",
    "output_modality_quote": "to translate source code from one programming language to another",
    "execution_environment": "包含单元测试的执行环境",
    "execution_environment_quote": "the vast majority of the samples in CodeTransOcean provides explicit input and output, which is equivalent to unit tests",
    "unique_features": "支持最多编程语言的代码翻译基准，包含跨框架深度学习代码翻译，专门评估大语言模型代码翻译可执行性的数据集",
    "unique_features_quote": "CodeTransOcean covers the largest number of popular and niche programming languages so far with the largest scale. It also includes an unprecedented dataset for translating code across different deep learning frameworks and a dataset and an automated pipeline for evaluating LLMs on code translation.",
    "data_size_quantity": 270507,
    "data_size_unit": "个样本",
    "last_updated_year": 2023,
    "last_updated_month": 10,
    "last_updated_day": null,
    "language_normalized": "['Python', 'C', 'C++', 'Visual Basic', 'Go', 'PHP', 'Java', 'C#', 'Swift', 'R', 'Rust', 'Fortran', 'Ada', 'Perl', 'COBOL', 'Lua']",
    "dimension_normalized": "['多语言代码翻译能力', '跨框架代码翻译能力', '代码可执行性']",
    "evaluation_method_normalized": "['Debugging Success Rate@K (DSR@K)', '模糊执行评估', '匹配度指标', '执行指标']",
    "evaluation_metrics_normalized": "['Debugging Success Rate@K (DSR@K)', '模糊执行评估']",
    "problem_domain_normalized": "['软件工程', '深度学习框架', '多语言编程环境']",
    "source_type_normalized": "['基于TIOBE编程语言社区指数的流行和利基编程语言分类构建']",
    "problem_difficulty_normalized": "工程级",
    "context_dependency_normalized": "多文件/项目级",
    "task_granularity_normalized": "代码翻译",
    "input_modality_normalized": "未知",
    "output_modality_normalized": "代码",
    "execution_environment_normalized": "需要特定依赖",
    "dataset_license_normalized": "未知",
    "contamination_status_normalized": "抗污染设计"
  },
  {
    "source_paper": "2310.06266v2_output/content.md",
    "benchmark_name": "HumanEval-x, CodefuseEval",
    "benchmark_name_quote": "Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodefuseEval for Chinese prompts.",
    "dataset_url": "https://github.com/codefuse-ai/codefuse-evaluation",
    "dataset_url_quote": "Moreover, we developed and open-sourced a more comprehensive benchmark, named CodefuseEval3, to support for a broader range of programming scenarios involving Chinese inputs.",
    "task_description": "代码生成、代码翻译、代码注释、测试用例生成等多语言代码相关任务",
    "task_description_quote": "In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",
    "dimension": "多语言代码理解能力、功能正确性",
    "dimension_quote": "However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied.",
    "evaluation_method": "pass@1指标、人工反馈评估",
    "evaluation_method_quote": "The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "软件工程、编程开发",
    "problem_domain_quote": "Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "支持40多种编程语言，包括Java、Python、C++、JavaScript等",
    "language_quote": "It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages.",
    "data_size": "HumanEval基准测试，具体规模未明确说明",
    "data_size_quote": "Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x",
    "source_type": "行业标准基准测试和专门设计的中文提示评估集",
    "source_type_quote": "the industry-standard benchmark HumanEval-x, and the specially designed CodefuseEval for Chinese prompts",
    "last_updated": "2024",
    "last_updated_quote": "ICSE-SEIP '24, April 14–20, 2024, Lisbon, Portugal",
    "build_type": "官方自建",
    "build_type_quote": "Moreover, we developed and open-sourced a more comprehensive benchmark, named CodefuseEval3",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "代码生成、代码翻译、代码注释、测试用例生成",
    "task_granularity_quote": "In practical scenarios, such as code generation, code translation, code comments, and testcase generation",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%",
    "input_modality": "自然语言（英文和中文提示）",
    "input_modality_quote": "It is specifically designed for code-related tasks with both English and Chinese prompts",
    "output_modality": "代码",
    "output_modality_quote": "CodeFuse performs better than other models when confronted with Chinese prompts in practical scenarios, such as code generation, code translation, code comments, and testcase generation",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "专门针对中文提示设计的评测基准，支持多语言代码任务评估",
    "unique_features_quote": "the specially designed CodefuseEval for Chinese prompts. To support for a broader range of programming scenarios involving Chinese inputs.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java', 'Python', 'C++', 'JavaScript']",
    "dimension_normalized": "['多语言代码理解能力', '功能正确性']",
    "evaluation_method_normalized": "['pass@1指标', '人工反馈评估']",
    "problem_domain_normalized": "['软件工程', '编程开发']",
    "source_type_normalized": "['行业标准基准测试', '专门设计的中文提示评估集']",
    "problem_difficulty_normalized": "未知",
    "context_dependency_normalized": "未知",
    "task_granularity_normalized": "代码生成",
    "input_modality_normalized": "自然语言",
    "output_modality_normalized": "代码",
    "execution_environment_normalized": "未知",
    "dataset_license_normalized": "未知",
    "contamination_status_normalized": "未知"
  },
  {
    "source_paper": "2310.15539_output/content.md",
    "benchmark_name": "XLCoST",
    "benchmark_name_quote": "With experiments on XLCoST datasets, SteloCoder achieves an average of 73.76 CodeBLEU score in multi-programming language-to-Python translation",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "多编程语言到Python的代码翻译",
    "task_description_quote": "SteloCoder, a decoder-only StarCoder-based LLM designed specifically for multi-programming language-to-Python code translation",
    "dimension": "代码翻译质量",
    "dimension_quote": "SteloCoder achieves an average of 73.76 CodeBLEU score in multi-programming language-to-Python translation",
    "evaluation_method": "CodeBLEU评分",
    "evaluation_method_quote": "SteloCoder achieves an average of 73.76 CodeBLEU score in multi-programming language-to-Python translation",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "编程语言翻译",
    "problem_domain_quote": "specialized in translating code from five PLs (C++, C#, PHP, JavaScript, Java) to Python",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "C++, C#, JavaScript, Java, PHP, Python",
    "language_quote": "SteloCoder achieves C++, C#, JavaScript, Java, or PHP-to-Python code translation",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2023-12-15",
    "last_updated_quote": "arXiv:2310.15539v2 [cs.CL] 15 Dec 2023",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "代码翻译",
    "task_granularity_quote": "code translation between programming languages (PL to PL)",
    "evaluation_metrics": "CodeBLEU",
    "evaluation_metrics_quote": "SteloCoder achieves an average of 73.76 CodeBLEU score",
    "input_modality": "编程语言代码",
    "input_modality_quote": "translating code from five PLs (C++, C#, PHP, JavaScript, Java) to Python",
    "output_modality": "Python代码",
    "output_modality_quote": "multi-programming language-to-Python code translation",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "支持多种编程语言到Python的自动翻译，无需指定输入语言",
    "unique_features_quote": "SteloCoder achieves C++, C#, JavaScript, Java, or PHP-to-Python code translation without specifying the input programming language",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2023,
    "last_updated_month": 12,
    "last_updated_day": 15,
    "language_normalized": "['C++', 'C#', 'JavaScript', 'Java', 'PHP', 'Python']",
    "dimension_normalized": "['代码翻译质量']",
    "evaluation_method_normalized": "['CodeBLEU评分']",
    "problem_domain_normalized": "['编程语言翻译']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "未知",
    "context_dependency_normalized": "未知",
    "task_granularity_normalized": "代码翻译",
    "input_modality_normalized": "未知",
    "output_modality_normalized": "代码",
    "execution_environment_normalized": "未知",
    "dataset_license_normalized": "未知",
    "contamination_status_normalized": "未知"
  }
]