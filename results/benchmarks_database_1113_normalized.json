[
  {
    "source_paper": "2511.15757_output/content.md",
    "benchmark_name": "RGym",
    "benchmark_name_quote": "In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "Linuxå†…æ ¸è‡ªåŠ¨ç¨‹åºä¿®å¤è¯„ä¼°ï¼Œä¸“æ³¨äºå†…æ ¸ç©ºé—´è°ƒè¯•å’Œä¿®å¤çš„å¤æ‚æ€§",
    "task_description_quote": "Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on user-space applications and overlook the complexities of kernel-space debugging and repair.",
    "dimension": "Linuxå†…æ ¸ç¨‹åºä¿®å¤èƒ½åŠ›è¯„ä¼°ï¼ŒåŒ…æ‹¬å®šä½ã€è¡¥ä¸ç”Ÿæˆã€éªŒè¯å’Œæˆæœ¬/å»¶è¿Ÿè€ƒè™‘",
    "dimension_quote": "These characteristics make the kernel an ideal stress test for evaluating LLM-based APR, from localization to patch generation, validation, and cost/latency consideration.",
    "evaluation_method": "é€šè¿‡ç¼–è¯‘ä¿®è¡¥åçš„å†…æ ¸ã€è¿è¡Œæ¦‚å¿µéªŒè¯ç¨‹åºå¹¶æŠ¥å‘Šç»“æœæ¥è¯„ä¼°è¡¥ä¸è´¨é‡",
    "evaluation_method_quote": "RGym overall compiles patched kernels, runs PoCs, and reports results.",
    "context_dependency": "Linuxå†…æ ¸çº§åˆ«çš„å¤æ‚ä¾èµ–å…³ç³»ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡ã€æ·±åº¦ä¾èµ–ã€æ™®éå¹¶å‘å’Œç¡¬ä»¶åº•å±‚äº¤äº’",
    "context_dependency_quote": "with its massive scale, deep dependency, and pervasive concurrency and low-level interactions with hardware.",
    "problem_domain": "æ“ä½œç³»ç»Ÿå†…æ ¸å¼€å‘ï¼Œç‰¹åˆ«æ˜¯Linuxå†…æ ¸å†…å­˜å®‰å…¨æ¼æ´ä¿®å¤",
    "problem_domain_quote": "The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions.",
    "problem_difficulty": "é«˜éš¾åº¦å†…æ ¸çº§bugä¿®å¤ï¼ŒåŒ…å«å†…å­˜æŸåç­‰æœ€ä¸¥é‡ç±»å‹çš„bug",
    "problem_difficulty_quote": "filtering to KASAN bugs [13], which represent the most severe types of bugs (memory corruption)",
    "language": "Cè¯­è¨€ï¼ˆLinuxå†…æ ¸å¼€å‘ï¼‰",
    "language_quote": "From 6,088 Syzbot bugs, we retain those with fix commits, reproducers, crash reports, and kernel configs",
    "data_size": "åŒ…å«143ä¸ªå·²éªŒè¯çš„å†…æ ¸bugçš„æ•°æ®é›†",
    "data_size_quote": "We test on a filtered and verified dataset of 143 bugs.",
    "source_type": "æ¥è‡ªSyzkallerå†…æ ¸æ¨¡ç³Šæµ‹è¯•å™¨å’ŒSyzbotè‡ªåŠ¨å´©æºƒæŠ¥å‘Šç³»ç»Ÿçš„çœŸå®å†…æ ¸bug",
    "source_type_quote": "Syzkaller [6], a coverage-guided kernel fuzzer, together with Syzbot [5], an automated online crash reporting system developed by Google, provides a valuable ecosystem that makes kernel-bug collection possible",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.15757v1 [cs.SE] 19 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºçœŸå®å†…æ ¸bugæ„å»º",
    "build_type_quote": "We organize a dataset of 143 kernel bugs from Syzbot into an easily consumable format and verified the reproducibility of the bug on the patch parent.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ï¼Œç‰¹åˆ«æ˜¯å†…æ ¸å†…å­˜å®‰å…¨æ¼æ´ä¿®å¤",
    "task_granularity_quote": "automated program repair (APR) in the Linux kernel space",
    "evaluation_metrics": "è¡¥ä¸é€šè¿‡ç‡ï¼ˆpass rateï¼‰ã€é”™è¯¯è¡¥ä¸ç‡ï¼ˆbad patch rateï¼‰ã€æ¯bugå¹³å‡æˆæœ¬",
    "evaluation_metrics_quote": "Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug.",
    "input_modality": "å´©æºƒæŠ¥å‘Šã€è°ƒç”¨æ ˆã€bugè¯±å¯¼æäº¤ã€å†…æ ¸é…ç½®",
    "input_modality_quote": "inputs (patch, commit, source, config, compiler, cores, timeout, metadata)",
    "output_modality": "å†…æ ¸è¡¥ä¸ä»£ç ",
    "output_modality_quote": "The LLM lists candidate functions, receives their definitions, and returns their patched definitions.",
    "task_io_type": "å´©æºƒä¿¡æ¯åˆ°å†…æ ¸è¡¥ä¸ä»£ç ",
    "task_io_type_quote": "Our APR generates a patch via the Simple Agent or Function Exploration Agent and tests it with RGym.",
    "execution_environment": "ä½¿ç”¨dockeræ†ç»‘ä½œä¸šä¾èµ–å’ŒQEMUè™šæ‹Ÿæœºï¼Œåœ¨æœ¬åœ°ç¡¬ä»¶ä¸Šè¿è¡Œ",
    "execution_environment_quote": "RGym runs locally using docker to bundle job dependencies and QEMU for VMs.",
    "unique_features": "è½»é‡çº§ã€å¹³å°æ— å…³çš„Linuxå†…æ ¸APRè¯„ä¼°æ¡†æ¶ï¼Œä¸“æ³¨äºæœ¬åœ°å•†å“ç¡¬ä»¶è¿è¡Œï¼Œè§£å†³äº†kGymå¯¹GCPçš„ç¡¬ä¾èµ–é—®é¢˜",
    "unique_features_quote": "we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. RGym solves the compiler and dependency problem by smartly switching build dependencies using docker images depending on the kernel version or compiler string provided in the kernel configuration.",
    "data_size_quantity": 143,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Cè¯­è¨€']",
    "dimension_normalized": "['Linuxå†…æ ¸ç¨‹åºä¿®å¤èƒ½åŠ›è¯„ä¼°', 'å®šä½', 'è¡¥ä¸ç”Ÿæˆ', 'éªŒè¯', 'æˆæœ¬/å»¶è¿Ÿè€ƒè™‘']",
    "evaluation_method_normalized": "['è¡¥ä¸é€šè¿‡ç‡', 'é”™è¯¯è¡¥ä¸ç‡', 'æ¯bugå¹³å‡æˆæœ¬']",
    "problem_domain_normalized": "['æ“ä½œç³»ç»Ÿå†…æ ¸å¼€å‘', 'Linuxå†…æ ¸å†…å­˜å®‰å…¨æ¼æ´ä¿®å¤']",
    "source_type_normalized": "['Syzkallerå†…æ ¸æ¨¡ç³Šæµ‹è¯•å™¨', 'Syzbotè‡ªåŠ¨å´©æºƒæŠ¥å‘Šç³»ç»Ÿ', 'çœŸå®å†…æ ¸bug']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.16005_output/content.md",
    "benchmark_name": "MultiSWE-bench-CPP",
    "benchmark_name_quote": "Evaluated on the MultiSWE-bench-CPP benchmark, InfCode-C++ achieves a resolution rate of 25.58%",
    "dataset_url": "https://github.com/Tokfinity/InfCode",
    "dataset_url_quote": "To support the research community and facilitate future work, we release InfCode-C++ and our evaluation benchmark as an open-source project at https://github.com/Tokfinity/InfCode",
    "task_description": "C++è½¯ä»¶é—®é¢˜è§£å†³ï¼Œä»è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ç”Ÿæˆä¿®å¤è¡¥ä¸",
    "task_description_quote": "Given a software repository ğ¶ and a natural-language issue description ğ·, the objective is to synthesize a patch ğ‘ such that the updated repository ğ¶â€² = ğ¶ âŠ• ğ‘ satisfies the behavioral requirements expressed in ğ· while preserving the original functionality",
    "dimension": "ä»£ç ä¿®å¤èƒ½åŠ›ã€ä¸Šä¸‹æ–‡æ£€ç´¢å‡†ç¡®æ€§ã€ç»“æ„åˆ†æèƒ½åŠ›",
    "dimension_quote": "Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution",
    "evaluation_method": "é€šè¿‡å›å½’æµ‹è¯•å¥—ä»¶éªŒè¯è¡¥ä¸æ­£ç¡®æ€§ï¼Œä½¿ç”¨è§£å†³ç‡ä½œä¸ºè¯„ä¼°æŒ‡æ ‡",
    "evaluation_method_quote": "A correct patch must satisfy: ğ‘¡(ğ¶â€²) = ğ‘¡(ğ¶), âˆ€ğ‘¡âˆˆğ‘‡ and âˆƒğ‘¡ğ·: ğ‘¡ğ·(ğ¶â€²) = PASS, where ğ‘¡ğ· is a failing test case that captures the erroneous behavior described in ğ·",
    "context_dependency": "ä»“åº“çº§åˆ«ï¼Œæ¶‰åŠå¤šæ–‡ä»¶C++é¡¹ç›®",
    "context_dependency_quote": "repository-level issue resolution... in large, statically typed C++ repositories",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼ŒC++ç¨‹åºä¿®å¤",
    "problem_domain_quote": "Automated software issue resolution stands as a critical challenge in software engineering... resolving issues in large-scale, high-performance systems written in C++",
    "problem_difficulty": "çœŸå®ä¸–ç•Œå·¥ç¨‹çº§éš¾åº¦",
    "problem_difficulty_quote": "thousands of real-world GitHub issues... a formidable, unsolved challenge",
    "language": "C++",
    "language_quote": "specifically designed for C++ projects... the first C++-aware autonomous system for end-to-end issue resolution",
    "data_size": "MultiSWE-benchçš„C++å­é›†",
    "data_size_quote": "Evaluated on the MultiSWE-bench-CPP benchmark... on the C++ subset of MultiSWE-bench",
    "source_type": "çœŸå®ä¸–ç•Œçš„GitHubé—®é¢˜",
    "source_type_quote": "thousands of real-world GitHub issues",
    "last_updated": 2025,
    "last_updated_quote": "2025. InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution. 1, 1 (November 2025)",
    "build_type": "åŸºäºç°æœ‰åŸºå‡†æ„å»ºçš„å­é›†",
    "build_type_quote": "the C++ subset of MultiSWE-bench",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤",
    "task_granularity_quote": "issue resolution... synthesize a patch",
    "evaluation_metrics": "è§£å†³ç‡",
    "evaluation_metrics_quote": "achieves a resolution rate of 25.58%",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°",
    "input_modality_quote": "natural-language issue description ğ·",
    "output_modality": "ä»£ç è¡¥ä¸",
    "output_modality_quote": "synthesize a patch ğ‘",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "from a natural language issue description... synthesize a patch",
    "execution_environment": "Dockerç¯å¢ƒ",
    "execution_environment_quote": "Repo Codebase Docker Env",
    "unique_features": "ä¸“é—¨é’ˆå¯¹C++è¯­è¨€çš„å¤æ‚æ€§è®¾è®¡ï¼ŒåŒ…å«è¯­ä¹‰æ£€ç´¢å’ŒASTç»“æ„åŒ–æŸ¥è¯¢",
    "unique_features_quote": "the first C++-aware autonomous system... combines two complementary retrieval mechanismsâ€”semantic code-intent retrieval and deterministic AST-structured queryingâ€”to construct accurate, language-aware context for repair",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['C++']",
    "dimension_normalized": "['ä»£ç ä¿®å¤èƒ½åŠ›', 'ä¸Šä¸‹æ–‡æ£€ç´¢å‡†ç¡®æ€§', 'ç»“æ„åˆ†æèƒ½åŠ›']",
    "evaluation_method_normalized": "['è§£å†³ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'C++ç¨‹åºä¿®å¤']",
    "source_type_normalized": "['çœŸå®ä¸–ç•Œçš„GitHubé—®é¢˜']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.17131_output/content.md",
    "benchmark_name": "UI-CUBE (UiPath Computer Use BEnchmark)",
    "benchmark_name_quote": "We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs.",
    "dataset_url": "https://github.com/UiPath/uipath_enterprise_benchmark",
    "dataset_url_quote": "GitHub: https://github.com/UiPath/uipath_enterprise_benchmark",
    "task_description": "è¯„ä¼°è®¡ç®—æœºä½¿ç”¨ä»£ç†åœ¨ä¼ä¸šç¯å¢ƒä¸­çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç®€å•çš„UIäº¤äº’å’Œå¤æ‚çš„å·¥ä½œæµç¨‹è‡ªåŠ¨åŒ–",
    "task_description_quote": "Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks)",
    "dimension": "åŠŸèƒ½æ­£ç¡®æ€§ã€æ“ä½œå¯é æ€§ã€ç•Œé¢é€‚åº”æ€§ã€å·¥ä½œæµåè°ƒèƒ½åŠ›",
    "dimension_quote": "measuring not only functional accuracy but also operational reliabilityâ€”providing clearer insight into how CUAs can move from experimental demos toward dependable enterprise-grade tools",
    "evaluation_method": "é€šè¿‡åº”ç”¨ç¨‹åºçŠ¶æ€çš„è‡ªåŠ¨åŒ–éªŒè¯æ¥è¯„ä¼°ä»»åŠ¡æˆåŠŸï¼ŒåŒ…æ‹¬å¤šåˆ†è¾¨ç‡æµ‹è¯•å’Œç³»ç»Ÿç•Œé¢å˜åŒ–è¦†ç›–",
    "evaluation_method_quote": "with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state",
    "context_dependency": "å¤šæ­¥éª¤å·¥ä½œæµç¨‹ã€è·¨åº”ç”¨ç¨‹åºåè°ƒ",
    "context_dependency_quote": "complex workflows including copy-paste tasks and enterprise application scenarios, multi-step processes where reliability is as critical as task completion",
    "problem_domain": "ä¼ä¸šè½¯ä»¶è‡ªåŠ¨åŒ–ã€UIäº¤äº’ã€å·¥ä½œæµç¨‹ç®¡ç†",
    "problem_domain_quote": "enterprise application scenarios, enterprise deployment readiness, enterprise workflow automation",
    "problem_difficulty": "ä¸¤ä¸ªéš¾åº¦å±‚çº§ï¼šç®€å•UIäº¤äº’å’Œå¤æ‚å·¥ä½œæµç¨‹",
    "problem_difficulty_quote": "comprising 226 tasks across two difficulty tiers: simple UI interactions (136 tasks) and complex workflows (90 tasks)",
    "language": "ä¸ç‰¹å®šäºç¼–ç¨‹è¯­è¨€ï¼Œä¸»è¦å…³æ³¨UIäº¤äº’",
    "language_quote": NaN,
    "data_size": "226ä¸ªä»»åŠ¡ï¼Œå…¶ä¸­136ä¸ªç®€å•UIäº¤äº’ä»»åŠ¡ï¼Œ90ä¸ªå¤æ‚å·¥ä½œæµç¨‹ä»»åŠ¡",
    "data_size_quote": "comprising 226 tasks across two difficulty tiers: simple UI interactions (136 tasks) and complex workflows (90 tasks comprising 50 copy-paste/business-process tasks and 40 enterprise application tasks)",
    "source_type": "åŸºäºä¼ä¸šåº”ç”¨åœºæ™¯çš„ç³»ç»Ÿæ„å»ºï¼ŒåŒ…å«æ¨¡æ‹Ÿçš„ä¼ä¸šç³»ç»Ÿå·¥ä½œæµç¨‹",
    "source_type_quote": "faithful mocks of complex enterprise workflows to test coordination and operational reliability, including testing with workflows from SAP, Workday, and other enterprise systems",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.17131v1 [cs.SE] 21 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we designed UI-CUBE, a new benchmark tailored to enterprise-like conditions, and used it to evaluate our own CUA implementation",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "UIäº¤äº’è‡ªåŠ¨åŒ–ã€å·¥ä½œæµç¨‹æ‰§è¡Œ",
    "task_granularity_quote": "systematic coverage of atomic UI interactions to map interface-level capabilities, and faithful mocks of complex enterprise workflows to test coordination and operational reliability",
    "evaluation_metrics": "ä»»åŠ¡æˆåŠŸç‡",
    "evaluation_metrics_quote": "Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%",
    "input_modality": "UIç•Œé¢ã€ä»»åŠ¡æŒ‡ä»¤",
    "input_modality_quote": "systematic interface variation coverage, multi-resolution testing",
    "output_modality": "UIäº¤äº’åŠ¨ä½œã€å·¥ä½œæµç¨‹æ‰§è¡Œç»“æœ",
    "output_modality_quote": "automated validation of task success through the application state",
    "task_io_type": "ä»»åŠ¡æŒ‡ä»¤åˆ°UIäº¤äº’",
    "task_io_type_quote": "measuring task completion effectively, they provide limited assessment of enterprise deployment readiness",
    "execution_environment": "ä¼ä¸šè½¯ä»¶ç¯å¢ƒã€å¤šåˆ†è¾¨ç‡æ˜¾ç¤º",
    "execution_environment_quote": "All tasks are evaluated across multiple screen resolutions to assess agent consistency under different display conditions",
    "unique_features": "ä¸“æ³¨äºä¼ä¸šçº§éƒ¨ç½²å‡†å¤‡åº¦è¯„ä¼°ï¼Œæ­ç¤ºèƒ½åŠ›æ‚¬å´–ç°è±¡è€Œéæ¸è¿›æ€§èƒ½ä¸‹é™ï¼Œæä¾›ç³»ç»Ÿç•Œé¢åˆ†ç±»è¦†ç›–",
    "unique_features_quote": "reveals a sharp capability cliff rather than gradual performance degradation, systematic interface variation coverage, multi-resolution testing",
    "data_size_quantity": 226,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['ä¸ç‰¹å®šäºç¼–ç¨‹è¯­è¨€', 'ä¸»è¦å…³æ³¨UIäº¤äº’']",
    "dimension_normalized": "['åŠŸèƒ½æ­£ç¡®æ€§', 'æ“ä½œå¯é æ€§', 'ç•Œé¢é€‚åº”æ€§', 'å·¥ä½œæµåè°ƒèƒ½åŠ›']",
    "evaluation_method_normalized": "['ä»»åŠ¡æˆåŠŸç‡']",
    "problem_domain_normalized": "['ä¼ä¸šè½¯ä»¶è‡ªåŠ¨åŒ–', 'UIäº¤äº’', 'å·¥ä½œæµç¨‹ç®¡ç†']",
    "source_type_normalized": "['åŸºäºä¼ä¸šåº”ç”¨åœºæ™¯çš„ç³»ç»Ÿæ„å»º', 'åŒ…å«æ¨¡æ‹Ÿçš„ä¼ä¸šç³»ç»Ÿå·¥ä½œæµç¨‹']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.15755_output/content.md",
    "benchmark_name": "MyAntFarm.ai",
    "benchmark_name_quote": "To test this hypothesis, we present MyAntFarm.ai, a reproducible experimental framework enabling controlled comparison of three conditions",
    "dataset_url": "https://github.com/Phildram1/myantfarm-ai",
    "dataset_url_quote": "Source code, Docker configurations, and trial outputs are available at: https://github.com/Phildram1/myantfarm-ai",
    "task_description": "è¯„ä¼°å¤šæ™ºèƒ½ä½“LLMç¼–æ’åœ¨äº‹ä»¶å“åº”ä¸­çš„å†³ç­–æ”¯æŒè´¨é‡ï¼Œé€šè¿‡æ¨¡æ‹Ÿè®¤è¯æœåŠ¡å›å½’äº‹ä»¶æ¥æµ‹è¯•ç³»ç»Ÿç”Ÿæˆå¯æ‰§è¡Œå»ºè®®çš„èƒ½åŠ›",
    "task_description_quote": "multi-agent orchestration fundamentally transforms LLM-based incident response quality... achieve 100% actionable recommendation rate",
    "dimension": "å†³ç­–è´¨é‡ã€è¡ŒåŠ¨ç‰¹å¼‚æ€§ã€è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§ã€ç”Ÿäº§å°±ç»ªæ€§",
    "dimension_quote": "We introduce Decision Quality (DQ), a multi-dimensional metric capturing validity, specificity, and correctness",
    "evaluation_method": "ä½¿ç”¨å†³ç­–è´¨é‡(DQ)æŒ‡æ ‡ï¼Œç»“åˆæœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§å’Œæ­£ç¡®æ€§ä¸‰ä¸ªç»´åº¦è¿›è¡Œè‡ªåŠ¨åŒ–è¯„åˆ†",
    "evaluation_method_quote": "DQ measures actionability through three dimensions... Validity, Specificity, Correctness",
    "context_dependency": "å•äº‹ä»¶åœºæ™¯ï¼ŒåŸºäºç‰¹å®šçš„äº‹ä»¶é¥æµ‹æ•°æ®",
    "context_dependency_quote": "All 348 trials used identical context to isolate orchestration effects from scenario variability",
    "problem_domain": "è¿ç»´æ™ºèƒ½(AIOps)ã€äº‹ä»¶å“åº”ã€ç”Ÿäº§ç¯å¢ƒæ•…éšœè¯Šæ–­",
    "problem_domain_quote": "Modern operational teams face a critical gap between incident detection and actionable comprehension... incident response",
    "problem_difficulty": "ç”Ÿäº§çº§äº‹ä»¶å“åº”ï¼Œéœ€è¦å…·ä½“çš„å¯æ‰§è¡Œå‘½ä»¤",
    "problem_difficulty_quote": "time-critical operational contexts... production deployment",
    "language": "ä¸»è¦ä½¿ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œäº‹ä»¶æè¿°å’Œå“åº”ç”Ÿæˆ",
    "language_quote": "The LLM generates a single unstructured text response attempting to address all objectives",
    "data_size": "348æ¬¡è¯•éªŒï¼Œæ¯ä¸ªæ¡ä»¶116æ¬¡è¯•éªŒ",
    "data_size_quote": "Through 348 controlled trials... 116 trials per condition",
    "source_type": "äººå·¥æ„å»ºçš„æ¨¡æ‹Ÿäº‹ä»¶åœºæ™¯",
    "source_type_quote": "All 348 trials used identical context... Authentication service regression post-deployment",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.15755v1 [cs.AI] 19 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºçš„ç ”ç©¶æ¡†æ¶",
    "build_type_quote": "we present MyAntFarm.ai, a reproducible experimental framework",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "äº‹ä»¶å“åº”å†³ç­–æ”¯æŒï¼ŒåŒ…æ‹¬è¯Šæ–­ã€è§„åˆ’å’Œé£é™©è¯„ä¼°",
    "task_granularity_quote": "coordinating specialized LLM agents for diagnosis, planning, and risk assessment",
    "evaluation_metrics": "å†³ç­–è´¨é‡(DQ)ã€æ—¶é—´åˆ°å¯ç”¨ç†è§£(T2U)ã€æœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§ã€æ­£ç¡®æ€§",
    "evaluation_metrics_quote": "We introduce Decision Quality (DQ), a multi-dimensional metric capturing validity, specificity, and correctness",
    "input_modality": "äº‹ä»¶é¥æµ‹æ•°æ®å’Œè‡ªç„¶è¯­è¨€æè¿°",
    "input_modality_quote": "Given the following telemetry: Service: auth-service v2.4.0, Error rate: 45%...",
    "output_modality": "ç»“æ„åŒ–çš„äº‹ä»¶ç®€æŠ¥ï¼ŒåŒ…å«æ ¹æœ¬åŸå› ã€å»ºè®®è¡ŒåŠ¨å’Œé£é™©è¯„ä¼°",
    "output_modality_quote": "A coordinator aggregates the three agent outputs into a structured incident brief containing root cause, recommended actions, and risk assessment",
    "task_io_type": "äº‹ä»¶æ•°æ®åˆ°å†³ç­–å»ºè®®",
    "task_io_type_quote": "bridging the gap between detection and actionable comprehension",
    "execution_environment": "å®¹å™¨åŒ–å¾®æœåŠ¡æ¡†æ¶ï¼Œä½¿ç”¨Docker Composeç¼–æ’",
    "execution_environment_quote": "MyAntFarm.ai consists of five containerized microservices orchestrated via Docker Compose",
    "unique_features": "ä¸“æ³¨äºå¤šæ™ºèƒ½ä½“ç¼–æ’åœ¨äº‹ä»¶å“åº”ä¸­çš„ç¡®å®šæ€§è´¨é‡ä¼˜åŠ¿ï¼Œå¼•å…¥å†³ç­–è´¨é‡(DQ)è¿™ä¸€æ–°çš„è¯„ä¼°æŒ‡æ ‡",
    "unique_features_quote": "multi-agent systems exhibit zero quality variance across all trials, making them production-ready... We introduce Decision Quality (DQ), a multi-dimensional metric capturing validity, specificity, and correctness",
    "data_size_quantity": 348,
    "data_size_unit": "æ¬¡è¯•éªŒ",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['è‡ªç„¶è¯­è¨€']",
    "dimension_normalized": "['å†³ç­–è´¨é‡', 'è¡ŒåŠ¨ç‰¹å¼‚æ€§', 'è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§', 'ç”Ÿäº§å°±ç»ªæ€§']",
    "evaluation_method_normalized": "['å†³ç­–è´¨é‡(DQ)', 'æ—¶é—´åˆ°å¯ç”¨ç†è§£(T2U)', 'æœ‰æ•ˆæ€§', 'ç‰¹å¼‚æ€§', 'æ­£ç¡®æ€§']",
    "problem_domain_normalized": "['è¿ç»´æ™ºèƒ½(AIOps)', 'äº‹ä»¶å“åº”', 'ç”Ÿäº§ç¯å¢ƒæ•…éšœè¯Šæ–­']",
    "source_type_normalized": "['äººå·¥æ„å»ºçš„æ¨¡æ‹Ÿäº‹ä»¶åœºæ™¯']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.16708_output/content.md",
    "benchmark_name": "CodeX-Verify",
    "benchmark_name_quote": "We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "å¤šæ™ºèƒ½ä½“ä»£ç éªŒè¯ç³»ç»Ÿï¼Œä½¿ç”¨å››ä¸ªä¸“é—¨åŒ–çš„æ™ºèƒ½ä½“æ¥æ£€æµ‹ä¸åŒç±»å‹çš„ä»£ç é”™è¯¯",
    "task_description_quote": "We built CodeX-Verify, a system that runs four specialized agents in parallel: Correctness (logic errors, edge cases, exception handling), Security (OWASP Top 10, CWE patterns, secrets), Performance (algorithmic complexity, resource leaks), and Style (maintainability, documentation). Each agent looks for different bug types.",
    "dimension": "ä»£ç æ­£ç¡®æ€§ã€å®‰å…¨æ€§ã€æ€§èƒ½å’Œé£æ ¼å››ä¸ªç»´åº¦çš„é”™è¯¯æ£€æµ‹",
    "dimension_quote": "Correctness (logic errors, edge cases, exception handling), Security (OWASP Top 10, CWE patterns, secrets), Performance (algorithmic complexity, resource leaks), and Style (maintainability, documentation)",
    "evaluation_method": "åœ¨99ä¸ªå¸¦æœ‰éªŒè¯æ ‡ç­¾çš„ä»£ç æ ·æœ¬ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæµ‹é‡çœŸé˜³æ€§ç‡å’Œå‡†ç¡®ç‡",
    "evaluation_method_quote": "Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€ä»£ç éªŒè¯ã€æ¼æ´æ£€æµ‹",
    "problem_domain_quote": "Multi-Agent Code Verification with Compound Vulnerability Detection",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": "99ä¸ªä»£ç æ ·æœ¬ï¼Œè¦†ç›–16ä¸ªé”™è¯¯ç±»åˆ«",
    "data_size_quote": "Dataset of 99 code samples with verified labels covering 16 bug categories",
    "source_type": "æ¥è‡ªçœŸå®SWE-benchå¤±è´¥çš„ä»£ç æ ·æœ¬",
    "source_type_quote": "covering 16 bug categories from real SWE-bench failures",
    "last_updated": 2025,
    "last_updated_quote": "October 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We built CodeX-Verify",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "å¼€æºå‘å¸ƒ",
    "dataset_license_quote": "released open-source",
    "task_granularity": "ä»£ç éªŒè¯å’Œé”™è¯¯æ£€æµ‹",
    "task_granularity_quote": "detect different types of bugs",
    "evaluation_metrics": "çœŸé˜³æ€§ç‡ã€å‡†ç¡®ç‡ã€å‡é˜³æ€§ç‡",
    "evaluation_metrics_quote": "catches 76.1% of bugs, achieving 76.1% TPR with 68.7% accuracy (Â±9.1% CI)",
    "input_modality": "ä»£ç ",
    "input_modality_quote": "code samples",
    "output_modality": "é”™è¯¯æ£€æµ‹ç»“æœ",
    "output_modality_quote": "detect different types of bugs",
    "task_io_type": "ä»£ç åˆ°é”™è¯¯æ£€æµ‹ç»“æœ",
    "task_io_type_quote": "analyzes code c âˆˆC through domain-specific function Ï•i : C â†’Oi, producing observation Ai = Ï•i(c) and decision Di âˆˆ{0, 1}",
    "execution_environment": "æ— éœ€æ‰§è¡Œä»£ç çš„é™æ€åˆ†æ",
    "execution_environment_quote": "running faster and without executing code",
    "unique_features": "å¤šæ™ºèƒ½ä½“ååŒéªŒè¯ã€å¤åˆæ¼æ´é£é™©å»ºæ¨¡ã€æ•°å­¦ç†è®ºè¯æ˜ã€æµ‹è¯•15ç§æ™ºèƒ½ä½“ç»„åˆ",
    "unique_features_quote": "We tested all 15 combinations of agents: single agents (4 configs), pairs (6 configs), triples (4 configs), and the full system. We formalize how multiple vulnerabilities in the same code create exponentially more risk. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent.",
    "data_size_quantity": 99,
    "data_size_unit": "ä¸ªä»£ç æ ·æœ¬",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç æ­£ç¡®æ€§', 'å®‰å…¨æ€§', 'æ€§èƒ½', 'é£æ ¼']",
    "evaluation_method_normalized": "['çœŸé˜³æ€§ç‡', 'å‡†ç¡®ç‡', 'å‡é˜³æ€§ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç éªŒè¯', 'æ¼æ´æ£€æµ‹']",
    "source_type_normalized": "['æ¥è‡ªçœŸå®SWE-benchå¤±è´¥çš„ä»£ç æ ·æœ¬']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.21509_output/content.md",
    "benchmark_name": "SV-LIB 1.0",
    "benchmark_name_quote": "SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks",
    "dataset_url": "https://gitlab.com/sosy-lab/benchmarking/sv-lib",
    "dataset_url_quote": "https://gitlab.com/sosy-lab/benchmarking/sv-lib",
    "task_description": "è½¯ä»¶éªŒè¯ä»»åŠ¡çš„äº¤æ¢æ ¼å¼å’Œä¸­é—´è¯­è¨€ï¼ŒåŒ…æ‹¬ç¨‹åºã€è§„èŒƒå’ŒéªŒè¯è§è¯",
    "task_description_quote": "we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses.",
    "dimension": "è½¯ä»¶éªŒè¯ã€ç¨‹åºåˆ†æã€äº¤æ¢æ ¼å¼ã€éªŒè¯è§è¯",
    "dimension_quote": "Additional Key Words and Phrases: Software verification, Program analysis, Exchange format, Witness, Certifying Algorithm, Intermediate language, SV-LIB, SMT-LIB",
    "evaluation_method": NaN,
    "evaluation_method_quote": NaN,
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶éªŒè¯ã€ç¨‹åºåˆ†æ",
    "problem_domain_quote": "Additional Key Words and Phrases: Software verification, Program analysis, Exchange format, Witness, Certifying Algorithm, Intermediate language, SV-LIB, SMT-LIB",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "åŸºäºå‘½ä»¤å¼ç¼–ç¨‹è¯­è¨€æ¦‚å¿µï¼Œä½¿ç”¨SMT-LIBè¡¨ç¤ºè¡¨è¾¾å¼å’Œç±»å‹",
    "language_quote": "SV-LIB is based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2025-11-26",
    "last_updated_quote": "Version 1.0 SV-LIB 2025-11-26",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we propose SV-LIB, an exchange format and intermediate language for software-verification tasks",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "è½¯ä»¶éªŒè¯ä»»åŠ¡äº¤æ¢æ ¼å¼",
    "task_granularity_quote": "SV-LIB, an exchange format and intermediate language for software-verification tasks",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "ç¨‹åºã€è§„èŒƒå’ŒéªŒè¯è§è¯",
    "input_modality_quote": "including programs, specifications, and verification witnesses",
    "output_modality": "éªŒè¯ç»“æœå’Œè§è¯",
    "output_modality_quote": "SV-LIB defines a witness format for both correct and incorrect SV-LIB programs",
    "task_io_type": "è½¯ä»¶éªŒè¯ä»»åŠ¡åˆ°éªŒè¯ç»“æœ",
    "task_io_type_quote": "exchange format and intermediate language for software-verification tasks",
    "execution_environment": "åŸºäºSMTæ±‚è§£å™¨çš„éªŒè¯å·¥å…·åŸºç¡€è®¾æ–½",
    "execution_environment_quote": "This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already.",
    "unique_features": "å®šä¹‰äº†æ­£ç¡®å’Œé”™è¯¯ç¨‹åºçš„è§è¯æ ¼å¼ï¼Œæ”¯æŒç‹¬ç«‹è§è¯éªŒè¯å™¨ï¼Œå¯é‡ç”¨éªŒè¯å™¨ä½œä¸ºè§è¯éªŒè¯å™¨",
    "unique_features_quote": "Furthermore, SV-LIB defines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 11,
    "last_updated_day": 26,
    "language_normalized": "['SMT-LIB']",
    "dimension_normalized": "['è½¯ä»¶éªŒè¯', 'ç¨‹åºåˆ†æ', 'äº¤æ¢æ ¼å¼', 'éªŒè¯è§è¯']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['è½¯ä»¶éªŒè¯', 'ç¨‹åºåˆ†æ']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.15817_output/content.md",
    "benchmark_name": "CodeSmellEval",
    "benchmark_name_quote": "The CodeSmellEval benchmark [37] proposed the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating specific types of smell.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä»£ç çš„ç»“æ„è´¨é‡ï¼Œç‰¹åˆ«æ˜¯å…¶äº§ç”Ÿä»£ç å¼‚å‘³ï¼ˆå³å½±å“å¯è¯»æ€§ã€å¯ç»´æŠ¤æ€§æˆ–è®¾è®¡å®Œæ•´æ€§çš„æ¨¡å¼ï¼‰çš„å€¾å‘æ€§ã€‚",
    "task_description_quote": "This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality.",
    "dimension": "ä»£ç ç»“æ„è´¨é‡ï¼Œå…·ä½“ä¸ºä»£ç å¼‚å‘³çš„å€¾å‘æ€§ã€‚",
    "dimension_quote": "We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality.",
    "evaluation_method": "ä½¿ç”¨å€¾å‘æ€§å¼‚å‘³åˆ†æ•°ï¼ˆPSCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ¨¡å‹åœ¨è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­é¢„æµ‹çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œæ¦‚ç‡çš„æ¦‚ç‡åº¦é‡ã€‚",
    "evaluation_method_quote": "The Propensity Smelly Score (PSC) is a probabilistic metric that estimates the likelihood that a LLM generates a specific type of code smell [37]. It relies on the next-token probabilities predicted by the model during autoregressive generation.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œä»£ç è´¨é‡è¯„ä¼°ã€‚",
    "problem_domain_quote": "Code smells offer a foundation for evaluating these broader quality concerns. They capture recurring design and implementation issues that affect readability, complexity, and long-term maintainability [8, 23, 36].",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Pythonï¼ˆä»ç¤ºä¾‹ä»£ç ç‰‡æ®µæ¨æ–­ï¼‰",
    "language_quote": "Figure 1: Propensity Smelly Score (SCM) Computation. The python snippet at the bottom contains two code smells: C0103 (i.e., invalid-name) and C0415 (i.e., import-outside-toplevel).",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2026",
    "last_updated_quote": "In 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE â€™26), April 12â€“18, 2026, Rio de Janeiro, Brazil.",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆåŸºäºå…ˆå‰å·¥ä½œï¼‰",
    "build_type_quote": "The CodeSmellEval benchmark [37] proposed the Propensity Smelly Score (PSC)...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç¦æ­¢æ¼”ç» 4.0 å›½é™…è®¸å¯åè®®",
    "dataset_license_quote": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "LLMs have rapidly moved from experimental tools to everyday assistants in software engineering (SE) [38]. Developers rely on them for many tasks such as code completion [5, 16, 41], summarization [1], program repair [11], clone detection [40] and test generation [39].",
    "evaluation_metrics": "å€¾å‘æ€§å¼‚å‘³åˆ†æ•°ï¼ˆPSCï¼‰",
    "evaluation_metrics_quote": "We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types...",
    "input_modality": NaN,
    "input_modality_quote": NaN,
    "output_modality": "ä»£ç ",
    "output_modality_quote": "This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code.",
    "task_io_type": NaN,
    "task_io_type_quote": NaN,
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "è¯¥åŸºå‡†ä¸“æ³¨äºä»£ç å¼‚å‘³ï¼ˆç»“æ„è´¨é‡é—®é¢˜ï¼‰çš„å€¾å‘æ€§è¯„ä¼°ï¼Œè€Œéä¼ ç»Ÿçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªæ¦‚ç‡æ€§åº¦é‡ï¼ˆPSCï¼‰ï¼Œç”¨äºä¼°è®¡æ¨¡å‹ç”Ÿæˆç‰¹å®šç±»å‹å¼‚å‘³çš„å¯èƒ½æ€§ï¼Œå¹¶å¯ä½œä¸ºå› æœåˆ†æçš„å·¥å…·æ¥ç†è§£ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å½±å“å› ç´ ã€‚",
    "unique_features_quote": "Instead of treating PSC as a standalone benchmark metric, we use it as an instrument for reasoning about the factors that influence smell propensity in LLM-generated code. Our study begins by assessing the robustness and explanatory value of PSC, establishing that it provides a stable foundation for causal analysis.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç»“æ„è´¨é‡', 'ä»£ç å¼‚å‘³çš„å€¾å‘æ€§']",
    "evaluation_method_normalized": "['å€¾å‘æ€§å¼‚å‘³åˆ†æ•°ï¼ˆPSCï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç è´¨é‡è¯„ä¼°']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "CC-BY-SA 4.0",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.03421_output/content.md",
    "benchmark_name": "BugT",
    "benchmark_name_quote": "This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",
    "dataset_url": "https://github.com/Xucranger/PLofLBFL",
    "dataset_url_quote": "To facilitate future study, we share our source code and experimental data in a GitHub repository 1. 1https://github.com/Xucranger/PLofLBFL",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆå­¦è€…ç¨‹åºæ•…éšœå®šä½ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚",
    "task_description_quote": "This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets.",
    "dimension": "æ•…éšœå®šä½çš„å‡†ç¡®æ€§ã€æ•ˆç‡ã€å¯ç”¨æ€§ï¼Œä»¥åŠå¯¹é—®é¢˜éš¾åº¦çš„é²æ£’æ€§ã€‚",
    "dimension_quote": "To investigate these issues, we aim to empirically assess the performance of different LLMs across various datasets, evaluating their accuracy, efficiency, and usability in fault localization for novice programs.",
    "evaluation_method": "åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°LLMsçš„æ•…éšœå®šä½æ€§èƒ½ï¼Œå¹¶ä¸ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚SBFLã€MBFLï¼‰è¿›è¡Œæ¯”è¾ƒã€‚",
    "evaluation_method_quote": "This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets. All closed-source LLMs outperform traditional SBFL and MBFL methods...",
    "context_dependency": "é’ˆå¯¹åŒ…å«é”™è¯¯çš„åˆå­¦è€…ç¨‹åºä»£ç ç‰‡æ®µã€‚",
    "context_dependency_quote": "Novice Program [18] refers to code written by novice programmers, usually in the early stages of learning programming. These programs usually contain multiple errors...",
    "problem_domain": "ç¼–ç¨‹æ•™è‚²ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åˆå­¦è€…çš„æ•…éšœå®šä½å’Œè°ƒè¯•ã€‚",
    "problem_domain_quote": "Novice programming assistance [19] is a crucial educational approach aimed at helping students develop their fundamental coding skills and enhance problem-solving abilities.",
    "problem_difficulty": "åŒ…å«ä¸åŒéš¾åº¦çº§åˆ«çš„é—®é¢˜ï¼Œä»ç®€å•åˆ°å¤æ‚ã€‚",
    "problem_difficulty_quote": "LLM accuracy decreases as problem difficulty increases in Codeflaws and Condefects, but top models maintain high accuracy even at peak difficulty in BugT, suggesting its lower complexity.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "è‡ªå»ºçš„åŒ…å«çœŸå®ç¼–ç¨‹é”™è¯¯çš„æ•°æ®é›†ã€‚",
    "source_type_quote": "We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMsâ€™ capabilities.",
    "last_updated": "2025",
    "last_updated_quote": "Preprint submitted to Journal of LATEX Templates December 4, 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We introduce a new self-created dataset with real programming faults...",
    "contamination_status": "ä¸“é—¨è®¾è®¡ä»¥å‡è½»æ•°æ®æ³„éœ²é—®é¢˜ã€‚",
    "contamination_status_quote": "...with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ•…éšœå®šä½",
    "task_granularity_quote": "This study evaluates the fault localization performance...",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "åŒ…å«é”™è¯¯çš„ä»£ç ",
    "input_modality_quote": "Novice Program [18] refers to code written by novice programmers, usually in the early stages of learning programming. These programs usually contain multiple errors...",
    "output_modality": "æ•…éšœä½ç½®å’Œè§£é‡Š",
    "output_modality_quote": "LLM generated fault explanations demonstrate significant value for novice programmer assistance...",
    "task_io_type": "ä»£ç åˆ°æ–‡æœ¬ï¼ˆæ•…éšœä½ç½®/è§£é‡Šï¼‰",
    "task_io_type_quote": "LLM generated fault explanations demonstrate significant value for novice programmer assistance...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“é—¨ä¸ºåˆå­¦è€…ç¼–ç¨‹æ•™è‚²è®¾è®¡ï¼Œæ—¨åœ¨å‡è½»æ•°æ®æ³„éœ²é—®é¢˜ï¼Œå¹¶åŒ…å«çœŸå®ç¼–ç¨‹é”™è¯¯ã€‚",
    "unique_features_quote": "...with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ...We introduce a new self-created dataset with real programming faults...",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['æ•…éšœå®šä½çš„å‡†ç¡®æ€§', 'æ•ˆç‡', 'å¯ç”¨æ€§', 'å¯¹é—®é¢˜éš¾åº¦çš„é²æ£’æ€§']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['ç¼–ç¨‹æ•™è‚²', 'é’ˆå¯¹åˆå­¦è€…çš„æ•…éšœå®šä½å’Œè°ƒè¯•']",
    "source_type_normalized": "['è‡ªå»ºçš„åŒ…å«çœŸå®ç¼–ç¨‹é”™è¯¯çš„æ•°æ®é›†']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "ä»£ç åˆ°æ–‡æœ¬",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.05073_output/content.md",
    "benchmark_name": "Comprehensive Verilog Design Problems (CVDP)",
    "benchmark_name_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€è§„èŒƒç”Ÿæˆç»è¿‡éªŒè¯çš„å¯„å­˜å™¨ä¼ è¾“çº§ï¼ˆRTLï¼‰ç¡¬ä»¶è®¾è®¡å®ç°ã€‚",
    "task_description_quote": "transform design intent from the CVDP dataset into verified register transfer level (RTL) implementations.",
    "dimension": "ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆå’Œä»£ç ç†è§£ã€‚",
    "dimension_quote": "We evaluate SLM models (1.7Bâ€“20B) across code generation and comprehension, establishing baseline performance.",
    "evaluation_method": "ä½¿ç”¨åŸºäºCocoTBçš„æµ‹è¯•å¥—ä»¶è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "evaluation_method_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "context_dependency": "å•æ¨¡å—è®¾è®¡é—®é¢˜ï¼ŒåŒ…å«æ¥å£è§„èŒƒå’ŒåŠŸèƒ½è¦æ±‚ã€‚",
    "context_dependency_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "problem_domain": "ç¡¬ä»¶è®¾è®¡ï¼Œå…·ä½“ä¸ºVerilogç¡¬ä»¶æè¿°è¯­è¨€ç¼–ç¨‹ã€‚",
    "problem_domain_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "problem_difficulty": "ä»£è¡¨ç°å®ä¸–ç•Œå¤æ‚æ€§çš„ç”Ÿäº§çº§IPå—ã€‚",
    "problem_difficulty_quote": "Derived from production IP blocks, it represents realistic complexity.",
    "language": "Verilogç¡¬ä»¶æè¿°è¯­è¨€ã€‚",
    "language_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "data_size": "åŒ…å«336ä¸ªé—®é¢˜ã€‚",
    "data_size_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "source_type": "æºè‡ªNVIDIAçš„ç”Ÿäº§IPå—ã€‚",
    "source_type_quote": "Derived from production IP blocks, it represents realistic complexity.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±NVIDIAå¼€å‘ï¼‰ã€‚",
    "build_type_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è§„èŒƒç”ŸæˆRTLä»£ç ï¼‰ã€‚",
    "task_granularity_quote": "transform design intent from the CVDP dataset into verified register transfer level (RTL) implementations.",
    "evaluation_metrics": "é€šè¿‡ç‡ï¼ˆpass rateï¼‰ã€‚",
    "evaluation_metrics_quote": "State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot), highlighting substantial improvement opportunity.",
    "input_modality": "è‡ªç„¶è¯­è¨€è§„èŒƒã€æ¨¡å—æ¥å£å’ŒåŠŸèƒ½è¦æ±‚ã€‚",
    "input_modality_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "output_modality": "Verilog RTLä»£ç ã€‚",
    "output_modality_quote": "transform design intent from the CVDP dataset into verified register transfer level (RTL) implementations.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ã€‚",
    "task_io_type_quote": "transform design intent from the CVDP dataset into verified register transfer level (RTL) implementations.",
    "execution_environment": "åŸºäºCocoTBçš„æµ‹è¯•å¥—ä»¶ã€‚",
    "execution_environment_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "unique_features": "ä¸“æ³¨äºç¡¬ä»¶è®¾è®¡ï¼ˆVerilogï¼‰çš„åŸºå‡†ï¼Œé—®é¢˜æºè‡ªç°å®ä¸–ç•Œçš„ç”Ÿäº§IPå—ï¼Œæ¶µç›–ç®—æœ¯è¿ç®—ã€æ§åˆ¶é€»è¾‘ã€å†…å­˜ç³»ç»Ÿç­‰å¤šç§è®¾è®¡ç±»åˆ«ã€‚",
    "unique_features_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs. Derived from production IP blocks, it represents realistic complexity.",
    "data_size_quantity": 336,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Verilogç¡¬ä»¶æè¿°è¯­è¨€']",
    "dimension_normalized": "['ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–èƒ½åŠ›', 'ä»£ç ç”Ÿæˆ', 'ä»£ç ç†è§£']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡ï¼ˆpass rateï¼‰']",
    "problem_domain_normalized": "['ç¡¬ä»¶è®¾è®¡', 'Verilogç¡¬ä»¶æè¿°è¯­è¨€ç¼–ç¨‹']",
    "source_type_normalized": "['æºè‡ªNVIDIAçš„ç”Ÿäº§IPå—']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.16787_output/content.md",
    "benchmark_name": "BLP-2025 Shared Task on Code Generation from Bangla Instructions",
    "benchmark_name_quote": "This paper presents the winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ ¹æ®ç»™å®šçš„å­ŸåŠ æ‹‰è¯­æŒ‡ä»¤ç”ŸæˆPythonä»£ç ã€‚æŒ‡ä»¤åŒ…å«å­ŸåŠ æ‹‰è¯­æè¿°ã€å‡½æ•°åå’Œå‚æ•°åã€‚",
    "task_description_quote": "The goal of the shared task is to generate Python code from a given Bangla instruction. The instruction itself contains the given Bangla instruction, function name, and argument names.",
    "dimension": "ä»éè‹±è¯­ï¼ˆå­ŸåŠ æ‹‰è¯­ï¼‰è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "This imbalance narrows access to program-synthesis tools for non-English speakers and limits our understanding of how linguistic factors â€“ such as morphology, script variation, and code-mixing â€“ impact the path from instructions to executable programs.",
    "evaluation_method": "ä½¿ç”¨pytesté£æ ¼çš„å•å…ƒæµ‹è¯•è¯„ä¼°ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Œé‡‡ç”¨Pass@1æŒ‡æ ‡",
    "evaluation_method_quote": "The candidate program is then executed against the provided unit tests (pytest-style, assert-based). ... For this task, the evaluation metric is Pass@1 (Chen et al., 2021).",
    "context_dependency": "å•å‡½æ•°ç”Ÿæˆï¼Œä¾èµ–æŒ‡ä»¤å’Œå•å…ƒæµ‹è¯•",
    "context_dependency_quote": "A candidate program must define the function precisely as provided by the function name since pytest is designed with the same function names.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜",
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "This paper presents the winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions, which consists of a multi-agent pipeline. First, a code-generation agent produces an initial solution from the input instruction.",
    "data_size": "å¼€å‘é›†400ä¸ªå­ŸåŠ æ‹‰è¯­æŒ‡ä»¤ï¼Œæµ‹è¯•é›†500ä¸ªå­ŸåŠ æ‹‰è¯­æŒ‡ä»¤",
    "data_size_quote": "Development dataset Our development set consists of 400 Bangla instructions paired with function names, each accompanied by three unit tests. ... Test dataset The test set contains 500 Bangla instructions with function names, each accompanied by a single unit test",
    "source_type": "ç”±ä»»åŠ¡ç»„ç»‡è€…æä¾›",
    "source_type_quote": "The organizers provide this dataset (Raihan et al., 2025a).",
    "last_updated": "2025",
    "last_updated_quote": "BLP-2025 Shared Task on Code Generation from Bangla Instructions",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆå…±äº«ä»»åŠ¡ç»„ç»‡è€…ï¼‰",
    "build_type_quote": "The organizers provide this dataset (Raihan et al., 2025a).",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "The goal of the shared task is to generate Python code from a given Bangla instruction.",
    "evaluation_metrics": "Pass@1",
    "evaluation_metrics_quote": "For this task, the evaluation metric is Pass@1 (Chen et al., 2021).",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆå­ŸåŠ æ‹‰è¯­ï¼‰",
    "input_modality_quote": "The goal of the shared task is to generate Python code from a given Bangla instruction.",
    "output_modality": "ä»£ç ï¼ˆPythonï¼‰",
    "output_modality_quote": "The goal of the shared task is to generate Python code from a given Bangla instruction.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "The goal of the shared task is to generate Python code from a given Bangla instruction.",
    "execution_environment": "pytestæµ‹è¯•ç¯å¢ƒ",
    "execution_environment_quote": "The candidate program is then executed against the provided unit tests (pytest-style, assert-based).",
    "unique_features": "ä¸“æ³¨äºå­ŸåŠ æ‹‰è¯­åˆ°Pythonä»£ç ç”Ÿæˆï¼Œå¡«è¡¥éè‹±è¯­ä»£ç ç”Ÿæˆè¯„æµ‹çš„ç©ºç™½",
    "unique_features_quote": "However, most benchmarks and systems remain vastly English-centric (Jiang et al., 2024). ... Bangla â€“ spoken by over 270 million people worldwide â€“ is an example of an inadequately supported language in this area.",
    "data_size_quantity": 900,
    "data_size_unit": "ä¸ªæŒ‡ä»¤",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»éè‹±è¯­ï¼ˆå­ŸåŠ æ‹‰è¯­ï¼‰è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['Pass@1']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "['ç”±ä»»åŠ¡ç»„ç»‡è€…æä¾›']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.05908_output/content.md",
    "benchmark_name": "DNext",
    "benchmark_name_quote": "Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "åœ¨å¤šä»“åº“å¾®æœåŠ¡æ¶æ„ä¸­ï¼Œæ ¹æ®è‡ªç„¶è¯­è¨€é”™è¯¯æŠ¥å‘Šå®šä½é”™è¯¯æ‰€åœ¨çš„ä»£ç æ–‡ä»¶ã€‚",
    "task_description_quote": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code... We propose reframing this as a natural language reasoning task... performing NL-to-NL search instead of cross-modal retrieval.",
    "dimension": "é”™è¯¯å®šä½çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šä»“åº“ç¯å¢ƒä¸‹çš„æœç´¢ç©ºé—´è·¯ç”±å’Œåˆ†å±‚å®šä½èƒ½åŠ›ã€‚",
    "dimension_quote": "Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories.",
    "evaluation_method": "ä½¿ç”¨Pass@kå’ŒRecall@kæŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œå…¶ä¸­k=10ç”¨äºæ–‡ä»¶å®šä½ï¼Œk=3ç”¨äºä»“åº“è·¯ç”±ã€‚åŒæ—¶ä½¿ç”¨å¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰ã€‚",
    "evaluation_method_quote": "We evaluate performance using standard metrics: Pass@k and Recall@k. Pass@k measures the proportion of bug reports where at least one correct file is found in the top-ğ‘˜ results. Recall@k measures the fraction of all correct files for a given bug that are successfully retrieved within the top-ğ‘˜ results. Given that the maximum number of modified files for any bug in our dataset is 10 (average 7.2), we set ğ‘˜= 10 as a fair and comprehensive threshold... For the preliminary search space routing phase, we use a tighter ğ‘˜= 3.",
    "context_dependency": "å¤šä»“åº“ã€å¤šæ–‡ä»¶é¡¹ç›®ï¼Œæ¶‰åŠæ–‡ä»¶ã€ç›®å½•å’Œä»“åº“ä¸‰ä¸ªå±‚æ¬¡ã€‚",
    "context_dependency_quote": "Our approach builds context-aware summaries at file, directory, and repository levels... This structured repository â†’directory â†’file search path provides an inherently transparent and auditable reasoning process.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œç‰¹åˆ«æ˜¯ç”µä¿¡é¢†åŸŸçš„å¾®æœåŠ¡æ¶æ„ç³»ç»Ÿã€‚",
    "problem_domain_quote": "Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",
    "problem_difficulty": "å·¥ä¸šçº§ï¼ŒåŒ…å«çœŸå®ã€å˜ˆæ‚çš„é”™è¯¯æŠ¥å‘Šï¼Œè§„æ¨¡å¤§ä¸”å¤æ‚ã€‚",
    "problem_difficulty_quote": "Its scale (see Table 1) and use of real-world, often noisy, bug reports provide a challenging benchmark that standard academic datasets cannot replicate.",
    "language": "Java",
    "language_quote": "Programming Language: Java",
    "data_size": "åŒ…å«46ä¸ªä»“åº“ï¼Œ7077ä¸ªä»£ç æ–‡ä»¶ï¼Œçº¦110ä¸‡è¡Œç‰©ç†ä»£ç ï¼Œ87ä¸ªé”™è¯¯å·¥å•ï¼Œå¹³å‡æ¯ä¸ªå·¥å•æ¶‰åŠ7.2ä¸ªé”™è¯¯æ–‡ä»¶ã€‚",
    "data_size_quote": "Number of Repositories: 46, Total Number of Code Files: 7,077, Total Physical Lines of Code: âˆ¼1.1M, Number of Bug Tickets: 87, Average Buggy Files per Ticket: 7.2",
    "source_type": "ä¸“æœ‰çš„å·¥ä¸šçº§å¾®æœåŠ¡ç³»ç»Ÿï¼Œé”™è¯¯å·¥å•çš„åŸºå‡†äº‹å®æ˜¯è§£å†³è¯¥é—®é¢˜çš„æ‹‰å–è¯·æ±‚ä¸­ä¿®æ”¹çš„æ–‡ä»¶é›†ã€‚",
    "source_type_quote": "Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system... The ground truth for each bug ticket is the set of files modified in the pull request that resolved the issue.",
    "last_updated": "2026",
    "last_updated_quote": "ICSE 2026, Rio de Janeiro, Brazil",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆä¸“æœ‰å·¥ä¸šç³»ç»Ÿï¼‰",
    "build_type_quote": "a proprietary, industrial-scale microservice system",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç æ£€ç´¢/å®šä½",
    "task_granularity_quote": "Bug Localization, Code Retrieval",
    "evaluation_metrics": "Pass@10, Recall@10, MRR (ç”¨äºæ–‡ä»¶å®šä½)ï¼›Pass@3, Recall@3, MRR (ç”¨äºä»“åº“è·¯ç”±)",
    "evaluation_metrics_quote": "We evaluate performance using standard metrics: Pass@k and Recall@k... we set ğ‘˜= 10... For the preliminary search space routing phase, we use a tighter ğ‘˜= 3.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆé”™è¯¯æŠ¥å‘Šï¼‰",
    "input_modality_quote": "natural language bug reports",
    "output_modality": "ä»£ç æ–‡ä»¶åˆ—è¡¨ï¼ˆæ’åï¼‰",
    "output_modality_quote": "produces the final ranked list of files most likely to be the source of the bug.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆæ£€ç´¢ï¼‰",
    "task_io_type_quote": "reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“ä¸ºå¤šä»“åº“å¾®æœåŠ¡æ¶æ„ä¸­çš„é”™è¯¯å®šä½è®¾è®¡ï¼Œä½¿ç”¨åˆ†å±‚è‡ªç„¶è¯­è¨€æ‘˜è¦ï¼ˆæ–‡ä»¶ã€ç›®å½•ã€ä»“åº“çº§ï¼‰å°†é—®é¢˜é‡æ„ä¸ºNL-to-NLæ¨ç†ä»»åŠ¡ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„ä»“åº“â†’ç›®å½•â†’æ–‡ä»¶æœç´¢è·¯å¾„ã€‚",
    "unique_features_quote": "Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories... This structured repository â†’directory â†’file search path provides an inherently transparent and auditable reasoning process.",
    "data_size_quantity": 1100000,
    "data_size_unit": "è¡Œ",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['é”™è¯¯å®šä½çš„å‡†ç¡®æ€§', 'é”™è¯¯å®šä½çš„æ•ˆç‡', 'å¤šä»“åº“ç¯å¢ƒä¸‹çš„æœç´¢ç©ºé—´è·¯ç”±', 'åˆ†å±‚å®šä½èƒ½åŠ›']",
    "evaluation_method_normalized": "['Pass@10', 'Recall@10', 'MRR', 'Pass@3', 'Recall@3', 'MRR']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ç”µä¿¡é¢†åŸŸçš„å¾®æœåŠ¡æ¶æ„ç³»ç»Ÿ']",
    "source_type_normalized": "['ä¸“æœ‰çš„å·¥ä¸šçº§å¾®æœåŠ¡ç³»ç»Ÿ', 'é”™è¯¯å·¥å•çš„åŸºå‡†äº‹å®æ˜¯è§£å†³è¯¥é—®é¢˜çš„æ‹‰å–è¯·æ±‚ä¸­ä¿®æ”¹çš„æ–‡ä»¶é›†']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.08867_output/content.md",
    "benchmark_name": "SimpleDevQA",
    "benchmark_name_quote": "we introduce SimpleDevQA, a multilingual Dev Knowledge QA benchmark derived from real user dialogues.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "å¼€å‘çŸ¥è¯†é—®ç­”ï¼Œæ—¨åœ¨ä¸ºè½¯ä»¶å¼€å‘è¿‡ç¨‹ä¸­æå‡ºçš„çŸ¥è¯†å¯»æ±‚é—®é¢˜æä¾›å‡†ç¡®çš„è‡ªç„¶è¯­è¨€ç­”æ¡ˆã€‚",
    "task_description_quote": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to address knowledge-seeking questions posed during the software development process by providing accurate and relevant natural language answers.",
    "dimension": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è½¯ä»¶å¼€å‘çŸ¥è¯†çš„ç†è§£èƒ½åŠ›ã€‚",
    "dimension_quote": "to assess LLMsâ€™ understanding capability of software development knowledge in real programming scenarios.",
    "evaluation_method": "é€šè¿‡æç¤ºä¸€ä¸ªç‹¬ç«‹çš„è¯„åˆ¤LLMï¼Œå°†æ¨¡å‹çš„é¢„æµ‹ä¸å‚è€ƒç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒä»¥éªŒè¯æ­£ç¡®æ€§ã€‚",
    "evaluation_method_quote": "we evaluate SimpleDevQA by prompting a separate judge LLM with the modelâ€™s prediction and the reference answer to verify correctness.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶å¼€å‘ï¼Œæ¶µç›–åº•å±‚ç³»ç»Ÿã€æ•°æ®åº“ã€ç½‘ç»œåè®®ã€ç®—æ³•åŸç†ç­‰å¹¿æ³›çŸ¥è¯†ï¼Œè€Œä¸ä»…ä»…æ˜¯ä»£ç ç†è§£ã€‚",
    "problem_domain_quote": "practical software development demands a much broader understanding, encompassing knowledge of underlying systems, databases, network protocols, algorithmic principles, etc.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "è‹±è¯­ã€ä¸­æ–‡å’Œä¿„è¯­ã€‚",
    "language_quote": "This dataset contains 2,740 QA pairs in three languages (English, Chinese, and Russian)",
    "data_size": "åŒ…å«2,740ä¸ªå¼€å‘çŸ¥è¯†é—®ç­”å¯¹ã€‚",
    "data_size_quote": "This dataset contains 2,740 QA pairs",
    "source_type": "æºè‡ªçœŸå®çš„ç”¨æˆ·å¯¹è¯ï¼ˆæ¥è‡ªWildChatæ•°æ®é›†ï¼‰å’Œç½‘ç»œæ£€ç´¢çš„å‚è€ƒæ–‡æ¡£ã€‚",
    "source_type_quote": "a Dev Knowledge QA benchmark built from real developer dialogues; Using these topics, we then retrieve relevant reference documents from the web",
    "last_updated": "2025",
    "last_updated_quote": "Publication date: December 2025.",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œé€šè¿‡ä¸€ä¸ªä¸‰é˜¶æ®µçš„æ•°æ®æ„å»ºæµç¨‹å°†çœŸå®å¯¹è¯è½¬åŒ–ä¸ºåŸºå‡†ã€‚",
    "build_type_quote": "we design and implement a three-phase data construction pipeline to convert real-world SE-related conversations into a Dev Knowledge QA benchmark.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å¼€å‘çŸ¥è¯†é—®ç­”",
    "task_granularity_quote": "Development Knowledge Question Answering (Dev Knowledge QA) task",
    "evaluation_metrics": "å‡†ç¡®æ€§ï¼ˆé€šè¿‡LLMè¯„åˆ¤éªŒè¯æ­£ç¡®æ€§ï¼‰",
    "evaluation_metrics_quote": "verify correctness",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜",
    "input_modality_quote": "questions inquiring about development knowledge",
    "output_modality": "è‡ªç„¶è¯­è¨€ç­”æ¡ˆ",
    "output_modality_quote": "providing accurate and relevant natural language answers",
    "task_io_type": "æ–‡æœ¬åˆ°æ–‡æœ¬ï¼ˆè‡ªç„¶è¯­è¨€é—®é¢˜åˆ°è‡ªç„¶è¯­è¨€ç­”æ¡ˆï¼‰",
    "task_io_type_quote": "knowledge-seeking questions... providing accurate and relevant natural language answers",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. æºè‡ªçœŸå®çš„å¼€å‘è€…å¯¹è¯ï¼Œåæ˜ çœŸå®çš„å¼€å‘è€…ä»»åŠ¡éœ€æ±‚å’ŒæŸ¥è¯¢æ¨¡å¼ã€‚2. ä¸“æ³¨äºå…·æœ‰å•ä¸€ã€æ˜ç¡®ã€å¯éªŒè¯ç­”æ¡ˆçš„é—®é¢˜ï¼Œä½¿è¯„ä¼°æ›´å‡†ç¡®ç®€å•ã€‚3. æ¶µç›–å¹¿æ³›çš„è½¯ä»¶å¼€å‘çŸ¥è¯†ï¼Œè€Œä¸ä»…ä»…æ˜¯ä»£ç ç†è§£ã€‚4. æ¯ä¸ªé—®ç­”å¯¹éƒ½é™„æœ‰å¤šä¸ªç½‘ç»œæ£€ç´¢çš„å‚è€ƒï¼Œå¯ç”¨äºéªŒè¯ç­”æ¡ˆçš„äº‹å®å‡†ç¡®æ€§ã€‚",
    "unique_features_quote": "SimpleDevQA, a Dev Knowledge QA benchmark built from real developer dialogues, to address the aforementioned problems; The benchmark focuses on questions with single, correct answers, making evaluation more accurate and simple; each QA pair is also accompanied by multiple web-retrieved references, which can be used to verify the factual accuracy of the answer.",
    "data_size_quantity": 2740,
    "data_size_unit": "ä¸ªå¼€å‘çŸ¥è¯†é—®ç­”å¯¹",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['è‹±è¯­', 'ä¸­æ–‡', 'ä¿„è¯­']",
    "dimension_normalized": "['è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è½¯ä»¶å¼€å‘çŸ¥è¯†çš„ç†è§£èƒ½åŠ›']",
    "evaluation_method_normalized": "['å‡†ç¡®æ€§ï¼ˆé€šè¿‡LLMè¯„åˆ¤éªŒè¯æ­£ç¡®æ€§ï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶å¼€å‘', 'æ¶µç›–åº•å±‚ç³»ç»Ÿ', 'æ•°æ®åº“', 'ç½‘ç»œåè®®', 'ç®—æ³•åŸç†ç­‰å¹¿æ³›çŸ¥è¯†', 'è€Œä¸ä»…ä»…æ˜¯ä»£ç ç†è§£']",
    "source_type_normalized": "['æºè‡ªçœŸå®çš„ç”¨æˆ·å¯¹è¯ï¼ˆæ¥è‡ªWildChatæ•°æ®é›†ï¼‰å’Œç½‘ç»œæ£€ç´¢çš„å‚è€ƒæ–‡æ¡£']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.10713_output/content.md",
    "benchmark_name": "PACIFIC",
    "benchmark_name_quote": "We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¡ºåºæŒ‡ä»¤éµå¾ªå’Œä»£ç å¹²è¿è¡Œï¼ˆå³æ¨¡æ‹Ÿä»£ç æ‰§è¡Œï¼‰èƒ½åŠ›ã€‚æ¨¡å‹éœ€è¦æ ¹æ®ç»™å®šçš„åˆå§‹è¾“å…¥å’Œä¸€ç³»åˆ—æŒ‡ä»¤ï¼Œæ‰§è¡ŒæŒ‡ä»¤å¹¶ç”Ÿæˆæœ€ç»ˆè¾“å‡ºã€‚",
    "task_description_quote": "...a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs... The model under evaluation is tasked with executing the instructions on the given input and producing the final output in the specified format.",
    "dimension": "æŒ‡ä»¤éµå¾ªçš„ç²¾ç¡®æ€§ã€ä»£ç å¹²è¿è¡Œï¼ˆæ¨¡æ‹Ÿæ‰§è¡Œï¼‰èƒ½åŠ›ã€å¤„ç†å¤šæ­¥ä»»åŠ¡çš„èƒ½åŠ›",
    "dimension_quote": "...rigorously assess sequential instruction-following and code dry-running capabilities in LLMs... Our results highlight the importance of code instruction following and dry-running ability as core competencies for LLM-based code assistants...",
    "evaluation_method": "åŸºäºè§„åˆ™çš„ç¡®å®šæ€§è¯„ä¼°ï¼Œé€šè¿‡å°†æ¨¡å‹è¾“å‡ºä¸å‚è€ƒä»£ç æ‰§è¡Œç”Ÿæˆçš„é¢„æœŸç»“æœè¿›è¡Œç®€å•æ¯”è¾ƒã€‚",
    "evaluation_method_quote": "PACIFIC benchmarks allow deterministic evaluation, relying on simple output comparison against expected results generated via reference code... To ensure transparency, reproducibility, and efficiency, PACIFIC employs a fully rule-based evaluation pipeline.",
    "context_dependency": "å¤šæ­¥é¡ºåºæŒ‡ä»¤ï¼ŒæŒ‡ä»¤å¯ä»¥ä¸²è”å½¢æˆå¤æ‚çš„å¤šæ­¥ä»»åŠ¡ã€‚",
    "context_dependency_quote": "These instructions can be concatenated to form complex multi-step tasks... Multiple instructions can then be composed into a pipeline, where each instruction receives as input the output produced by the preceding one.",
    "problem_domain": "è®¡ç®—æœºç§‘å­¦åŸºç¡€ã€ç®—æ³•ä¸å­—ç¬¦ä¸²æ“ä½œ",
    "problem_domain_quote": "We note that the instructions which are the frameworkâ€™s building blocks are made to be easily dry-run by a first year computer science major... (Example instructions: 'next_perfect_square', 'shift_back', prime number, day of week calculation)",
    "problem_difficulty": "å¯æ§åˆ¶éš¾åº¦ï¼ŒèŒƒå›´ä»æ˜“åˆ°æå…·æŒ‘æˆ˜æ€§ã€‚éš¾åº¦ç»´åº¦åŒ…æ‹¬æŒ‡ä»¤æ•°é‡å’Œé¢„æœŸè¾“å‡ºé•¿åº¦ã€‚",
    "problem_difficulty_quote": "...while allowing control over benchmark difficulty... We introduce an approach to control sample difficulty... The difficulty dimensions are: (1) LLM input: the number of instructions in each sample (prompt). (2) LLM expected output: the expected length of each sampleâ€™s output.",
    "language": "æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…æ‹¬Pythonã€Javaå’ŒC++ã€‚",
    "language_quote": "Currently, instructions are implemented in multiple programming languages, including Python, Java, and C++.",
    "data_size": "è§„æ¨¡å¯æ‰©å±•ï¼Œå¯æ ¹æ®å‚æ•°ï¼ˆå¦‚æ¯ä¸ªç¼–ç¨‹è¯­è¨€çš„æ ·æœ¬æ•°ï¼‰ç”Ÿæˆå¤§é‡å¤šæ ·åŒ–çš„åŸºå‡†æµ‹è¯•ã€‚",
    "data_size_quote": "To produce meaningful results, the framework must support large-scale and diverse benchmark generation... â€¢ Samples per Programming Language: The number of unique samples to generate for each supported language.",
    "source_type": "é€šè¿‡æ¡†æ¶è‡ªåŠ¨ç”Ÿæˆï¼ŒæŒ‡ä»¤æ± ä½œä¸ºæ„å»ºå—ã€‚",
    "source_type_quote": "PACIFIC, a framework designed to automatically generate benchmarks... Instructions serve as the fundamental building blocks of PACIFIC.",
    "last_updated": "2026",
    "last_updated_quote": "AI-SQE, April 14, 2026, Rio de Janeiro, Brazil",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±IBM Researchå›¢é˜Ÿæå‡ºï¼‰",
    "build_type_quote": "IBM Research Haifa, Israel {Itay.Dreyfuss,Antonio.Abu.Nassar,Samuel.Ackerman,Axel.Bendavid}@ibm.com",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ï¼Œæ¡†æ¶å¯è½»æ¾ç”Ÿæˆæ–°çš„åŸºå‡†æµ‹è¯•å˜ä½“ä»¥å‡è½»è®­ç»ƒæ•°æ®æ±¡æŸ“é£é™©ã€‚",
    "contamination_status_quote": "...our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations... Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆä¸æ¨ç†ï¼ˆæ ¹æ®æŒ‡ä»¤åºåˆ—ç”Ÿæˆæœ€ç»ˆè¾“å‡ºï¼‰",
    "task_granularity_quote": "The model under evaluation is tasked with executing the instructions on the given input and producing the final output in the specified format.",
    "evaluation_metrics": "Prompt-Level Accuracyï¼ˆæç¤ºçº§å‡†ç¡®ç‡ï¼‰",
    "evaluation_metrics_quote": "Inspired by the metric design proposed in [21], we compute two metrics: â€¢ Prompt-Level Acc",
    "input_modality": "è‡ªç„¶è¯­è¨€æŒ‡ä»¤æˆ–ä»£ç ç‰‡æ®µï¼Œä»¥åŠåˆå§‹è¾“å…¥ï¼ˆæ•°å­—æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚",
    "input_modality_quote": "â€¢ Instruction Type: Either Code (instructions as code snippets) or NL (instructions expressed in natural language). (1) An initial input â€” either a number or a string. (2) A sequence of instructions â€” operations to be applied to the input.",
    "output_modality": "ä»£ç æ‰§è¡Œåçš„æœ€ç»ˆè¾“å‡ºå€¼ï¼ˆæ•°å­—æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚",
    "output_modality_quote": "The model under evaluation is tasked with executing the instructions on the given input and producing the final output in the specified format.",
    "task_io_type": "æ–‡æœ¬ï¼ˆæŒ‡ä»¤ï¼‰åˆ°ä»£ç ï¼ˆæ¨ç†ç»“æœï¼‰ï¼Œæˆ–ä»£ç åˆ°ä»£ç ã€‚",
    "task_io_type_quote": "The model under evaluation is tasked with executing the instructions on the given input and producing the final output... (Instructions can be in Code or NL form)",
    "execution_environment": "å¹²è¿è¡Œï¼ˆæ¨¡æ‹Ÿæ‰§è¡Œï¼‰ï¼Œæ— éœ€å®é™…æ‰§è¡Œç¯å¢ƒæˆ–å¤–éƒ¨å·¥å…·ã€‚",
    "execution_environment_quote": "The framework emphasizes the ability of LLMs to simulate code execution (i.e. dry-running) without relying on external tools or agentic mechanisms.",
    "unique_features": "1. ä¸“æ³¨äºè¯„ä¼°ä»£ç å¹²è¿è¡Œï¼ˆæ¨¡æ‹Ÿæ‰§è¡Œï¼‰èƒ½åŠ›ï¼Œè€Œéå®é™…å·¥å…·ä½¿ç”¨ã€‚2. æ¡†æ¶å¯è‡ªåŠ¨ç”ŸæˆåŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒéš¾åº¦æ§åˆ¶å’ŒæŠ—æ±¡æŸ“è®¾è®¡ã€‚3. æä¾›ç¡®å®šæ€§çš„ã€åŸºäºè§„åˆ™è€ŒéLLMè¯„åˆ¤çš„è¯„ä¼°æ–¹æ³•ã€‚",
    "unique_features_quote": "In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLMâ€™s intrinsic ability to reason through code behavior step-by-step without execution (dry running)... PACIFIC benchmarks allow deterministic evaluation, relying on simple output comparison against expected results generated via reference code, without requiring tool use or LLM-as-a-judge paradigms... The framework must provide explicit mechanisms for controlling the difficulty of the generated benchmarks... To produce meaningful results, the framework must support large-scale and diverse benchmark generation. Furthermore, it should allow rapid creation of alternative benchmark versions to mitigate training data contamination risks.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python', 'Java', 'C++']",
    "dimension_normalized": "['æŒ‡ä»¤éµå¾ªçš„ç²¾ç¡®æ€§', 'ä»£ç å¹²è¿è¡Œï¼ˆæ¨¡æ‹Ÿæ‰§è¡Œï¼‰èƒ½åŠ›', 'å¤„ç†å¤šæ­¥ä»»åŠ¡çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['Prompt-Level Accuracyï¼ˆæç¤ºçº§å‡†ç¡®ç‡ï¼‰']",
    "problem_domain_normalized": "['è®¡ç®—æœºç§‘å­¦åŸºç¡€', 'ç®—æ³•ä¸å­—ç¬¦ä¸²æ“ä½œ']",
    "source_type_normalized": "['é€šè¿‡æ¡†æ¶è‡ªåŠ¨ç”Ÿæˆ', 'æŒ‡ä»¤æ± ä½œä¸ºæ„å»ºå—']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æ ‡å‡†åº“",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.10485_output/content.md",
    "benchmark_name": "VentiVul",
    "benchmark_name_quote": "We constructed a new, small-scale evaluation dataset, which we name VentiVul.",
    "dataset_url": "https://github.com/Chaomeng-Lu/A-Practical-Evaluation-of-Deep-Learning-Models-and-LLMs-for-Vulnerability-Detection.git",
    "dataset_url_quote": "All of our datasets, code, and results are available at the following link1. 1https://github.com/Chaomeng-Lu/A-Practical-Evaluation-of-Deep-Learning-Models-and-LLMs-for-Vulnerability-Detection.git",
    "task_description": "æ£€æµ‹è½¯ä»¶æºä»£ç ä¸­çš„å®‰å…¨æ¼æ´ã€‚",
    "task_description_quote": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets... Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code...",
    "dimension": "æ¼æ´æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ã€é²æ£’æ€§ã€åœ¨çœŸå®éƒ¨ç½²åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
    "dimension_quote": "Our primary motivation arises from a critical observation about existing research. DLâ€“based vulnerability detection models have seldom been rigorously evaluated in deployment-oriented, realistic settings involving novel or previously unseen vulnerabilities.",
    "evaluation_method": "åœ¨æ—¶é—´åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œä½¿ç”¨Whole-Fileå’ŒFunction-Pairä¸¤ç§æ–°é¢–çš„è¯„ä¼°æ¨¡å¼ã€‚",
    "evaluation_method_quote": "We manually construct a small but high-quality out-of-distribution vulnerability dataset and design a deployment-oriented evaluation framework introducing two novel modes, Whole-File and Function-Pair.",
    "context_dependency": "å‡½æ•°çº§å’Œæ–‡ä»¶çº§ã€‚",
    "context_dependency_quote": "For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits. These were carefully aligned to form function pairs... To provide additional context and challenge for model evaluation, we also included non-vulnerable functions from the same files...",
    "problem_domain": "è½¯ä»¶å®‰å…¨ã€æ¼æ´æ£€æµ‹ã€‚",
    "problem_domain_quote": "Software vulnerabilities remain a persistent and critical threat in modern computing environments... In response, the research community and industry have turned to automation, leading to the development of a variety of techniques for detecting vulnerabilities automatically...",
    "problem_difficulty": "çœŸå®ä¸–ç•Œã€æ–°è¿‘æŠ«éœ²çš„ã€å¤æ‚çš„æ¼æ´ã€‚",
    "problem_difficulty_quote": "Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably.",
    "language": "C/C++ã€‚",
    "language_quote": "Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database... These pairs span 21 distinct .c source files...",
    "data_size": "åŒ…å«æ¥è‡ª20ä¸ªLinux CVEçš„25ä¸ªå‡½æ•°å¯¹ï¼ˆæ¯ä¸ªåŒ…å«æ¼æ´ç‰ˆæœ¬å’Œä¿®å¤ç‰ˆæœ¬ï¼‰ï¼Œä»¥åŠæ¥è‡ªç›¸åŒæ–‡ä»¶çš„835ä¸ªæ— å…³å‡½æ•°ã€‚",
    "data_size_quote": "In total, we collected 25 function pairs (each consisting of a vulnerable and a patched version) from the 20 selected Linux CVEs. These pairs span 21 distinct .c source files and include 835 unrelated functions from the same files...",
    "source_type": "ä»2025å¹´5æœˆæŠ«éœ²çš„Linuxå†…æ ¸CVEæŠ¥å‘Šä¸­æ‰‹åŠ¨æ”¶é›†ï¼ŒåŸºäºæ˜ç¡®çš„ä¿®å¤æäº¤ã€‚",
    "source_type_quote": "Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database. Each report was then manually examined to identify those containing explicit fix commits with detailed code changes.",
    "last_updated": "2025å¹´5æœˆ",
    "last_updated_quote": "Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database.",
    "build_type": "ä½œè€…æ‰‹åŠ¨æ„å»ºã€‚",
    "build_type_quote": "We manually construct a small but high-quality out-of-distribution vulnerability dataset...",
    "contamination_status": "è®¾è®¡ä¸ºæ—¶é—´åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ•°æ®é›†ï¼Œæ—¨åœ¨æœ€å°åŒ–è®­ç»ƒæ•°æ®æ±¡æŸ“ã€‚",
    "contamination_status_quote": "When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset...",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ¼æ´æ£€æµ‹ï¼ˆäºŒåˆ†ç±»ï¼šæ¼æ´/éæ¼æ´ï¼‰ã€‚",
    "task_granularity_quote": "For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits. These were carefully aligned to form function pairs, labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "æºä»£ç ã€‚",
    "input_modality_quote": "For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits.",
    "output_modality": "äºŒåˆ†ç±»æ ‡ç­¾ï¼ˆæ¼æ´/éæ¼æ´ï¼‰ã€‚",
    "output_modality_quote": "...labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",
    "task_io_type": "ä»£ç åˆ°æ ‡ç­¾ã€‚",
    "task_io_type_quote": "For each CVE, we extracted the vulnerable and patched versions of affected functions... labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°æ¨¡å‹åœ¨çœŸå®ã€éƒ¨ç½²å¯¼å‘åœºæ™¯ä¸‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹æ—¶é—´åˆ†å¸ƒå¤–ï¼ˆOODï¼‰å’Œæ–°è¿‘æ¼æ´çš„æ£€æµ‹ã€‚æ•°æ®é›†åŸºäº2025å¹´5æœˆçœŸå®Linuxå†…æ ¸CVEæ‰‹åŠ¨æ„å»ºï¼ŒåŒ…å«Whole-Fileå’ŒFunction-Pairä¸¤ç§è¯„ä¼°æ¨¡å¼ã€‚",
    "unique_features_quote": "The novelty of this work lies in its comprehensive and deployment-oriented perspective... We manually construct a small but high-quality out-of-distribution vulnerability dataset and design a deployment-oriented evaluation framework introducing two novel modes, Whole-File and Function-Pair.",
    "data_size_quantity": 25,
    "data_size_unit": "ä¸ªå‡½æ•°å¯¹",
    "last_updated_year": 2025,
    "last_updated_month": 5,
    "last_updated_day": null,
    "language_normalized": "['C/C++']",
    "dimension_normalized": "['æ¼æ´æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›', 'é²æ£’æ€§', 'åœ¨çœŸå®éƒ¨ç½²åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'æ¼æ´æ£€æµ‹']",
    "source_type_normalized": "['ä»2025å¹´5æœˆæŠ«éœ²çš„Linuxå†…æ ¸CVEæŠ¥å‘Šä¸­æ‰‹åŠ¨æ”¶é›†', 'åŸºäºæ˜ç¡®çš„ä¿®å¤æäº¤']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.15699_output/content.md",
    "benchmark_name": "FrontierCS",
    "benchmark_name_quote": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science...",
    "dataset_url": "Frontier-CS GitHub",
    "dataset_url_quote": "Code and Data: Frontier-CS GitHub",
    "task_description": "è¯„æµ‹LLMè§£å†³å¼€æ”¾å¼è®¡ç®—æœºç§‘å­¦é—®é¢˜çš„èƒ½åŠ›ï¼Œè¿™äº›é—®é¢˜æ²¡æœ‰å·²çŸ¥çš„å°é—­å½¢å¼æˆ–ç¡®å®šæ€§æœ€ä¼˜è§£ã€‚æ¨¡å‹éœ€è¦å®ç°å¯æ‰§è¡Œç¨‹åºæ¥è§£å†³é—®é¢˜ï¼Œè€Œéç›´æ¥è¾“å‡ºç­”æ¡ˆã€‚",
    "task_description_quote": "FrontierCS, a coding benchmark that evaluates LLMs on solving open-ended computer science problems, where no known closed-form or deterministic optimal solution exists in practice. Unlike math or reasoning benchmarks that require a direct answer (e.g., AIME), FrontierCS requires models to implement executable programs to solve the problem (e.g., LiveCodeBench).",
    "dimension": "å¼€æ”¾å¼æ¨ç†ã€ç®—æ³•å®ç°ã€è§£å†³æ–¹æ¡ˆè´¨é‡ã€è·¨é¢†åŸŸé—®é¢˜è§£å†³èƒ½åŠ›",
    "dimension_quote": "FrontierCS provides a benchmark at the frontier of computer-science difficulty. ... stress-testing a modelâ€™s ability to perform deep open-ended reasoning and discover nontrivial optimization strategies.",
    "evaluation_method": "é€šè¿‡è‡ªåŠ¨è¯„ä¼°å™¨è¿è¡Œç”Ÿæˆçš„ç¨‹åºï¼Œæ ¹æ®ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡ï¼ˆå¦‚æ‰“åŒ…å¯†åº¦ã€è¿è¡Œæ—¶é—´ã€å†…å­˜ä½¿ç”¨ï¼‰åœ¨èµ„æºé™åˆ¶ä¸‹å¯¹è¾“å‡ºè¿›è¡Œç¡®å®šæ€§éªŒè¯å’Œå®šé‡è¯„åˆ†ï¼Œè€Œéç®€å•çš„é€šè¿‡/å¤±è´¥ã€‚",
    "evaluation_method_quote": "Each FrontierCS task can be solved by submitting executable code: the evaluator runs the program on generated instances and scores its outputs by task-specific metrics under resource limits (e.g., time and memory usage). ... Solutions with runtime limit can be automatically checked for validity and assigned a numeric score that reflects the quality of the solution, rather than a simple pass-or-fail.",
    "context_dependency": "å•ä»»åŠ¡é—®é¢˜ï¼Œæ¨¡å‹æ ¹æ®é—®é¢˜è§„èŒƒï¼ˆåŠæ‰€éœ€çš„I/Oæˆ–APIå­˜æ ¹ï¼‰ç”Ÿæˆç‹¬ç«‹çš„æ±‚è§£ç¨‹åºã€‚",
    "context_dependency_quote": "The model is prompted with the problem specification (and any required I/O or API stubs) and must produce a self-contained solver program.",
    "problem_domain": "è®¡ç®—æœºç§‘å­¦ï¼ŒåŒ…å«ç®—æ³•ä¼˜åŒ–ï¼ˆå¦‚NP-hardå˜ä½“ï¼‰å’Œè·¨é¢†åŸŸç ”ç©¶é—®é¢˜ï¼ˆæ“ä½œç³»ç»Ÿã€é«˜æ€§èƒ½è®¡ç®—ã€äººå·¥æ™ºèƒ½ã€æ•°æ®åº“ã€ç¼–ç¨‹è¯­è¨€ã€å®‰å…¨ï¼‰ã€‚",
    "problem_domain_quote": "FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. ... Research Problems, spanning six major CS domains: OS (Operating Systems), HPC (High-Performance Computing), AI (Artificial Intelligence research tasks), DB (Databases), PL (Programming Languages), and Security (cybersecurity and vulnerability analysis).",
    "problem_difficulty": "å‰æ²¿çº§ï¼Œé—®é¢˜æœ€ä¼˜è§£æœªçŸ¥æˆ–è®¡ç®—ä¸Šä¸å¯è¡Œï¼Œæ—¨åœ¨æŒ‘æˆ˜è®¡ç®—æœºç§‘å­¦å‰æ²¿éš¾åº¦ã€‚",
    "problem_difficulty_quote": "FrontierCS provides a benchmark at the frontier of computer-science difficulty. ... The global optimum is unknown to compute over all problem instances...",
    "language": "æœªæ˜ç¡®æŒ‡å®šï¼Œä½†ä»»åŠ¡è¦æ±‚æäº¤å¯æ‰§è¡Œä»£ç ï¼Œæ¨æ–­ä¸»è¦æ¶‰åŠé€šç”¨ç¼–ç¨‹è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": "åŒ…å«156ä¸ªå¼€æ”¾å¼é—®é¢˜ï¼Œå…¶ä¸­ç®—æ³•é—®é¢˜107ä¸ªï¼Œç ”ç©¶é—®é¢˜49ä¸ªã€‚",
    "data_size_quote": "FrontierCS consists 156 problems across two tracks: Algorithmic Problems and Research Problems. The Algorithmic Problems track contains 107 problems... The Research Problems track contains 49 problems...",
    "source_type": "ç”±ä¸“å®¶ï¼ˆåŒ…æ‹¬CSåšå£«å’Œé¡¶çº§ç«èµ›ç¼–ç¨‹å‚ä¸è€…åŠå‡ºé¢˜è€…ï¼‰è®¾è®¡å’Œè¯„å®¡ã€‚ç®—æ³•é—®é¢˜æ”¹ç¼–è‡ªç¼–ç¨‹ç«èµ›ï¼Œç ”ç©¶é—®é¢˜æºè‡ªçœŸå®ä¸–ç•Œè®¡ç®—æœºç§‘å­¦ç ”ç©¶é—®é¢˜ã€‚",
    "source_type_quote": "designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. ... The Algorithmic Problems track contains 107 problems adapted from programming contests... The Research Problems track contains 49 problems sourced from real-world computer science research questions...",
    "last_updated": "2025-12-17 (arXivç‰ˆæœ¬)",
    "last_updated_quote": "arXiv:2512.15699v1  [cs.LG]  17 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±å¤šæ‰€å¤§å­¦çš„ç ”ç©¶å›¢é˜Ÿå’Œä¸“å®¶é¡¾é—®å…±åŒæ„å»ºã€‚",
    "build_type_quote": "Contributors... Advisors... Affiliations... (åˆ—å‡ºäº†UC Berkeley, Princeton Universityç­‰å¤šæ‰€æœºæ„çš„ç ”ç©¶äººå‘˜)",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ï¼Œé€šè¿‡å‚æ•°åŒ–é—®é¢˜ç”Ÿæˆå™¨äº§ç”Ÿå¤§é‡å¯å˜éš¾åº¦çš„å®ä¾‹ï¼Œä½¿ç”¨æ–°é²œçš„ã€æœªè§è¿‡çš„æµ‹è¯•ç”¨ä¾‹æ¥é˜²æ­¢æ³„éœ²å’Œè¿‡æ‹Ÿåˆã€‚",
    "contamination_status_quote": "Parametric Problem Generator: The task specification induces a large, variable-difficulty space of instances, enabling fresh, unseen test cases to prevent leak and overfitting.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼Œç”Ÿæˆå®Œæ•´çš„æ±‚è§£ç¨‹åºã€‚",
    "task_granularity_quote": "Models solve these tasks by implementing executable programs rather than outputting a direct answer.",
    "evaluation_metrics": "ä»»åŠ¡ç‰¹å®šçš„å®šé‡è¯„åˆ†æŒ‡æ ‡ï¼ˆå¦‚æ‰“åŒ…å¯†åº¦ï¼‰ï¼Œè€Œéå•ä¸€æŒ‡æ ‡ã€‚",
    "evaluation_metrics_quote": "scored its outputs by task-specific metrics... assigned a numeric score that reflects the quality of the solution",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆé—®é¢˜è§„èŒƒï¼‰åŠå¯èƒ½çš„ä»£ç å­˜æ ¹ã€‚",
    "input_modality_quote": "The model is prompted with the problem specification (and any required I/O or API stubs)...",
    "output_modality": "ä»£ç ï¼ˆè‡ªåŒ…å«çš„æ±‚è§£ç¨‹åºï¼‰ã€‚",
    "output_modality_quote": "...must produce a self-contained solver program.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "The model is prompted with the problem specification... and must produce a self-contained solver program.",
    "execution_environment": "åœ¨èµ„æºé™åˆ¶ï¼ˆå¦‚æ—¶é—´å’Œå†…å­˜ä½¿ç”¨ï¼‰ä¸‹è¿è¡Œï¼Œå…·ä½“ç¯å¢ƒæœªæ˜ç¡®è¯´æ˜ã€‚",
    "execution_environment_quote": "the evaluator runs the program on generated instances and scores its outputs by task-specific metrics under resource limits (e.g., time and memory usage).",
    "unique_features": "ä¸“æ³¨äºå¼€æ”¾å¼ã€æœªè§£å†³ã€å¯éªŒè¯ä¸”å¤šæ ·åŒ–çš„é—®é¢˜ã€‚é—®é¢˜çš„æœ€ä¼˜è§£æœªçŸ¥ï¼Œä½†è§£å†³æ–¹æ¡ˆçš„è´¨é‡å¯ä»¥å®¢è§‚è¯„ä¼°ã€‚åŒ…å«ç®—æ³•å’Œç ”ç©¶ä¸¤æ¡è½¨é“ï¼Œé¼“åŠ±åœ¨å¼€æ”¾å¼ç¯å¢ƒä¸­è¿›è¡Œè¿­ä»£æ”¹è¿›è€Œéè¿½æ±‚ç¡®å®šæ€§æœ€ä¼˜è§£ã€‚",
    "unique_features_quote": "FrontierCS, an unsolved, open-ended, verifiable, and diverse benchmark for computer science tasks. ... Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. ... The design of FrontierCS encourages iterative improvement in an open-ended landscape rather than aiming for a deterministic optimal solution, since none of its problems have known practical optima.",
    "data_size_quantity": 156,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 17,
    "language_normalized": "['é€šç”¨ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['å¼€æ”¾å¼æ¨ç†', 'ç®—æ³•å®ç°', 'è§£å†³æ–¹æ¡ˆè´¨é‡', 'è·¨é¢†åŸŸé—®é¢˜è§£å†³èƒ½åŠ›']",
    "evaluation_method_normalized": "['ä»»åŠ¡ç‰¹å®šçš„å®šé‡è¯„åˆ†æŒ‡æ ‡']",
    "problem_domain_normalized": "['è®¡ç®—æœºç§‘å­¦', 'ç®—æ³•ä¼˜åŒ–', 'è·¨é¢†åŸŸç ”ç©¶é—®é¢˜', 'æ“ä½œç³»ç»Ÿ', 'é«˜æ€§èƒ½è®¡ç®—', 'äººå·¥æ™ºèƒ½', 'æ•°æ®åº“', 'ç¼–ç¨‹è¯­è¨€', 'å®‰å…¨']",
    "source_type_normalized": "['ä¸“å®¶è®¾è®¡å’Œè¯„å®¡', 'ç¼–ç¨‹ç«èµ›æ”¹ç¼–', 'çœŸå®ä¸–ç•Œè®¡ç®—æœºç§‘å­¦ç ”ç©¶é—®é¢˜']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.20482_output/content.md",
    "benchmark_name": "SWELOCMULTI",
    "benchmark_name_quote": "We create SWELOCMULTI, a large-scale multilingual dataset curated specifically for issue localization.",
    "dataset_url": "https://github.com/SalesforceAIResearch/SweRank",
    "dataset_url_quote": "1Code and models will be released here: https://github.com/com/SalesforceAIResearch/SweRank",
    "task_description": "è½¯ä»¶é—®é¢˜å®šä½ï¼Œå³æ ¹æ®è‡ªç„¶è¯­è¨€é”™è¯¯æè¿°æˆ–åŠŸèƒ½è¯·æ±‚ï¼Œåœ¨ä»£ç åº“ä¸­å®šä½éœ€è¦ä¿®æ”¹çš„ç›¸å…³å‡½æ•°ã€‚",
    "task_description_quote": "Maintaining large-scale, multilingual code-bases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified.",
    "dimension": "å¤šè¯­è¨€ä»£ç æ’åèƒ½åŠ›ã€å¤šè½®è¿­ä»£æ¨ç†èƒ½åŠ›ã€é—®é¢˜å®šä½å‡†ç¡®æ€§",
    "dimension_quote": "SWERANK+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization",
    "evaluation_method": "åœ¨é—®é¢˜å®šä½åŸºå‡†ä¸Šè¿›è¡Œå®éªŒï¼Œè¯„ä¼°å®šä½å‡†ç¡®æ€§ï¼ˆå¦‚Accuracy@10ï¼‰ï¼Œå¹¶ä¸åŸºçº¿æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚",
    "evaluation_method_quote": "Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SWERANK-MULTI, while SWERANKAGENT further improves localization over single-pass ranking.",
    "context_dependency": "å¤šæ–‡ä»¶ã€å¤šè¯­è¨€ä»£ç åº“ã€‚",
    "context_dependency_quote": "modern code repositories grow in size and complexity to encompass thousands of files across multiple programming languages",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€è½¯ä»¶ç»´æŠ¤ã€é—®é¢˜å®šä½",
    "problem_domain_quote": "The maintenance of large-scale software systems constitutes a significant and ever-growing portion of the software development lifecycle. A persistent bottleneck in this process is software issue localization",
    "problem_difficulty": "ç°å®ä¸–ç•Œä¼ä¸šçº§è½¯ä»¶ç³»ç»Ÿçš„å¤æ‚é—®é¢˜å®šä½",
    "problem_difficulty_quote": "real-world enterprise systems are inherently multilingual, comprising interconnected components written in diverse programming languages... complex issues that demand iterative reasoning or involve changes dispersed across multiple, loosely coupled functions.",
    "language": "JavaScript, Java, TypeScript, Ruby, Rust, Go, PHP, C, C++, Python",
    "language_quote": "SWELOCMULTI extends the data collection and filtering pipeline of SWELOC to encompass JavaScript, Java, TypeScript, Ruby, Rust, Go, PHP, C, and C++.",
    "data_size": "å¤§è§„æ¨¡æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª4060ä¸ªä»“åº“ã€56279ä¸ªæ‹‰å–è¯·æ±‚çš„155663ä¸ªè®­ç»ƒå®ä¾‹ã€‚",
    "data_size_quote": "Total 4060 56279 155663",
    "source_type": "ä»GitHubä¸Šæµè¡Œçš„å¼€æºä»“åº“ä¸­æå–ï¼Œç­›é€‰æ¡ä»¶åŒ…æ‹¬ï¼šç›®æ ‡è¯­è¨€ä»£ç å æ¯”è‡³å°‘40%ã€è¶…è¿‡1000æ˜Ÿã€è¿‡å»å…­ä¸ªæœˆå†…æœ‰æäº¤ã€‚æ•°æ®æ¥è‡ªä¸GitHubé—®é¢˜æ˜ç¡®å…³è”å¹¶åŒ…å«æµ‹è¯•æ–‡ä»¶ä¿®æ”¹çš„æ‹‰å–è¯·æ±‚ã€‚",
    "source_type_quote": "Following SWERANK's methodology, we identify popular open-source repositories on GitHub for each language, filtering for repositories with at least 40% code in the target language, over 1,000 stars, and at least one commit in the preceding six months. From this curated set, we extract pull requests (PRs) explicitly linked to GitHub issues that include test file modifications.",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.20482v1 [cs.SE] 23 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We create SWELOCMULTI, a large-scale multilingual dataset curated specifically for issue localization.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç æ’å/æ£€ç´¢",
    "task_granularity_quote": "reformulates issue localization as a retrieve-and-rerank problem",
    "evaluation_metrics": "Accuracy@10",
    "evaluation_metrics_quote": "Figure 1: Comparison of function localization accuracy@10 against the SWERANK baseline.",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "mapping natural-language error descriptions to the relevant functions",
    "output_modality": "ä»£ç ï¼ˆå‡½æ•°ï¼‰æ’ååˆ—è¡¨",
    "output_modality_quote": "identifying where in a codebase a fix should be applied",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç æ’å",
    "task_io_type_quote": "mapping natural language descriptions... to specific code elements including files, modules, or functions.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“æ³¨äºå¤šè¯­è¨€è½¯ä»¶é—®é¢˜å®šä½çš„åŸºå‡†æ•°æ®é›†ï¼Œè¦†ç›–10ç§æµè¡Œç¼–ç¨‹è¯­è¨€ï¼Œå¹¶æ”¯æŒå¤šè½®ä»£ç†å¼æœç´¢è¿›è¡Œè¿­ä»£æ¨ç†ã€‚",
    "unique_features_quote": "We present the first framework to address issue localization in a multilingual setting. ... We propose SWERANKAGENT, an iterative, multi-turn localization framework that further improves over single-pass ranking.",
    "data_size_quantity": 155663,
    "data_size_unit": "ä¸ªè®­ç»ƒå®ä¾‹",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['JavaScript', 'Java', 'TypeScript', 'Ruby', 'Rust', 'Go', 'PHP', 'C', 'C++', 'Python']",
    "dimension_normalized": "['å¤šè¯­è¨€ä»£ç æ’åèƒ½åŠ›', 'å¤šè½®è¿­ä»£æ¨ç†èƒ½åŠ›', 'é—®é¢˜å®šä½å‡†ç¡®æ€§']",
    "evaluation_method_normalized": "['Accuracy@10']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'è½¯ä»¶ç»´æŠ¤', 'é—®é¢˜å®šä½']",
    "source_type_normalized": "['ä»GitHubä¸Šæµè¡Œçš„å¼€æºä»“åº“ä¸­æå–ï¼Œç­›é€‰æ¡ä»¶åŒ…æ‹¬ï¼šç›®æ ‡è¯­è¨€ä»£ç å æ¯”è‡³å°‘40%ã€è¶…è¿‡1000æ˜Ÿã€è¿‡å»å…­ä¸ªæœˆå†…æœ‰æäº¤ã€‚æ•°æ®æ¥è‡ªä¸GitHubé—®é¢˜æ˜ç¡®å…³è”å¹¶åŒ…å«æµ‹è¯•æ–‡ä»¶ä¿®æ”¹çš„æ‹‰å–è¯·æ±‚ã€‚']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.21238_output/content.md",
    "benchmark_name": "Basket",
    "benchmark_name_quote": "â€¢ Basket, a Bloomâ€™s taxonomy-guided frAmework for Software security Knowledge EvaluaTion. This framework allows the systematic assessment of LLMsâ€™ capabilities with respect to software security knowledge (Section 3).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è½¯ä»¶å®‰å…¨çŸ¥è¯†çš„ç†è§£ã€åº”ç”¨å’Œæ‰©å±•èƒ½åŠ›ï¼Œæ¶µç›–ä»äº‹å®å›å¿†åˆ°é«˜é˜¶æ¨ç†å’Œåˆ›é€ æ€§è®¾è®¡çš„å…­ä¸ªè®¤çŸ¥å±‚æ¬¡ã€‚",
    "task_description_quote": "To answer this question, we developed Basket, a framework that enables the systematic assessment of LLMs guided by Bloomâ€™s Taxonomy [22], a widely adopted framework in education research, which categorizes learning objectives across six cognitive levels: remembering, understanding, applying, analyzing, evaluating, and creating. ... to comprehensively assess how well LLMs comprehend, apply, and extend the software security knowledge boundary.",
    "dimension": "è½¯ä»¶å®‰å…¨çŸ¥è¯†ç†è§£ï¼Œæ¶µç›–å…­ä¸ªè®¤çŸ¥ç»´åº¦ï¼šè®°å¿†ã€ç†è§£ã€åº”ç”¨ã€åˆ†æã€è¯„ä¼°å’Œåˆ›é€ ã€‚",
    "dimension_quote": "We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating.",
    "evaluation_method": "ä½¿ç”¨å¤šç§æ•°æ®é›†è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬ç²¾å¿ƒè®¾è®¡çš„å¤šé¡¹é€‰æ‹©é¢˜ã€æ˜“å—æ”»å‡»çš„ä»£ç ç‰‡æ®µã€è¯¾ç¨‹è¯„ä¼°ã€çœŸå®ä¸–ç•Œæ¡ˆä¾‹ç ”ç©¶å’ŒåŸºäºé¡¹ç›®çš„å¼€æ”¾å¼åˆ›å»ºä»»åŠ¡ã€‚",
    "evaluation_method_quote": "Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶å®‰å…¨",
    "problem_domain_quote": "Assessing the Software Security Comprehension of Large Language Models",
    "problem_difficulty": "æ¶µç›–ä»ä½çº§è®¤çŸ¥ä»»åŠ¡ï¼ˆå¦‚å›å¿†äº‹å®ï¼‰åˆ°é«˜çº§è®¤çŸ¥ä»»åŠ¡ï¼ˆå¦‚æ¨ç†ã€æ¶æ„è¯„ä¼°å’Œå®‰å…¨ç³»ç»Ÿåˆ›å»ºï¼‰çš„å¤šä¸ªéš¾åº¦å±‚æ¬¡ã€‚",
    "problem_difficulty_quote": "Results show that while LLMs perform well on lower-level cognitive tasks, such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "å¤šç§æ¥æºï¼šåŒ…æ‹¬ç²¾å¿ƒè®¾è®¡çš„å¤šé¡¹é€‰æ‹©é¢˜ã€æ˜“å—æ”»å‡»çš„ä»£ç ç‰‡æ®µï¼ˆSALLMï¼‰ã€è¯¾ç¨‹è¯„ä¼°ã€çœŸå®ä¸–ç•Œæ¡ˆä¾‹ç ”ç©¶ï¼ˆXBOWï¼‰å’ŒåŸºäºé¡¹ç›®çš„åˆ›å»ºä»»åŠ¡ã€‚",
    "source_type_quote": "Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "To answer this question, we developed Basket, a framework that enables the systematic assessment of LLMs guided by Bloomâ€™s Taxonomy [22]...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "çŸ¥è¯†è¯„ä¼°ä¸ç†è§£ï¼Œæ¶µç›–ä»äº‹å®å›å¿†åˆ°åˆ›é€ æ€§è®¾è®¡çš„å¤šç§ä»»åŠ¡ç²’åº¦ã€‚",
    "task_granularity_quote": "Rather than introducing yet another benchmark for a single security task, we used a combination of curated multiple-choice questions, vulnerable code snippets, course assessments, real-world case studies, and open-ended project tasks...",
    "evaluation_metrics": "åŸºäºè®¤çŸ¥æ°´å¹³çš„å¯é æ€§èƒ½ï¼ˆçŸ¥è¯†è¾¹ç•Œï¼‰å’Œå‡†ç¡®æ€§ã€‚",
    "evaluation_metrics_quote": "Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance.",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜ã€ä»£ç ç‰‡æ®µã€æ¡ˆä¾‹ç ”ç©¶æè¿°ç­‰ã€‚",
    "input_modality_quote": "Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",
    "output_modality": "è‡ªç„¶è¯­è¨€ç­”æ¡ˆã€ä»£ç ã€è®¾è®¡ç­‰ã€‚",
    "output_modality_quote": "This way, such a framework allows us not only to measure an LLMâ€™s factual recall abilities but also to probe deeper into its reasoning, diagnostic ability, and creativity in secure software development tasks.",
    "task_io_type": "æ–‡æœ¬åˆ°æ–‡æœ¬ã€ä»£ç åˆ°æ–‡æœ¬ã€æ–‡æœ¬åˆ°ä»£ç ç­‰ï¼Œå–å†³äºå…·ä½“ä»»åŠ¡ã€‚",
    "task_io_type_quote": "Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "åŸºäºå¸ƒé²å§†åˆ†ç±»æ³•çš„è®¤çŸ¥å±‚æ¬¡è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°LLMçš„è½¯ä»¶å®‰å…¨çŸ¥è¯†è¾¹ç•Œå’Œè¯†åˆ«å…¶ç³»ç»Ÿæ€§è¯¯è§£æ¨¡å¼ï¼Œè€Œéä»…é’ˆå¯¹å•ä¸€å®‰å…¨ä»»åŠ¡ã€‚",
    "unique_features_quote": "Rather than introducing yet another benchmark for a single security task, we used a combination of curated multiple-choice questions, vulnerable code snippets, course assessments, real-world case studies, and open-ended project tasks to comprehensively assess how well LLMs comprehend, apply, and extend the software security knowledge boundary. ... we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identified 51 recurring misconception patterns made by LLMs across Bloomâ€™s levels.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['è½¯ä»¶å®‰å…¨çŸ¥è¯†ç†è§£', 'è®°å¿†', 'ç†è§£', 'åº”ç”¨', 'åˆ†æ', 'è¯„ä¼°', 'åˆ›é€ ']",
    "evaluation_method_normalized": "['åŸºäºè®¤çŸ¥æ°´å¹³çš„å¯é æ€§èƒ½', 'çŸ¥è¯†è¾¹ç•Œ', 'å‡†ç¡®æ€§']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨']",
    "source_type_normalized": "['å¤šé¡¹é€‰æ‹©é¢˜', 'æ˜“å—æ”»å‡»çš„ä»£ç ç‰‡æ®µ', 'SALLM', 'è¯¾ç¨‹è¯„ä¼°', 'çœŸå®ä¸–ç•Œæ¡ˆä¾‹ç ”ç©¶', 'XBOW', 'åŸºäºé¡¹ç›®çš„åˆ›å»ºä»»åŠ¡']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.21236_output/content.md",
    "benchmark_name": "SPELL",
    "benchmark_name_quote": "To address this gap, we propose SPELL, a comprehensive testing framework for LLM developers and the Secure Team, specifically designed to evaluate the weakness of security alignment in malicious code generation.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¶æ„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„å®‰å…¨å¯¹é½å¼±ç‚¹ã€‚",
    "task_description_quote": "To address this gap, we propose SPELL, a comprehensive testing framework for LLM developers and the Secure Team, specifically designed to evaluate the weakness of security alignment in malicious code generation.",
    "dimension": "å®‰å…¨å¯¹é½ã€æ¶æ„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€è¶Šç‹±æ”»å‡»æˆåŠŸç‡ã€‚",
    "dimension_quote": "specifically designed to evaluate the weakness of security alignment in malicious code generation.",
    "evaluation_method": "é€šè¿‡ç³»ç»Ÿæ„å»ºè¶Šç‹±æç¤ºè¯ï¼Œè¯„ä¼°æ¨¡å‹ç”Ÿæˆæ¶æ„ä»£ç çš„æ”»å‡»æˆåŠŸç‡ï¼Œå¹¶ä½¿ç”¨æœ€å…ˆè¿›çš„æ£€æµ‹ç³»ç»Ÿç¡®è®¤ç”Ÿæˆçš„ä»£ç æ˜¯å¦ä¸ºæ¶æ„ã€‚",
    "evaluation_method_quote": "Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts... Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELLâ€™s effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%...",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç½‘ç»œå®‰å…¨ã€æ¶æ„è½¯ä»¶ç”Ÿæˆã€‚",
    "problem_domain_quote": "malicious code generation... including malware, ransomware, and other security threats.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "ä»å…ˆéªŒçŸ¥è¯†æ•°æ®é›†ä¸­æå–å¥å­æ™ºèƒ½ç»„åˆã€‚",
    "source_type_quote": "Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset...",
    "last_updated": "2025-12-24",
    "last_updated_quote": "arXiv:2512.21236v1  [cs.CR]  24 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we propose SPELL, a comprehensive testing framework...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "malicious code generation",
    "evaluation_metrics": "æ”»å‡»æˆåŠŸç‡",
    "evaluation_metrics_quote": "achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆè¶Šç‹±æç¤ºè¯ï¼‰",
    "input_modality_quote": "systematically constructs jailbreaking prompts",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generate malicious code",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "generate malicious code",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªåŠ¨æ€å‘ç°å’Œç»„åˆæç¤ºè¯ç»„ä»¶çš„è‡ªåŠ¨åŒ–æ¡†æ¶ï¼Œç”¨äºæ¶æ„ä»£ç ç”Ÿæˆè¯„ä¼°ï¼›é‡‡ç”¨åˆ†æ—¶é€‰æ‹©ç­–ç•¥ï¼Œå¹³è¡¡å¯¹æ–°æ”»å‡»æ¨¡å¼çš„æ¢ç´¢å’Œå¯¹æˆåŠŸæŠ€æœ¯çš„åˆ©ç”¨ï¼›è¯„ä¼°äº†å…«ç±»æ¶æ„ä»£ç ï¼›åœ¨çœŸå®IDEç¯å¢ƒä¸­éªŒè¯ã€‚",
    "unique_features_quote": "We propose SPELL, the first automated framework that dynamically discovers and combines prompt components for malicious code generation... Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques... across eight malicious code categories... successful real-world deployment in production IDE environments.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 24,
    "language_normalized": "[]",
    "dimension_normalized": "['å®‰å…¨å¯¹é½', 'æ¶æ„ä»£ç ç”Ÿæˆèƒ½åŠ›', 'è¶Šç‹±æ”»å‡»æˆåŠŸç‡']",
    "evaluation_method_normalized": "['æ”»å‡»æˆåŠŸç‡']",
    "problem_domain_normalized": "['ç½‘ç»œå®‰å…¨', 'æ¶æ„è½¯ä»¶ç”Ÿæˆ']",
    "source_type_normalized": "['ä»å…ˆéªŒçŸ¥è¯†æ•°æ®é›†ä¸­æå–å¥å­æ™ºèƒ½ç»„åˆ']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.04540_output/content.md",
    "benchmark_name": "AdaptEval",
    "benchmark_name_quote": "To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation.",
    "dataset_url": "https://github.com/ztwater/AdaptEval",
    "dataset_url_quote": "We propose AdaptEval1, the first benchmark for code snippet adaptation... 1https://github.com/ztwater/AdaptEval.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç‰‡æ®µé€‚åº”ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä»£ç ç‰‡æ®µé€‚åº”æ˜¯ä»£ç é‡ç”¨è¿‡ç¨‹ä¸­çš„å…³é”®æ­¥éª¤ï¼Œæ¶‰åŠå°†ä»£ç ç‰‡æ®µä»ä¸€ä¸ªç‰¹å®šç¯å¢ƒä¿®æ”¹ä»¥é€‚åº”å¦ä¸€ä¸ªç¯å¢ƒã€‚",
    "task_description_quote": "AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation. ... A critical step in this process is adaptation, where developers modify code snippets to fit their specific context.",
    "dimension": "ä»£ç ç‰‡æ®µé€‚åº”èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£ã€å¯¹ä»»åŠ¡è¦æ±‚çš„éµå¾ªã€ä»¥åŠæ‰§è¡Œå…·ä½“ä»£ç ä¿®æ”¹çš„èƒ½åŠ›ã€‚",
    "dimension_quote": "AdaptEval enables the assessment of LLMsâ€™ adaptation capabilities from various perspectives. ... evaluating LLMsâ€™ performance across various individual adaptations.",
    "evaluation_method": "é‡‡ç”¨åŒå±‚æµ‹è¯•æ¡†æ¶ï¼Œç»“åˆé€‚åº”çº§åˆ«å’Œå‡½æ•°çº§åˆ«çš„æµ‹è¯•ï¼Œè¯„ä¼°LLMç”Ÿæˆä»£ç çš„æ­£ç¡®æ€§ã€‚",
    "evaluation_method_quote": "AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMsâ€™ performance across various individual adaptations.",
    "context_dependency": "ä»»åŠ¡æ¥æºäºå¼€å‘è€…çš„å®é™…å®è·µï¼Œä¿ç•™äº†æ¥è‡ªStack Overflowå’ŒGitHubç¤¾åŒºçš„ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚",
    "context_dependency_quote": "Tasks in AdaptEval are derived from developersâ€™ practices, preserving rich contextual information from Stack Overflow and GitHub communities.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œå…·ä½“æ˜¯ä»£ç é‡ç”¨å’Œä»£ç ç‰‡æ®µé€‚åº”ã€‚",
    "problem_domain_quote": "Code Snippet Adaptation, Large Language Models, Benchmark, Code Reuse",
    "problem_difficulty": "åŸºäºçœŸå®å¼€å‘è€…å®è·µçš„å·¥ç¨‹çº§ä»»åŠ¡ã€‚",
    "problem_difficulty_quote": "Tasks in AdaptEval are derived from developersâ€™ practices... comprising 164 tasks with 523 adaptations in Python language.",
    "language": "Python",
    "language_quote": "comprising 164 tasks with 523 adaptations in Python language.",
    "data_size": "åŒ…å«164ä¸ªä»»åŠ¡ï¼Œå…±523ä¸ªé€‚åº”æ“ä½œã€‚",
    "data_size_quote": "comprising 164 tasks with 523 adaptations in Python language.",
    "source_type": "æ•°æ®æ¥æºäºå¼€å‘è€…çš„å®é™…é€‚åº”å®è·µï¼Œä»Stack Overflowå¸–å­å’Œå…³è”çš„GitHubä»“åº“ä¸­æ”¶é›†ã€‚",
    "source_type_quote": "each task in AdaptEval is collected from the actual adaptation practice of developers. We preserve the original context by including the referenced SO post and the associated GitHub repository",
    "last_updated": "2026",
    "last_updated_quote": "arXiv:2601.04540v1  [cs.SE]  8 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜Ÿæ„å»ºå’Œæ ‡æ³¨ã€‚",
    "build_type_quote": "Overall, it takes approximately 550 person-hours to construct AdaptEval... We propose AdaptEval, the first benchmark for code snippet adaptation.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç¼–è¾‘/é€‚åº”ï¼Œæ¶‰åŠå¯¹ç°æœ‰ä»£ç ç‰‡æ®µçš„ä¿®æ”¹ã€‚",
    "task_granularity_quote": "code snippet adaptation... where developers modify code snippets to fit their specific context.",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "where an increase up to 34.84% in pass@1 is observed.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆä»»åŠ¡çº§å’Œé€‚åº”çº§æè¿°ï¼‰ä¸ä»£ç ï¼ˆåŸå§‹ä»£ç ç‰‡æ®µï¼‰ç›¸ç»“åˆã€‚",
    "input_modality_quote": "Task-level annotations describe concise developer intentions... while adaptation-level ones simulate developersâ€™ step-by-step adaptation process, serving as specific instructions for LLMs to implement code changes.",
    "output_modality": "ä»£ç ï¼Œå³é€‚åº”åçš„ä»£ç ç‰‡æ®µã€‚",
    "output_modality_quote": "implement code changes... assessing the correctness of code",
    "task_io_type": "æ–‡æœ¬ä¸ä»£ç åˆ°ä»£ç ï¼Œè¾“å…¥æ˜¯è‡ªç„¶è¯­è¨€è¦æ±‚å’ŒåŸå§‹ä»£ç ï¼Œè¾“å‡ºæ˜¯ä¿®æ”¹åçš„ä»£ç ã€‚",
    "task_io_type_quote": "serving as specific instructions for LLMs to implement code changes.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. å®é™…ä¸Šä¸‹æ–‡ï¼šä»»åŠ¡æ¥æºäºå¼€å‘è€…å®è·µï¼Œä¿ç•™Stack Overflowå’ŒGitHubçš„ä¸Šä¸‹æ–‡ã€‚2. å¤šç²’åº¦æ ‡æ³¨ï¼šæ¯ä¸ªä»»åŠ¡åœ¨ä»»åŠ¡çº§å’Œé€‚åº”çº§éƒ½æœ‰æ ‡æ³¨ã€‚3. ç»†ç²’åº¦è¯„ä¼°ï¼šåŒ…å«ç»“åˆé€‚åº”çº§å’Œå‡½æ•°çº§çš„åŒå±‚æµ‹è¯•æ¡†æ¶ã€‚",
    "unique_features_quote": "AdaptEval incorporates the following three distinctive features: First, practical context... Second, multi-granularity annotation... Third, fine-grained evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests",
    "data_size_quantity": 164,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç‰‡æ®µé€‚åº”èƒ½åŠ›', 'å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£', 'å¯¹ä»»åŠ¡è¦æ±‚çš„éµå¾ª', 'æ‰§è¡Œå…·ä½“ä»£ç ä¿®æ”¹çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['pass@1']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç é‡ç”¨', 'ä»£ç ç‰‡æ®µé€‚åº”']",
    "source_type_normalized": "['æ•°æ®æ¥æºäºå¼€å‘è€…çš„å®é™…é€‚åº”å®è·µ', 'ä»Stack Overflowå¸–å­å’Œå…³è”çš„GitHubä»“åº“ä¸­æ”¶é›†']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.08806_output/content.md",
    "benchmark_name": "APEXâ€“SWE",
    "benchmark_name_quote": "We introduce the AI Productivity Index for Software Engineering (APEXâ€“SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work.",
    "dataset_url": "https://huggingface.co/datasets/mercor/APEX-SWE, https://github.com/Mercor-Intelligence/apex-evals",
    "dataset_url_quote": "We have released an open-source dev set via Hugging Face with a CC-BY license1 and our grading harness is available open-source on GitHub.2\n1https://huggingface.co/datasets/mercor/APEX-SWE\n2https://github.com/Mercor-Intelligence/apex-evals",
    "task_description": "è¯„ä¼°å‰æ²¿AIæ¨¡å‹æ‰§è¡Œå…·æœ‰ç»æµä»·å€¼çš„è½¯ä»¶å·¥ç¨‹å·¥ä½œçš„èƒ½åŠ›ã€‚åŒ…å«ä¸¤ç§ä»»åŠ¡ç±»å‹ï¼šé›†æˆä»»åŠ¡ï¼ˆæ„å»ºè·¨å¼‚æ„äº‘åŸè¯­ã€ä¸šåŠ¡åº”ç”¨å’ŒåŸºç¡€è®¾æ–½å³ä»£ç æœåŠ¡çš„ç«¯åˆ°ç«¯ç³»ç»Ÿï¼‰å’Œå¯è§‚æµ‹æ€§ä»»åŠ¡ï¼ˆä½¿ç”¨é¥æµ‹ä¿¡å·ï¼ˆå¦‚æ—¥å¿—å’Œä»ªè¡¨æ¿ï¼‰ä»¥åŠéç»“æ„åŒ–ä¸Šä¸‹æ–‡è°ƒè¯•ç”Ÿäº§æ•…éšœï¼‰ã€‚",
    "task_description_quote": "APEXâ€“SWE assesses two novel task types that reflect real-world software engineering: (1) Integration tasks (n = 100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n = 100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context.",
    "dimension": "çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é›†æˆä¸å¯è§‚æµ‹æ€§ä»»åŠ¡ä¸­çš„è®¤çŸ¥æ¨ç†å’Œä»£ç†èƒ½åŠ›ã€‚",
    "dimension_quote": "Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting.",
    "evaluation_method": "ä½¿ç”¨Pass@1å’ŒPass@3æŒ‡æ ‡ã€‚é›†æˆä»»åŠ¡é€šè¿‡pytestå¥—ä»¶éªŒè¯æ­£ç¡®æ€§ï¼›å¯è§‚æµ‹æ€§ä»»åŠ¡é‡‡ç”¨FAIL_TO_PASS / PASS_TO_PASSæ–¹æ³•ï¼Œåº”ç”¨æ¨¡å‹è¡¥ä¸å¹¶æ‰§è¡Œæ¡†æ¶ç‰¹å®šæµ‹è¯•ã€‚",
    "evaluation_method_quote": "For the APEXâ€“SWE leaderboard, we assess modelsâ€™ outputs using Pass@1, defined as the percentage of tasks where the modelâ€™s first attempt passes all correctness tests. For Integration tasks, correctness is verified via a pytest suite that interacts directly with service APIs... For Observability tasks, correctness follows a FAIL_TO_PASS / PASS_TO_PASS methodology inspired by SWE-bench (Jimenez et al., 2024). The harness applies the modelâ€™s patch and executes framework-specific tests (pytest, go test, jest).",
    "context_dependency": "è·¨å¹³å°é›†æˆã€å¤šæœåŠ¡ç¯å¢ƒã€ç”Ÿäº§çº§åº”ç”¨å’ŒåŸºç¡€è®¾æ–½ã€‚éœ€è¦å¤„ç†é•¿ä¸Šä¸‹æ–‡ä¾èµ–ã€‚",
    "context_dependency_quote": "Real production environments involve cross-platform integration, infrastructure provisioning, and debugging production failures with incomplete information. ... whereas Qwen3 Coder and Grok 4 perform considerably worse at 14%, struggling to manage the long-context dependencies required for complex integration tasks.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œç‰¹åˆ«æ˜¯äº‘é›†æˆã€åŸºç¡€è®¾æ–½å³ä»£ç ã€ç”Ÿäº§ç¯å¢ƒè°ƒè¯•ã€‚",
    "problem_domain_quote": "APEXâ€“SWE assesses two novel task types that reflect real-world software engineering: (1) Integration tasks... (2) Observability tasks...",
    "problem_difficulty": "çœŸå®ä¸–ç•Œç”Ÿäº§çº§è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œå¤æ‚åº¦é«˜ï¼Œåæ˜ ç°å®ä¸–ç•Œçš„ç»¿è‰²å·¥ç¨‹å·¥ä½œæµç¨‹ã€‚",
    "problem_difficulty_quote": "Tasks reflect realistic greenfield engineering workflows, including implementing idempotent payment pipelines, configuring serverless event-driven architectures, and synchronizing data between CRM and ticketing systems.",
    "language": "é›†æˆä»»åŠ¡æœªæ˜ç¡®æŒ‡å®šå•ä¸€è¯­è¨€ï¼Œæ¶‰åŠå¤šç§æœåŠ¡é…ç½®ã€‚å¯è§‚æµ‹æ€§ä»»åŠ¡æ¶‰åŠGoã€Pythonã€TypeScriptã€Javaå’ŒC++ã€‚",
    "language_quote": "Each task includes a real GitHub issue from widely adopted programming languages, including Go, TypeScript, Python, Rust, and Java. ... Observability tasks are distributed across five languages: Go (30%), Python (25%), TypeScript (25%), Java (10%), and C++ (10%).",
    "data_size": "åŒ…å«200ä¸ªä»»åŠ¡ï¼š100ä¸ªé›†æˆä»»åŠ¡å’Œ100ä¸ªå¯è§‚æµ‹æ€§ä»»åŠ¡ã€‚å¦æœ‰ä¸€ä¸ªåŒ…å«50ä¸ªä»»åŠ¡çš„å¼€å‘é›†ã€‚",
    "data_size_quote": "Integration tasks (n = 100)... Observability tasks (n = 100)... We open-source the APEXâ€“SWE evaluation harness and a dev set (n = 50).",
    "source_type": "é›†æˆä»»åŠ¡ç”±æ‹¥æœ‰3å¹´ä»¥ä¸Šç»éªŒçš„è½¯ä»¶å·¥ç¨‹å¸ˆåˆ›å»ºã€‚å¯è§‚æµ‹æ€§ä»»åŠ¡æºè‡ªçœŸå®ä¸–ç•Œçš„GitHub Issueâ€“PRå¯¹ï¼Œæ¥è‡ªè‡³å°‘æœ‰350é¢—æ˜Ÿçš„ä»“åº“ã€‚",
    "source_type_quote": "Tasks were created by software engineers with 3+ years of experience. ... Tasks are derived from real-world GitHub Issueâ€“PR pairs, sourced from repositories with at least 350 stars.",
    "last_updated": "2026",
    "last_updated_quote": "arXiv:2601.08806v1  [cs.SE]  13 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±Mercorå…¬å¸çš„ä¸“å®¶å›¢é˜Ÿæ„å»ºã€‚",
    "build_type_quote": "Production processes for APEXâ€“SWE Integration and APEXâ€“SWE Observability, leveraging Mercorâ€™s platform of experts.",
    "contamination_status": "è®¾è®¡ä¸Šå¯èƒ½å…·æœ‰è¾ƒä½æ±¡æŸ“é£é™©ï¼Œå› ä¸ºä»»åŠ¡åŸºäºæ–°åˆ›å»ºçš„é›†æˆåœºæ™¯å’Œç»è¿‡ç­›é€‰çš„å¤æ‚GitHub Issue-PRå¯¹ã€‚",
    "contamination_status_quote": "Tasks were created by software engineers... Tasks are derived from real-world GitHub Issueâ€“PR pairs... We filtered for complexity, selecting only patches with at least 100 lines of code impacting at least three files.",
    "dataset_license": "CC-BYè®¸å¯è¯",
    "dataset_license_quote": "We have released an open-source dev set via Hugging Face with a CC-BY license",
    "task_granularity": "ç«¯åˆ°ç«¯ç³»ç»Ÿæ„å»ºä¸ç”Ÿäº§æ•…éšœè¯Šæ–­ä¿®å¤ï¼Œå±äºå®è§‚å·¥ç¨‹ä»»åŠ¡ã€‚",
    "task_granularity_quote": "APEXâ€“SWE assesses two novel task types that reflect real-world software engineering: (1) Integration tasks... which require constructing end-to-end systems... (2) Observability tasks... which require debugging production failures...",
    "evaluation_metrics": "Pass@1, Pass@3ã€‚æ­¤å¤–è¿˜ä½¿ç”¨è¯„ä¼°å·¥ç¨‹è´¨é‡çš„è¯„åˆ†æ ‡å‡†ï¼ˆåŠŸèƒ½æ€§ã€é²æ£’æ€§ã€é£æ ¼ï¼‰ã€‚",
    "evaluation_metrics_quote": "For the APEXâ€“SWE leaderboard, we assess modelsâ€™ outputs using Pass@1... We also report Pass@3 in this paper... APEXâ€“SWE uses rubrics to assess engineering quality.",
    "input_modality": "è‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°ã€ç³»ç»Ÿæç¤ºã€å·¥å…·å®šä¹‰ã€æ–‡ä»¶ï¼ˆå¦‚interface.mdï¼‰ã€æ—¥å¿—ã€èŠå¤©å†å²ã€APIè§„èŒƒã€‚",
    "input_modality_quote": "The model receives a system prompt containing task instructions and tool definitions. ... The model is provided with a summary of the user issue, an interface.md file describing relevant repository functions, and specifications for available Model Context Protocol (MCP) tools. Chat data is pulled from public developer discussions...",
    "output_modality": "ä»£ç ã€é…ç½®è„šæœ¬ã€åŸºç¡€è®¾æ–½å³ä»£ç ã€è¡¥ä¸æ–‡ä»¶ã€ç»ˆç«¯å‘½ä»¤åºåˆ—ã€‚",
    "output_modality_quote": "Models are required to write application code, configure infrastructure, and deploy functioning services. ... before implementing a fix that resolves the issue...",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ä¸é…ç½®ï¼Œä»¥åŠä»£ç /æ—¥å¿—åˆ°ä¿®å¤æ–¹æ¡ˆã€‚",
    "task_io_type_quote": "APEXâ€“SWE assesses two novel task types... (1) Integration tasks... (2) Observability tasks...",
    "execution_environment": "å®¹å™¨åŒ–ç¯å¢ƒï¼ŒåŒ…å«å¤šç§äº‘æœåŠ¡æ¨¡æ‹Ÿï¼ˆAWS LocalStackï¼‰ã€ä¸šåŠ¡åº”ç”¨ï¼ˆEspoCRM, Medusaç­‰ï¼‰ã€æ—¥å¿—èšåˆç³»ç»Ÿï¼ˆLoki, Grafanaï¼‰ã€æ•°æ®åº“ï¼ˆPostgreSQLï¼‰ã€‚æ¨¡å‹é€šè¿‡ç»ˆç«¯ã€æ–‡ä»¶æ“ä½œå’ŒMCPæœåŠ¡å™¨å·¥å…·ä¸ç¯å¢ƒäº¤äº’ã€‚",
    "execution_environment_quote": "The test stack includes cloud primitives (AWS LocalStack: S3, Lambda, DynamoDB, Kinesis) and production-grade business applications (EspoCRM, Medusa, Zammad, Plane). ... Each task deploys a containerized environment orchestrating five services: a client workspace, Loki and Promtail for log aggregation, Grafana for visualization, and Plane/Mattermost for ticket and chat context. ... Models have access to three categories of tools: Terminal (Bash execution...), File Operations..., and MCP Servers...",
    "unique_features": "ä¸“æ³¨äºçœŸå®ä¸–ç•Œã€ç»æµä»·å€¼çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯é›†æˆå’Œå¯è§‚æµ‹æ€§è¿™ä¸¤ç§æ–°ä»»åŠ¡ç±»å‹ã€‚å¼ºè°ƒè®¤çŸ¥æ¨ç†å’Œä»£ç†èƒ½åŠ›ã€‚åŒ…å«å¤æ‚çš„èº«ä»½éªŒè¯å’Œå®‰å…¨æ–¹æ¡ˆæµ‹è¯•ã€‚",
    "unique_features_quote": "Unlike existing evaluations that focus on narrow, well-defined tasks, APEXâ€“SWE assesses two novel task types that reflect real-world software engineering... Tasks have authentication schemes to test modelsâ€™ ability to handle real-world credential management... Our analysis shows that strong performance is primarily driven by epistemic reasoning... combined with agency to resolve uncertainty prior to acting.",
    "data_size_quantity": 200,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['é›†æˆä»»åŠ¡æœªæ˜ç¡®æŒ‡å®šå•ä¸€è¯­è¨€ï¼Œæ¶‰åŠå¤šç§æœåŠ¡é…ç½®', 'å¯è§‚æµ‹æ€§ä»»åŠ¡æ¶‰åŠGo', 'Python', 'TypeScript', 'Java', 'C++']",
    "dimension_normalized": "['çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹èƒ½åŠ›', 'ç‰¹åˆ«æ˜¯é›†æˆä¸å¯è§‚æµ‹æ€§ä»»åŠ¡ä¸­çš„è®¤çŸ¥æ¨ç†å’Œä»£ç†èƒ½åŠ›']",
    "evaluation_method_normalized": "['Pass@1', 'Pass@3', 'è¯„ä¼°å·¥ç¨‹è´¨é‡çš„è¯„åˆ†æ ‡å‡†ï¼ˆåŠŸèƒ½æ€§', 'é²æ£’æ€§', 'é£æ ¼ï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ç‰¹åˆ«æ˜¯äº‘é›†æˆ', 'åŸºç¡€è®¾æ–½å³ä»£ç ', 'ç”Ÿäº§ç¯å¢ƒè°ƒè¯•']",
    "source_type_normalized": "['é›†æˆä»»åŠ¡ç”±æ‹¥æœ‰3å¹´ä»¥ä¸Šç»éªŒçš„è½¯ä»¶å·¥ç¨‹å¸ˆåˆ›å»º', 'å¯è§‚æµ‹æ€§ä»»åŠ¡æºè‡ªçœŸå®ä¸–ç•Œçš„GitHub Issueâ€“PRå¯¹ï¼Œæ¥è‡ªè‡³å°‘æœ‰350é¢—æ˜Ÿçš„ä»“åº“']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "CC-BY-SA 4.0",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2601.09703_output/content.md",
    "benchmark_name": "ShorterCodeBench",
    "benchmark_name_quote": "producing ShorterCodeBenchâ€”a corpus of validated âŸ¨original code, simplified codeâŸ©pairs with semantic consistency",
    "dataset_url": "https://github.com/DeepSoftwareAnalytics/ShorterCode",
    "dataset_url_quote": "We provide the code and data at https://github.com/DeepSoftwareAnalytics/ShorterCode.",
    "task_description": "è¯¥è¯„æµ‹åŸºå‡†æ˜¯ä¸€ä¸ªç”¨äºä»£ç ç®€æ´æ€§ä¼˜åŒ–çš„æ•°æ®é›†ï¼ŒåŒ…å«åŸå§‹ä»£ç å’Œç®€åŒ–åçš„ä»£ç å¯¹ï¼Œæ—¨åœ¨ä¼˜åŒ–ä»£ç ç”Ÿæˆæ•ˆç‡ã€‚",
    "task_description_quote": "producing ShorterCodeBench, a corpus of 828 validated âŸ¨original code, simplified codeâŸ©pairs with semantic consistency.",
    "dimension": "ä»£ç ç”Ÿæˆæ•ˆç‡ï¼ˆç®€æ´æ€§ï¼‰",
    "dimension_quote": "optimizes code generation efficiency while preserving semantic equivalence and readability.",
    "evaluation_method": "é€šè¿‡ç”Ÿæˆæ•ˆç‡çš„æ”¹è¿›ç™¾åˆ†æ¯”è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä¸ç°æœ‰æ–¹æ³•åœ¨HumanEvalç­‰åŸºå‡†ä¸Šçš„è¡¨ç°è¿›è¡Œæ¯”è¾ƒã€‚",
    "evaluation_method_quote": "achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶å·¥ç¨‹è‡ªåŠ¨åŒ–ï¼Œä»£ç ç”Ÿæˆ",
    "problem_domain_quote": "As a pivotal branch of software engineering automation, code generation tasks aim to translate user specifications into executable implementations",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "we first design ten syntax-level simplification rules for Python",
    "data_size": "åŒ…å«828ä¸ªç»è¿‡éªŒè¯çš„ä»£ç å¯¹",
    "data_size_quote": "producing ShorterCodeBench, a corpus of 828 validated âŸ¨original code, simplified codeâŸ©pairs",
    "source_type": "åŸºäºMBPPæ•°æ®é›†çš„æ‰‹åŠ¨æ£€æŸ¥å’Œä¸“å®¶æ‰©å±•ï¼Œç»“åˆåŸºäºè§„åˆ™çš„æ”¹å†™å’ŒLLMå¼•å¯¼çš„ç»†åŒ–",
    "source_type_quote": "We manually conduct inspection of each MBPP dataset entry... We engage 3 experts... a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement",
    "last_updated": "2026",
    "last_updated_quote": "arXiv:2601.09703v1  [cs.SE]  14 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We present and publicly release ShorterCodeBench, a high-quality code brevity optimization dataset comprising 828 carefully curated âŸ¨original code, simplified codeâŸ© pairs.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆç®€æ´æ€§ä¼˜åŒ–ï¼‰",
    "task_granularity_quote": "optimizes code generation efficiency",
    "evaluation_metrics": "ç”Ÿæˆæ•ˆç‡æ”¹è¿›ç™¾åˆ†æ¯”",
    "evaluation_metrics_quote": "achieving an improvement of 18.1%-37.8% in generation efficiency",
    "input_modality": "ä»£ç ",
    "input_modality_quote": "âŸ¨original code, simplified codeâŸ©pairs",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "âŸ¨original code, simplified codeâŸ©pairs",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "âŸ¨original code, simplified codeâŸ©pairs",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºé€šè¿‡è¯­æ³•çº§ç®€åŒ–è§„åˆ™ï¼ˆå¦‚å¤šå˜é‡èµ‹å€¼ç®€åŒ–ã€returnè¯­å¥ç®€åŒ–ã€èµ‹å€¼æ“ä½œç®€åŒ–ã€æ¡ä»¶è¯­å¥ç®€åŒ–ï¼‰æ¥ä¼˜åŒ–ä»£ç é•¿åº¦ï¼ŒåŒæ—¶ä¿æŒè¯­ä¹‰ç­‰ä»·å’Œå¯è¯»æ€§ã€‚æ•°æ®é›†é€šè¿‡æ··åˆæ•°æ®åˆæˆæµç¨‹æ„å»ºï¼Œç»“åˆäº†åŸºäºè§„åˆ™çš„æ”¹å†™å’ŒLLMå¼•å¯¼çš„ç»†åŒ–ã€‚",
    "unique_features_quote": "we first design ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise... a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement",
    "data_size_quantity": 828,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ•ˆç‡ï¼ˆç®€æ´æ€§ï¼‰']",
    "evaluation_method_normalized": "['ç”Ÿæˆæ•ˆç‡æ”¹è¿›ç™¾åˆ†æ¯”']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹è‡ªåŠ¨åŒ–', 'ä»£ç ç”Ÿæˆ']",
    "source_type_normalized": "['åŸºäºMBPPæ•°æ®é›†çš„æ‰‹åŠ¨æ£€æŸ¥å’Œä¸“å®¶æ‰©å±•', 'ç»“åˆåŸºäºè§„åˆ™çš„æ”¹å†™å’ŒLLMå¼•å¯¼çš„ç»†åŒ–']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.11077_output/content.md",
    "benchmark_name": "ABC-Bench",
    "benchmark_name_quote": "To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic back-end coding within a realistic, executable workflow.",
    "dataset_url": "https://github.com/OpenMOSS/ABC-Bench, https://huggingface.co/datasets/OpenMOSS-Team/ABC-Bench",
    "dataset_url_quote": "Github: https://github.com/OpenMOSS/ABC-Bench\nDataset: https://huggingface.co/datasets/OpenMOSS-Team/ABC-Bench",
    "task_description": "è¯„ä¼°æ™ºèƒ½ä½“åœ¨çœŸå®ã€å¯æ‰§è¡Œçš„å·¥ä½œæµä¸­è¿›è¡Œåç«¯ç¼–ç çš„èƒ½åŠ›ï¼Œè¦æ±‚æ™ºèƒ½ä½“ç®¡ç†ä»ä»“åº“æ¢ç´¢åˆ°å®ä¾‹åŒ–å®¹å™¨åŒ–æœåŠ¡å¹¶æœ€ç»ˆé€šè¿‡ç«¯åˆ°ç«¯APIæµ‹è¯•çš„æ•´ä¸ªå¼€å‘ç”Ÿå‘½å‘¨æœŸã€‚",
    "task_description_quote": "ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests.",
    "dimension": "æ™ºèƒ½ä½“åç«¯ç¼–ç èƒ½åŠ›ï¼Œæ¶µç›–ä»“åº“æ¢ç´¢ã€ä»£ç ç¼–è¾‘ã€ç¯å¢ƒé…ç½®ã€éƒ¨ç½²å’Œç«¯åˆ°ç«¯æµ‹è¯•çš„å…¨ç”Ÿå‘½å‘¨æœŸè¯„ä¼°ã€‚",
    "dimension_quote": "Real-world backend development mandates a continuous workflow spanning five distinct stages: (1) Repository Exploration (Expl.), (2) Code Editing (Code), (3) Environment Setup (Env.), (4) Deployment (Deploy), and (5) End-to-End Testing (E2E).",
    "evaluation_method": "é€šè¿‡å¤–éƒ¨APIçº§åˆ«çš„é›†æˆæµ‹è¯•æ¥ä¸¥æ ¼è¯„ä¼°æ­£ç¡®æ€§ï¼Œåªæœ‰åœ¨éƒ¨ç½²çš„æœåŠ¡æ­£ç¡®å¯åŠ¨å¹¶è¡¨ç°å‡ºé¢„æœŸè¡Œä¸ºæ—¶æ‰ç»™äºˆé€šè¿‡ã€‚",
    "evaluation_method_quote": "Once the service is launched, we evaluate correctness strictly via external API-level integration tests, awarding credit only when the deployed service starts correctly and exhibits the expected behavior.",
    "context_dependency": "ä»“åº“çº§åˆ«ï¼Œæ¶‰åŠå¤šæ–‡ä»¶é¡¹ç›®ï¼Œéœ€è¦æ¢ç´¢å’Œç†è§£æ•´ä¸ªä»£ç åº“ç»“æ„ã€‚",
    "context_dependency_quote": "Each task goes beyond localized code edits and requires the agent to configure the environment and instantiate a containerized service.",
    "problem_domain": "åç«¯å¼€å‘ï¼Œæ¶µç›–æ•°æ®åˆ†æå’Œæœç´¢ç³»ç»Ÿã€å•†ä¸šå¹³å°ã€æ”¯ä»˜ç½‘å…³å’Œå¼€å‘è€…å·¥å…·ç­‰å¤šç§å®é™…å·¥ç¨‹é¢†åŸŸã€‚",
    "problem_domain_quote": "These domains range from data analytics and search systems to commerce platforms, payment gateways, and developer tooling.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„ç”Ÿäº§ç¯å¢ƒï¼Œè¦æ±‚ç«¯åˆ°ç«¯çš„æ“ä½œèƒ½åŠ›ã€‚",
    "problem_difficulty_quote": "ABC-Bench closes this gap by assessing the complete backend lifecycle with end-to-end, deployment-oriented verification.",
    "language": "C#ã€Rustã€JavaScriptã€Pythonã€Javaã€Goã€PHPã€Rubyï¼Œå…±8ç§ç¼–ç¨‹è¯­è¨€ã€‚",
    "language_quote": "224 curated tasks spanning 8 backend programming languages and 19 frameworks",
    "data_size": "åŒ…å«224ä¸ªä»»åŠ¡ã€‚",
    "data_size_quote": "ABC-Bench comprises 224 tasks",
    "source_type": "ä»2000ä¸ªå¼€æºã€MITè®¸å¯çš„ä»“åº“ä¸­ç­›é€‰å’Œæ„å»ºã€‚",
    "source_type_quote": "Applied to 2,000 open-source repositories, it yields 224 curated tasks...\nWe initiate the process by filtering a pool of 2,000 open-source, MIT-licensed repositories",
    "last_updated": "2026",
    "last_updated_quote": "arXiv:2601.11077v1  [cs.SE]  16 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œé€šè¿‡åä¸ºABC-Pipelineçš„è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ä»å¼€æºä»“åº“ç”Ÿæˆä»»åŠ¡ã€‚",
    "build_type_quote": "To construct the benchmark at scale, we build ABC-Pipeline, a task-generation workflow that produces candidate tasks from open-source backend repositories with greatly reduced manual intervention.",
    "contamination_status": "ä»å¼€æºä»“åº“æ–°æ„å»ºï¼Œæ—¨åœ¨åæ˜ çœŸå®å·¥ç¨‹éœ€æ±‚ï¼Œæ±¡æŸ“é£é™©ç›¸å¯¹è¾ƒä½ã€‚",
    "contamination_status_quote": "the tasks are drawn from a broad spectrum of real-world domains, ensuring that the benchmark reflects practical engineering needs.",
    "dataset_license": "MITè®¸å¯è¯",
    "dataset_license_quote": "MIT-licensed repositories",
    "task_granularity": "ç«¯åˆ°ç«¯çš„åç«¯å¼€å‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬ä»£ç ä¿®æ”¹ã€ç¯å¢ƒé…ç½®å’Œéƒ¨ç½²ã€‚",
    "task_granularity_quote": "Each task goes beyond localized code edits and requires the agent to configure the environment and instantiate a containerized service.",
    "evaluation_metrics": "pass@1ç‡",
    "evaluation_metrics_quote": "Even the top-performing model achieves only a 63.2% pass@1 rate",
    "input_modality": "è‡ªç„¶è¯­è¨€ä»»åŠ¡æŒ‡ä»¤",
    "input_modality_quote": "the evaluation setup launches an outer container that hosts the agent, delivers the task prompt.",
    "output_modality": "ä»£ç ä¿®æ”¹ã€ç¯å¢ƒé…ç½®æ–‡ä»¶å’ŒDockerfileç­‰",
    "output_modality_quote": "the agent is granted full autonomy to explore the repository, modify code, install dependencies, and update Docker configurations",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ä¸é…ç½®",
    "task_io_type_quote": "The agent must orchestrate the entire process without human intervention.",
    "execution_environment": "éš”ç¦»çš„æ²™ç®±å®¹å™¨ç¯å¢ƒï¼ŒåŒ…å«ç”¨äºæ™ºèƒ½ä½“çš„å·¥ä½œç©ºé—´å’Œç”¨äºéƒ¨ç½²æœåŠ¡çš„ç‹¬ç«‹å†…éƒ¨å®¹å™¨ã€‚",
    "execution_environment_quote": "We evaluate models and agents using a standardized, isolated sandbox environment, which strictly separates the agentâ€™s workspace from the backend service under test.",
    "unique_features": "é¦–ä¸ªæ¶µç›–å®Œæ•´åç«¯å¼€å‘ç”Ÿå‘½å‘¨æœŸï¼ˆä»“åº“æ¢ç´¢ã€ä»£ç ç¼–è¾‘ã€ç¯å¢ƒé…ç½®ã€éƒ¨ç½²ã€ç«¯åˆ°ç«¯æµ‹è¯•ï¼‰çš„åŸºå‡†ï¼Œä¸“æ³¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨çœŸå®ã€å¯æ‰§è¡Œå·¥ä½œæµä¸­çš„èƒ½åŠ›ï¼Œå¹¶é‡‡ç”¨è‡ªåŠ¨åŒ–æµæ°´çº¿ä»å¼€æºä»“åº“å¤§è§„æ¨¡æ„å»ºä»»åŠ¡ã€‚",
    "unique_features_quote": "ABC-Bench is the only benchmark to encompass this entire lifecycle.\nDistinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests.",
    "data_size_quantity": 224,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['C#', 'Rust', 'JavaScript', 'Python', 'Java', 'Go', 'PHP', 'Ruby']",
    "dimension_normalized": "['æ™ºèƒ½ä½“åç«¯ç¼–ç èƒ½åŠ›', 'ä»“åº“æ¢ç´¢', 'ä»£ç ç¼–è¾‘', 'ç¯å¢ƒé…ç½®', 'éƒ¨ç½²', 'ç«¯åˆ°ç«¯æµ‹è¯•', 'å…¨ç”Ÿå‘½å‘¨æœŸè¯„ä¼°']",
    "evaluation_method_normalized": "['pass@1ç‡']",
    "problem_domain_normalized": "['åç«¯å¼€å‘', 'æ•°æ®åˆ†æ', 'æœç´¢ç³»ç»Ÿ', 'å•†ä¸šå¹³å°', 'æ”¯ä»˜ç½‘å…³', 'å¼€å‘è€…å·¥å…·', 'å®é™…å·¥ç¨‹é¢†åŸŸ']",
    "source_type_normalized": "['å¼€æº', 'MITè®¸å¯', 'ä»“åº“']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "MIT",
    "contamination_status_normalized": "é«˜æ±¡æŸ“é£é™©"
  }
]