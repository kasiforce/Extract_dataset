[
  {
    "source_paper": "2511.15757_output/content.md",
    "benchmark_name": "RGym",
    "benchmark_name_quote": "In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "Linuxå†…æ ¸è‡ªåŠ¨ç¨‹åºä¿®å¤è¯„ä¼°ï¼Œä¸“æ³¨äºå†…æ ¸ç©ºé—´è°ƒè¯•å’Œä¿®å¤çš„å¤æ‚æ€§",
    "task_description_quote": "Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on user-space applications and overlook the complexities of kernel-space debugging and repair.",
    "dimension": "Linuxå†…æ ¸ç¨‹åºä¿®å¤èƒ½åŠ›è¯„ä¼°ï¼ŒåŒ…æ‹¬å®šä½ã€è¡¥ä¸ç”Ÿæˆã€éªŒè¯å’Œæˆæœ¬/å»¶è¿Ÿè€ƒè™‘",
    "dimension_quote": "These characteristics make the kernel an ideal stress test for evaluating LLM-based APR, from localization to patch generation, validation, and cost/latency consideration.",
    "evaluation_method": "é€šè¿‡ç¼–è¯‘ä¿®è¡¥åçš„å†…æ ¸ã€è¿è¡Œæ¦‚å¿µéªŒè¯ç¨‹åºå¹¶æŠ¥å‘Šç»“æœæ¥è¯„ä¼°è¡¥ä¸è´¨é‡",
    "evaluation_method_quote": "RGym overall compiles patched kernels, runs PoCs, and reports results.",
    "context_dependency": "Linuxå†…æ ¸çº§åˆ«çš„å¤æ‚ä¾èµ–å…³ç³»ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡ã€æ·±åº¦ä¾èµ–ã€æ™®éå¹¶å‘å’Œç¡¬ä»¶åº•å±‚äº¤äº’",
    "context_dependency_quote": "with its massive scale, deep dependency, and pervasive concurrency and low-level interactions with hardware.",
    "problem_domain": "æ“ä½œç³»ç»Ÿå†…æ ¸å¼€å‘ï¼Œç‰¹åˆ«æ˜¯Linuxå†…æ ¸å†…å­˜å®‰å…¨æ¼æ´ä¿®å¤",
    "problem_domain_quote": "The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions.",
    "problem_difficulty": "é«˜éš¾åº¦å†…æ ¸çº§bugä¿®å¤ï¼ŒåŒ…å«å†…å­˜æŸåç­‰æœ€ä¸¥é‡ç±»å‹çš„bug",
    "problem_difficulty_quote": "filtering to KASAN bugs [13], which represent the most severe types of bugs (memory corruption)",
    "language": "Cè¯­è¨€ï¼ˆLinuxå†…æ ¸å¼€å‘ï¼‰",
    "language_quote": "From 6,088 Syzbot bugs, we retain those with fix commits, reproducers, crash reports, and kernel configs",
    "data_size": "åŒ…å«143ä¸ªå·²éªŒè¯çš„å†…æ ¸bugçš„æ•°æ®é›†",
    "data_size_quote": "We test on a filtered and verified dataset of 143 bugs.",
    "source_type": "æ¥è‡ªSyzkallerå†…æ ¸æ¨¡ç³Šæµ‹è¯•å™¨å’ŒSyzbotè‡ªåŠ¨å´©æºƒæŠ¥å‘Šç³»ç»Ÿçš„çœŸå®å†…æ ¸bug",
    "source_type_quote": "Syzkaller [6], a coverage-guided kernel fuzzer, together with Syzbot [5], an automated online crash reporting system developed by Google, provides a valuable ecosystem that makes kernel-bug collection possible",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.15757v1 [cs.SE] 19 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºçœŸå®å†…æ ¸bugæ„å»º",
    "build_type_quote": "We organize a dataset of 143 kernel bugs from Syzbot into an easily consumable format and verified the reproducibility of the bug on the patch parent.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ï¼Œç‰¹åˆ«æ˜¯å†…æ ¸å†…å­˜å®‰å…¨æ¼æ´ä¿®å¤",
    "task_granularity_quote": "automated program repair (APR) in the Linux kernel space",
    "evaluation_metrics": "è¡¥ä¸é€šè¿‡ç‡ï¼ˆpass rateï¼‰ã€é”™è¯¯è¡¥ä¸ç‡ï¼ˆbad patch rateï¼‰ã€æ¯bugå¹³å‡æˆæœ¬",
    "evaluation_metrics_quote": "Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug.",
    "input_modality": "å´©æºƒæŠ¥å‘Šã€è°ƒç”¨æ ˆã€bugè¯±å¯¼æäº¤ã€å†…æ ¸é…ç½®",
    "input_modality_quote": "inputs (patch, commit, source, config, compiler, cores, timeout, metadata)",
    "output_modality": "å†…æ ¸è¡¥ä¸ä»£ç ",
    "output_modality_quote": "The LLM lists candidate functions, receives their definitions, and returns their patched definitions.",
    "task_io_type": "å´©æºƒä¿¡æ¯åˆ°å†…æ ¸è¡¥ä¸ä»£ç ",
    "task_io_type_quote": "Our APR generates a patch via the Simple Agent or Function Exploration Agent and tests it with RGym.",
    "execution_environment": "ä½¿ç”¨dockeræ†ç»‘ä½œä¸šä¾èµ–å’ŒQEMUè™šæ‹Ÿæœºï¼Œåœ¨æœ¬åœ°ç¡¬ä»¶ä¸Šè¿è¡Œ",
    "execution_environment_quote": "RGym runs locally using docker to bundle job dependencies and QEMU for VMs.",
    "unique_features": "è½»é‡çº§ã€å¹³å°æ— å…³çš„Linuxå†…æ ¸APRè¯„ä¼°æ¡†æ¶ï¼Œä¸“æ³¨äºæœ¬åœ°å•†å“ç¡¬ä»¶è¿è¡Œï¼Œè§£å†³äº†kGymå¯¹GCPçš„ç¡¬ä¾èµ–é—®é¢˜",
    "unique_features_quote": "we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. RGym solves the compiler and dependency problem by smartly switching build dependencies using docker images depending on the kernel version or compiler string provided in the kernel configuration.",
    "data_size_quantity": 143,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Cè¯­è¨€']",
    "dimension_normalized": "['Linuxå†…æ ¸ç¨‹åºä¿®å¤èƒ½åŠ›è¯„ä¼°', 'å®šä½', 'è¡¥ä¸ç”Ÿæˆ', 'éªŒè¯', 'æˆæœ¬/å»¶è¿Ÿè€ƒè™‘']",
    "evaluation_method_normalized": "['è¡¥ä¸é€šè¿‡ç‡', 'é”™è¯¯è¡¥ä¸ç‡', 'æ¯bugå¹³å‡æˆæœ¬']",
    "problem_domain_normalized": "['æ“ä½œç³»ç»Ÿå†…æ ¸å¼€å‘', 'Linuxå†…æ ¸å†…å­˜å®‰å…¨æ¼æ´ä¿®å¤']",
    "source_type_normalized": "['Syzkallerå†…æ ¸æ¨¡ç³Šæµ‹è¯•å™¨', 'Syzbotè‡ªåŠ¨å´©æºƒæŠ¥å‘Šç³»ç»Ÿ', 'çœŸå®å†…æ ¸bug']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.16005_output/content.md",
    "benchmark_name": "MultiSWE-bench-CPP",
    "benchmark_name_quote": "Evaluated on the MultiSWE-bench-CPP benchmark, InfCode-C++ achieves a resolution rate of 25.58%",
    "dataset_url": "https://github.com/Tokfinity/InfCode",
    "dataset_url_quote": "To support the research community and facilitate future work, we release InfCode-C++ and our evaluation benchmark as an open-source project at https://github.com/Tokfinity/InfCode",
    "task_description": "C++è½¯ä»¶é—®é¢˜è§£å†³ï¼Œä»è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ç”Ÿæˆä¿®å¤è¡¥ä¸",
    "task_description_quote": "Given a software repository ğ¶ and a natural-language issue description ğ·, the objective is to synthesize a patch ğ‘ such that the updated repository ğ¶â€² = ğ¶ âŠ• ğ‘ satisfies the behavioral requirements expressed in ğ· while preserving the original functionality",
    "dimension": "ä»£ç ä¿®å¤èƒ½åŠ›ã€ä¸Šä¸‹æ–‡æ£€ç´¢å‡†ç¡®æ€§ã€ç»“æ„åˆ†æèƒ½åŠ›",
    "dimension_quote": "Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution",
    "evaluation_method": "é€šè¿‡å›å½’æµ‹è¯•å¥—ä»¶éªŒè¯è¡¥ä¸æ­£ç¡®æ€§ï¼Œä½¿ç”¨è§£å†³ç‡ä½œä¸ºè¯„ä¼°æŒ‡æ ‡",
    "evaluation_method_quote": "A correct patch must satisfy: ğ‘¡(ğ¶â€²) = ğ‘¡(ğ¶), âˆ€ğ‘¡âˆˆğ‘‡ and âˆƒğ‘¡ğ·: ğ‘¡ğ·(ğ¶â€²) = PASS, where ğ‘¡ğ· is a failing test case that captures the erroneous behavior described in ğ·",
    "context_dependency": "ä»“åº“çº§åˆ«ï¼Œæ¶‰åŠå¤šæ–‡ä»¶C++é¡¹ç›®",
    "context_dependency_quote": "repository-level issue resolution... in large, statically typed C++ repositories",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼ŒC++ç¨‹åºä¿®å¤",
    "problem_domain_quote": "Automated software issue resolution stands as a critical challenge in software engineering... resolving issues in large-scale, high-performance systems written in C++",
    "problem_difficulty": "çœŸå®ä¸–ç•Œå·¥ç¨‹çº§éš¾åº¦",
    "problem_difficulty_quote": "thousands of real-world GitHub issues... a formidable, unsolved challenge",
    "language": "C++",
    "language_quote": "specifically designed for C++ projects... the first C++-aware autonomous system for end-to-end issue resolution",
    "data_size": "MultiSWE-benchçš„C++å­é›†",
    "data_size_quote": "Evaluated on the MultiSWE-bench-CPP benchmark... on the C++ subset of MultiSWE-bench",
    "source_type": "çœŸå®ä¸–ç•Œçš„GitHubé—®é¢˜",
    "source_type_quote": "thousands of real-world GitHub issues",
    "last_updated": 2025,
    "last_updated_quote": "2025. InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution. 1, 1 (November 2025)",
    "build_type": "åŸºäºç°æœ‰åŸºå‡†æ„å»ºçš„å­é›†",
    "build_type_quote": "the C++ subset of MultiSWE-bench",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤",
    "task_granularity_quote": "issue resolution... synthesize a patch",
    "evaluation_metrics": "è§£å†³ç‡",
    "evaluation_metrics_quote": "achieves a resolution rate of 25.58%",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°",
    "input_modality_quote": "natural-language issue description ğ·",
    "output_modality": "ä»£ç è¡¥ä¸",
    "output_modality_quote": "synthesize a patch ğ‘",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "from a natural language issue description... synthesize a patch",
    "execution_environment": "Dockerç¯å¢ƒ",
    "execution_environment_quote": "Repo Codebase Docker Env",
    "unique_features": "ä¸“é—¨é’ˆå¯¹C++è¯­è¨€çš„å¤æ‚æ€§è®¾è®¡ï¼ŒåŒ…å«è¯­ä¹‰æ£€ç´¢å’ŒASTç»“æ„åŒ–æŸ¥è¯¢",
    "unique_features_quote": "the first C++-aware autonomous system... combines two complementary retrieval mechanismsâ€”semantic code-intent retrieval and deterministic AST-structured queryingâ€”to construct accurate, language-aware context for repair",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['C++']",
    "dimension_normalized": "['ä»£ç ä¿®å¤èƒ½åŠ›', 'ä¸Šä¸‹æ–‡æ£€ç´¢å‡†ç¡®æ€§', 'ç»“æ„åˆ†æèƒ½åŠ›']",
    "evaluation_method_normalized": "['è§£å†³ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'C++ç¨‹åºä¿®å¤']",
    "source_type_normalized": "['çœŸå®ä¸–ç•Œçš„GitHubé—®é¢˜']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.17131_output/content.md",
    "benchmark_name": "UI-CUBE (UiPath Computer Use BEnchmark)",
    "benchmark_name_quote": "We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs.",
    "dataset_url": "https://github.com/UiPath/uipath_enterprise_benchmark",
    "dataset_url_quote": "GitHub: https://github.com/UiPath/uipath_enterprise_benchmark",
    "task_description": "è¯„ä¼°è®¡ç®—æœºä½¿ç”¨ä»£ç†åœ¨ä¼ä¸šç¯å¢ƒä¸­çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç®€å•çš„UIäº¤äº’å’Œå¤æ‚çš„å·¥ä½œæµç¨‹è‡ªåŠ¨åŒ–",
    "task_description_quote": "Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks)",
    "dimension": "åŠŸèƒ½æ­£ç¡®æ€§ã€æ“ä½œå¯é æ€§ã€ç•Œé¢é€‚åº”æ€§ã€å·¥ä½œæµåè°ƒèƒ½åŠ›",
    "dimension_quote": "measuring not only functional accuracy but also operational reliabilityâ€”providing clearer insight into how CUAs can move from experimental demos toward dependable enterprise-grade tools",
    "evaluation_method": "é€šè¿‡åº”ç”¨ç¨‹åºçŠ¶æ€çš„è‡ªåŠ¨åŒ–éªŒè¯æ¥è¯„ä¼°ä»»åŠ¡æˆåŠŸï¼ŒåŒ…æ‹¬å¤šåˆ†è¾¨ç‡æµ‹è¯•å’Œç³»ç»Ÿç•Œé¢å˜åŒ–è¦†ç›–",
    "evaluation_method_quote": "with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state",
    "context_dependency": "å¤šæ­¥éª¤å·¥ä½œæµç¨‹ã€è·¨åº”ç”¨ç¨‹åºåè°ƒ",
    "context_dependency_quote": "complex workflows including copy-paste tasks and enterprise application scenarios, multi-step processes where reliability is as critical as task completion",
    "problem_domain": "ä¼ä¸šè½¯ä»¶è‡ªåŠ¨åŒ–ã€UIäº¤äº’ã€å·¥ä½œæµç¨‹ç®¡ç†",
    "problem_domain_quote": "enterprise application scenarios, enterprise deployment readiness, enterprise workflow automation",
    "problem_difficulty": "ä¸¤ä¸ªéš¾åº¦å±‚çº§ï¼šç®€å•UIäº¤äº’å’Œå¤æ‚å·¥ä½œæµç¨‹",
    "problem_difficulty_quote": "comprising 226 tasks across two difficulty tiers: simple UI interactions (136 tasks) and complex workflows (90 tasks)",
    "language": "ä¸ç‰¹å®šäºç¼–ç¨‹è¯­è¨€ï¼Œä¸»è¦å…³æ³¨UIäº¤äº’",
    "language_quote": NaN,
    "data_size": "226ä¸ªä»»åŠ¡ï¼Œå…¶ä¸­136ä¸ªç®€å•UIäº¤äº’ä»»åŠ¡ï¼Œ90ä¸ªå¤æ‚å·¥ä½œæµç¨‹ä»»åŠ¡",
    "data_size_quote": "comprising 226 tasks across two difficulty tiers: simple UI interactions (136 tasks) and complex workflows (90 tasks comprising 50 copy-paste/business-process tasks and 40 enterprise application tasks)",
    "source_type": "åŸºäºä¼ä¸šåº”ç”¨åœºæ™¯çš„ç³»ç»Ÿæ„å»ºï¼ŒåŒ…å«æ¨¡æ‹Ÿçš„ä¼ä¸šç³»ç»Ÿå·¥ä½œæµç¨‹",
    "source_type_quote": "faithful mocks of complex enterprise workflows to test coordination and operational reliability, including testing with workflows from SAP, Workday, and other enterprise systems",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.17131v1 [cs.SE] 21 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we designed UI-CUBE, a new benchmark tailored to enterprise-like conditions, and used it to evaluate our own CUA implementation",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "UIäº¤äº’è‡ªåŠ¨åŒ–ã€å·¥ä½œæµç¨‹æ‰§è¡Œ",
    "task_granularity_quote": "systematic coverage of atomic UI interactions to map interface-level capabilities, and faithful mocks of complex enterprise workflows to test coordination and operational reliability",
    "evaluation_metrics": "ä»»åŠ¡æˆåŠŸç‡",
    "evaluation_metrics_quote": "Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%",
    "input_modality": "UIç•Œé¢ã€ä»»åŠ¡æŒ‡ä»¤",
    "input_modality_quote": "systematic interface variation coverage, multi-resolution testing",
    "output_modality": "UIäº¤äº’åŠ¨ä½œã€å·¥ä½œæµç¨‹æ‰§è¡Œç»“æœ",
    "output_modality_quote": "automated validation of task success through the application state",
    "task_io_type": "ä»»åŠ¡æŒ‡ä»¤åˆ°UIäº¤äº’",
    "task_io_type_quote": "measuring task completion effectively, they provide limited assessment of enterprise deployment readiness",
    "execution_environment": "ä¼ä¸šè½¯ä»¶ç¯å¢ƒã€å¤šåˆ†è¾¨ç‡æ˜¾ç¤º",
    "execution_environment_quote": "All tasks are evaluated across multiple screen resolutions to assess agent consistency under different display conditions",
    "unique_features": "ä¸“æ³¨äºä¼ä¸šçº§éƒ¨ç½²å‡†å¤‡åº¦è¯„ä¼°ï¼Œæ­ç¤ºèƒ½åŠ›æ‚¬å´–ç°è±¡è€Œéæ¸è¿›æ€§èƒ½ä¸‹é™ï¼Œæä¾›ç³»ç»Ÿç•Œé¢åˆ†ç±»è¦†ç›–",
    "unique_features_quote": "reveals a sharp capability cliff rather than gradual performance degradation, systematic interface variation coverage, multi-resolution testing",
    "data_size_quantity": 226,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['ä¸ç‰¹å®šäºç¼–ç¨‹è¯­è¨€', 'ä¸»è¦å…³æ³¨UIäº¤äº’']",
    "dimension_normalized": "['åŠŸèƒ½æ­£ç¡®æ€§', 'æ“ä½œå¯é æ€§', 'ç•Œé¢é€‚åº”æ€§', 'å·¥ä½œæµåè°ƒèƒ½åŠ›']",
    "evaluation_method_normalized": "['ä»»åŠ¡æˆåŠŸç‡']",
    "problem_domain_normalized": "['ä¼ä¸šè½¯ä»¶è‡ªåŠ¨åŒ–', 'UIäº¤äº’', 'å·¥ä½œæµç¨‹ç®¡ç†']",
    "source_type_normalized": "['åŸºäºä¼ä¸šåº”ç”¨åœºæ™¯çš„ç³»ç»Ÿæ„å»º', 'åŒ…å«æ¨¡æ‹Ÿçš„ä¼ä¸šç³»ç»Ÿå·¥ä½œæµç¨‹']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.15755_output/content.md",
    "benchmark_name": "MyAntFarm.ai",
    "benchmark_name_quote": "To test this hypothesis, we present MyAntFarm.ai, a reproducible experimental framework enabling controlled comparison of three conditions",
    "dataset_url": "https://github.com/Phildram1/myantfarm-ai",
    "dataset_url_quote": "Source code, Docker configurations, and trial outputs are available at: https://github.com/Phildram1/myantfarm-ai",
    "task_description": "è¯„ä¼°å¤šæ™ºèƒ½ä½“LLMç¼–æ’åœ¨äº‹ä»¶å“åº”ä¸­çš„å†³ç­–æ”¯æŒè´¨é‡ï¼Œé€šè¿‡æ¨¡æ‹Ÿè®¤è¯æœåŠ¡å›å½’äº‹ä»¶æ¥æµ‹è¯•ç³»ç»Ÿç”Ÿæˆå¯æ‰§è¡Œå»ºè®®çš„èƒ½åŠ›",
    "task_description_quote": "multi-agent orchestration fundamentally transforms LLM-based incident response quality... achieve 100% actionable recommendation rate",
    "dimension": "å†³ç­–è´¨é‡ã€è¡ŒåŠ¨ç‰¹å¼‚æ€§ã€è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§ã€ç”Ÿäº§å°±ç»ªæ€§",
    "dimension_quote": "We introduce Decision Quality (DQ), a multi-dimensional metric capturing validity, specificity, and correctness",
    "evaluation_method": "ä½¿ç”¨å†³ç­–è´¨é‡(DQ)æŒ‡æ ‡ï¼Œç»“åˆæœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§å’Œæ­£ç¡®æ€§ä¸‰ä¸ªç»´åº¦è¿›è¡Œè‡ªåŠ¨åŒ–è¯„åˆ†",
    "evaluation_method_quote": "DQ measures actionability through three dimensions... Validity, Specificity, Correctness",
    "context_dependency": "å•äº‹ä»¶åœºæ™¯ï¼ŒåŸºäºç‰¹å®šçš„äº‹ä»¶é¥æµ‹æ•°æ®",
    "context_dependency_quote": "All 348 trials used identical context to isolate orchestration effects from scenario variability",
    "problem_domain": "è¿ç»´æ™ºèƒ½(AIOps)ã€äº‹ä»¶å“åº”ã€ç”Ÿäº§ç¯å¢ƒæ•…éšœè¯Šæ–­",
    "problem_domain_quote": "Modern operational teams face a critical gap between incident detection and actionable comprehension... incident response",
    "problem_difficulty": "ç”Ÿäº§çº§äº‹ä»¶å“åº”ï¼Œéœ€è¦å…·ä½“çš„å¯æ‰§è¡Œå‘½ä»¤",
    "problem_difficulty_quote": "time-critical operational contexts... production deployment",
    "language": "ä¸»è¦ä½¿ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œäº‹ä»¶æè¿°å’Œå“åº”ç”Ÿæˆ",
    "language_quote": "The LLM generates a single unstructured text response attempting to address all objectives",
    "data_size": "348æ¬¡è¯•éªŒï¼Œæ¯ä¸ªæ¡ä»¶116æ¬¡è¯•éªŒ",
    "data_size_quote": "Through 348 controlled trials... 116 trials per condition",
    "source_type": "äººå·¥æ„å»ºçš„æ¨¡æ‹Ÿäº‹ä»¶åœºæ™¯",
    "source_type_quote": "All 348 trials used identical context... Authentication service regression post-deployment",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.15755v1 [cs.AI] 19 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºçš„ç ”ç©¶æ¡†æ¶",
    "build_type_quote": "we present MyAntFarm.ai, a reproducible experimental framework",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "äº‹ä»¶å“åº”å†³ç­–æ”¯æŒï¼ŒåŒ…æ‹¬è¯Šæ–­ã€è§„åˆ’å’Œé£é™©è¯„ä¼°",
    "task_granularity_quote": "coordinating specialized LLM agents for diagnosis, planning, and risk assessment",
    "evaluation_metrics": "å†³ç­–è´¨é‡(DQ)ã€æ—¶é—´åˆ°å¯ç”¨ç†è§£(T2U)ã€æœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§ã€æ­£ç¡®æ€§",
    "evaluation_metrics_quote": "We introduce Decision Quality (DQ), a multi-dimensional metric capturing validity, specificity, and correctness",
    "input_modality": "äº‹ä»¶é¥æµ‹æ•°æ®å’Œè‡ªç„¶è¯­è¨€æè¿°",
    "input_modality_quote": "Given the following telemetry: Service: auth-service v2.4.0, Error rate: 45%...",
    "output_modality": "ç»“æ„åŒ–çš„äº‹ä»¶ç®€æŠ¥ï¼ŒåŒ…å«æ ¹æœ¬åŸå› ã€å»ºè®®è¡ŒåŠ¨å’Œé£é™©è¯„ä¼°",
    "output_modality_quote": "A coordinator aggregates the three agent outputs into a structured incident brief containing root cause, recommended actions, and risk assessment",
    "task_io_type": "äº‹ä»¶æ•°æ®åˆ°å†³ç­–å»ºè®®",
    "task_io_type_quote": "bridging the gap between detection and actionable comprehension",
    "execution_environment": "å®¹å™¨åŒ–å¾®æœåŠ¡æ¡†æ¶ï¼Œä½¿ç”¨Docker Composeç¼–æ’",
    "execution_environment_quote": "MyAntFarm.ai consists of five containerized microservices orchestrated via Docker Compose",
    "unique_features": "ä¸“æ³¨äºå¤šæ™ºèƒ½ä½“ç¼–æ’åœ¨äº‹ä»¶å“åº”ä¸­çš„ç¡®å®šæ€§è´¨é‡ä¼˜åŠ¿ï¼Œå¼•å…¥å†³ç­–è´¨é‡(DQ)è¿™ä¸€æ–°çš„è¯„ä¼°æŒ‡æ ‡",
    "unique_features_quote": "multi-agent systems exhibit zero quality variance across all trials, making them production-ready... We introduce Decision Quality (DQ), a multi-dimensional metric capturing validity, specificity, and correctness",
    "data_size_quantity": 348,
    "data_size_unit": "æ¬¡è¯•éªŒ",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['è‡ªç„¶è¯­è¨€']",
    "dimension_normalized": "['å†³ç­–è´¨é‡', 'è¡ŒåŠ¨ç‰¹å¼‚æ€§', 'è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§', 'ç”Ÿäº§å°±ç»ªæ€§']",
    "evaluation_method_normalized": "['å†³ç­–è´¨é‡(DQ)', 'æ—¶é—´åˆ°å¯ç”¨ç†è§£(T2U)', 'æœ‰æ•ˆæ€§', 'ç‰¹å¼‚æ€§', 'æ­£ç¡®æ€§']",
    "problem_domain_normalized": "['è¿ç»´æ™ºèƒ½(AIOps)', 'äº‹ä»¶å“åº”', 'ç”Ÿäº§ç¯å¢ƒæ•…éšœè¯Šæ–­']",
    "source_type_normalized": "['äººå·¥æ„å»ºçš„æ¨¡æ‹Ÿäº‹ä»¶åœºæ™¯']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  }
]