[
  {
    "source_paper": "2511.15757_output/content.md",
    "benchmark_name": "RGym",
    "benchmark_name_quote": "In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "Linuxå†…æ ¸è‡ªåŠ¨ç¨‹åºä¿®å¤è¯„ä¼°ï¼Œä¸“æ³¨äºå†…æ ¸ç©ºé—´è°ƒè¯•å’Œä¿®å¤çš„å¤æ‚æ€§",
    "task_description_quote": "Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on user-space applications and overlook the complexities of kernel-space debugging and repair.",
    "dimension": "Linuxå†…æ ¸ç¨‹åºä¿®å¤èƒ½åŠ›è¯„ä¼°ï¼ŒåŒ…æ‹¬å®šä½ã€è¡¥ä¸ç”Ÿæˆã€éªŒè¯å’Œæˆæœ¬/å»¶è¿Ÿè€ƒè™‘",
    "dimension_quote": "These characteristics make the kernel an ideal stress test for evaluating LLM-based APR, from localization to patch generation, validation, and cost/latency consideration.",
    "evaluation_method": "é€šè¿‡ç¼–è¯‘ä¿®è¡¥åçš„å†…æ ¸ã€è¿è¡Œæ¦‚å¿µéªŒè¯ç¨‹åºå¹¶æŠ¥å‘Šç»“æœæ¥è¯„ä¼°è¡¥ä¸è´¨é‡",
    "evaluation_method_quote": "RGym overall compiles patched kernels, runs PoCs, and reports results.",
    "context_dependency": "Linuxå†…æ ¸çº§åˆ«çš„å¤æ‚ä¾èµ–å…³ç³»ï¼ŒåŒ…æ‹¬å¤§è§„æ¨¡ã€æ·±åº¦ä¾èµ–ã€æ™®éå¹¶å‘å’Œç¡¬ä»¶åº•å±‚äº¤äº’",
    "context_dependency_quote": "with its massive scale, deep dependency, and pervasive concurrency and low-level interactions with hardware.",
    "problem_domain": "æ“ä½œç³»ç»Ÿå†…æ ¸å¼€å‘ï¼Œç‰¹åˆ«æ˜¯Linuxå†…æ ¸å†…å­˜å®‰å…¨æ¼æ´ä¿®å¤",
    "problem_domain_quote": "The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions.",
    "problem_difficulty": "é«˜éš¾åº¦å†…æ ¸çº§bugä¿®å¤ï¼ŒåŒ…å«å†…å­˜æŸåç­‰æœ€ä¸¥é‡ç±»å‹çš„bug",
    "problem_difficulty_quote": "filtering to KASAN bugs [13], which represent the most severe types of bugs (memory corruption)",
    "language": "Cè¯­è¨€ï¼ˆLinuxå†…æ ¸å¼€å‘ï¼‰",
    "language_quote": "From 6,088 Syzbot bugs, we retain those with fix commits, reproducers, crash reports, and kernel configs",
    "data_size": "åŒ…å«143ä¸ªå·²éªŒè¯çš„å†…æ ¸bugçš„æ•°æ®é›†",
    "data_size_quote": "We test on a filtered and verified dataset of 143 bugs.",
    "source_type": "æ¥è‡ªSyzkallerå†…æ ¸æ¨¡ç³Šæµ‹è¯•å™¨å’ŒSyzbotè‡ªåŠ¨å´©æºƒæŠ¥å‘Šç³»ç»Ÿçš„çœŸå®å†…æ ¸bug",
    "source_type_quote": "Syzkaller [6], a coverage-guided kernel fuzzer, together with Syzbot [5], an automated online crash reporting system developed by Google, provides a valuable ecosystem that makes kernel-bug collection possible",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.15757v1 [cs.SE] 19 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºçœŸå®å†…æ ¸bugæ„å»º",
    "build_type_quote": "We organize a dataset of 143 kernel bugs from Syzbot into an easily consumable format and verified the reproducibility of the bug on the patch parent.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ï¼Œç‰¹åˆ«æ˜¯å†…æ ¸å†…å­˜å®‰å…¨æ¼æ´ä¿®å¤",
    "task_granularity_quote": "automated program repair (APR) in the Linux kernel space",
    "evaluation_metrics": "è¡¥ä¸é€šè¿‡ç‡ï¼ˆpass rateï¼‰ã€é”™è¯¯è¡¥ä¸ç‡ï¼ˆbad patch rateï¼‰ã€æ¯bugå¹³å‡æˆæœ¬",
    "evaluation_metrics_quote": "Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug.",
    "input_modality": "å´©æºƒæŠ¥å‘Šã€è°ƒç”¨æ ˆã€bugè¯±å¯¼æäº¤ã€å†…æ ¸é…ç½®",
    "input_modality_quote": "inputs (patch, commit, source, config, compiler, cores, timeout, metadata)",
    "output_modality": "å†…æ ¸è¡¥ä¸ä»£ç ",
    "output_modality_quote": "The LLM lists candidate functions, receives their definitions, and returns their patched definitions.",
    "task_io_type": "å´©æºƒä¿¡æ¯åˆ°å†…æ ¸è¡¥ä¸ä»£ç ",
    "task_io_type_quote": "Our APR generates a patch via the Simple Agent or Function Exploration Agent and tests it with RGym.",
    "execution_environment": "ä½¿ç”¨dockeræ†ç»‘ä½œä¸šä¾èµ–å’ŒQEMUè™šæ‹Ÿæœºï¼Œåœ¨æœ¬åœ°ç¡¬ä»¶ä¸Šè¿è¡Œ",
    "execution_environment_quote": "RGym runs locally using docker to bundle job dependencies and QEMU for VMs.",
    "unique_features": "è½»é‡çº§ã€å¹³å°æ— å…³çš„Linuxå†…æ ¸APRè¯„ä¼°æ¡†æ¶ï¼Œä¸“æ³¨äºæœ¬åœ°å•†å“ç¡¬ä»¶è¿è¡Œï¼Œè§£å†³äº†kGymå¯¹GCPçš„ç¡¬ä¾èµ–é—®é¢˜",
    "unique_features_quote": "we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. RGym solves the compiler and dependency problem by smartly switching build dependencies using docker images depending on the kernel version or compiler string provided in the kernel configuration.",
    "data_size_quantity": 143,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Cè¯­è¨€']",
    "dimension_normalized": "['Linuxå†…æ ¸ç¨‹åºä¿®å¤èƒ½åŠ›è¯„ä¼°', 'å®šä½', 'è¡¥ä¸ç”Ÿæˆ', 'éªŒè¯', 'æˆæœ¬/å»¶è¿Ÿè€ƒè™‘']",
    "evaluation_method_normalized": "['è¡¥ä¸é€šè¿‡ç‡', 'é”™è¯¯è¡¥ä¸ç‡', 'æ¯bugå¹³å‡æˆæœ¬']",
    "problem_domain_normalized": "['æ“ä½œç³»ç»Ÿå†…æ ¸å¼€å‘', 'Linuxå†…æ ¸å†…å­˜å®‰å…¨æ¼æ´ä¿®å¤']",
    "source_type_normalized": "['Syzkallerå†…æ ¸æ¨¡ç³Šæµ‹è¯•å™¨', 'Syzbotè‡ªåŠ¨å´©æºƒæŠ¥å‘Šç³»ç»Ÿ', 'çœŸå®å†…æ ¸bug']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.16005_output/content.md",
    "benchmark_name": "MultiSWE-bench-CPP",
    "benchmark_name_quote": "Evaluated on the MultiSWE-bench-CPP benchmark, InfCode-C++ achieves a resolution rate of 25.58%",
    "dataset_url": "https://github.com/Tokfinity/InfCode",
    "dataset_url_quote": "To support the research community and facilitate future work, we release InfCode-C++ and our evaluation benchmark as an open-source project at https://github.com/Tokfinity/InfCode",
    "task_description": "C++è½¯ä»¶é—®é¢˜è§£å†³ï¼Œä»è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ç”Ÿæˆä¿®å¤è¡¥ä¸",
    "task_description_quote": "Given a software repository ğ¶ and a natural-language issue description ğ·, the objective is to synthesize a patch ğ‘ such that the updated repository ğ¶â€² = ğ¶ âŠ• ğ‘ satisfies the behavioral requirements expressed in ğ· while preserving the original functionality",
    "dimension": "ä»£ç ä¿®å¤èƒ½åŠ›ã€ä¸Šä¸‹æ–‡æ£€ç´¢å‡†ç¡®æ€§ã€ç»“æ„åˆ†æèƒ½åŠ›",
    "dimension_quote": "Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution",
    "evaluation_method": "é€šè¿‡å›å½’æµ‹è¯•å¥—ä»¶éªŒè¯è¡¥ä¸æ­£ç¡®æ€§ï¼Œä½¿ç”¨è§£å†³ç‡ä½œä¸ºè¯„ä¼°æŒ‡æ ‡",
    "evaluation_method_quote": "A correct patch must satisfy: ğ‘¡(ğ¶â€²) = ğ‘¡(ğ¶), âˆ€ğ‘¡âˆˆğ‘‡ and âˆƒğ‘¡ğ·: ğ‘¡ğ·(ğ¶â€²) = PASS, where ğ‘¡ğ· is a failing test case that captures the erroneous behavior described in ğ·",
    "context_dependency": "ä»“åº“çº§åˆ«ï¼Œæ¶‰åŠå¤šæ–‡ä»¶C++é¡¹ç›®",
    "context_dependency_quote": "repository-level issue resolution... in large, statically typed C++ repositories",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼ŒC++ç¨‹åºä¿®å¤",
    "problem_domain_quote": "Automated software issue resolution stands as a critical challenge in software engineering... resolving issues in large-scale, high-performance systems written in C++",
    "problem_difficulty": "çœŸå®ä¸–ç•Œå·¥ç¨‹çº§éš¾åº¦",
    "problem_difficulty_quote": "thousands of real-world GitHub issues... a formidable, unsolved challenge",
    "language": "C++",
    "language_quote": "specifically designed for C++ projects... the first C++-aware autonomous system for end-to-end issue resolution",
    "data_size": "MultiSWE-benchçš„C++å­é›†",
    "data_size_quote": "Evaluated on the MultiSWE-bench-CPP benchmark... on the C++ subset of MultiSWE-bench",
    "source_type": "çœŸå®ä¸–ç•Œçš„GitHubé—®é¢˜",
    "source_type_quote": "thousands of real-world GitHub issues",
    "last_updated": 2025,
    "last_updated_quote": "2025. InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution. 1, 1 (November 2025)",
    "build_type": "åŸºäºç°æœ‰åŸºå‡†æ„å»ºçš„å­é›†",
    "build_type_quote": "the C++ subset of MultiSWE-bench",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤",
    "task_granularity_quote": "issue resolution... synthesize a patch",
    "evaluation_metrics": "è§£å†³ç‡",
    "evaluation_metrics_quote": "achieves a resolution rate of 25.58%",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°",
    "input_modality_quote": "natural-language issue description ğ·",
    "output_modality": "ä»£ç è¡¥ä¸",
    "output_modality_quote": "synthesize a patch ğ‘",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "from a natural language issue description... synthesize a patch",
    "execution_environment": "Dockerç¯å¢ƒ",
    "execution_environment_quote": "Repo Codebase Docker Env",
    "unique_features": "ä¸“é—¨é’ˆå¯¹C++è¯­è¨€çš„å¤æ‚æ€§è®¾è®¡ï¼ŒåŒ…å«è¯­ä¹‰æ£€ç´¢å’ŒASTç»“æ„åŒ–æŸ¥è¯¢",
    "unique_features_quote": "the first C++-aware autonomous system... combines two complementary retrieval mechanismsâ€”semantic code-intent retrieval and deterministic AST-structured queryingâ€”to construct accurate, language-aware context for repair",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['C++']",
    "dimension_normalized": "['ä»£ç ä¿®å¤èƒ½åŠ›', 'ä¸Šä¸‹æ–‡æ£€ç´¢å‡†ç¡®æ€§', 'ç»“æ„åˆ†æèƒ½åŠ›']",
    "evaluation_method_normalized": "['è§£å†³ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'C++ç¨‹åºä¿®å¤']",
    "source_type_normalized": "['çœŸå®ä¸–ç•Œçš„GitHubé—®é¢˜']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.17131_output/content.md",
    "benchmark_name": "UI-CUBE (UiPath Computer Use BEnchmark)",
    "benchmark_name_quote": "We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs.",
    "dataset_url": "https://github.com/UiPath/uipath_enterprise_benchmark",
    "dataset_url_quote": "GitHub: https://github.com/UiPath/uipath_enterprise_benchmark",
    "task_description": "è¯„ä¼°è®¡ç®—æœºä½¿ç”¨ä»£ç†åœ¨ä¼ä¸šç¯å¢ƒä¸­çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬ç®€å•çš„UIäº¤äº’å’Œå¤æ‚çš„å·¥ä½œæµç¨‹è‡ªåŠ¨åŒ–",
    "task_description_quote": "Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks)",
    "dimension": "åŠŸèƒ½æ­£ç¡®æ€§ã€æ“ä½œå¯é æ€§ã€ç•Œé¢é€‚åº”æ€§ã€å·¥ä½œæµåè°ƒèƒ½åŠ›",
    "dimension_quote": "measuring not only functional accuracy but also operational reliabilityâ€”providing clearer insight into how CUAs can move from experimental demos toward dependable enterprise-grade tools",
    "evaluation_method": "é€šè¿‡åº”ç”¨ç¨‹åºçŠ¶æ€çš„è‡ªåŠ¨åŒ–éªŒè¯æ¥è¯„ä¼°ä»»åŠ¡æˆåŠŸï¼ŒåŒ…æ‹¬å¤šåˆ†è¾¨ç‡æµ‹è¯•å’Œç³»ç»Ÿç•Œé¢å˜åŒ–è¦†ç›–",
    "evaluation_method_quote": "with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state",
    "context_dependency": "å¤šæ­¥éª¤å·¥ä½œæµç¨‹ã€è·¨åº”ç”¨ç¨‹åºåè°ƒ",
    "context_dependency_quote": "complex workflows including copy-paste tasks and enterprise application scenarios, multi-step processes where reliability is as critical as task completion",
    "problem_domain": "ä¼ä¸šè½¯ä»¶è‡ªåŠ¨åŒ–ã€UIäº¤äº’ã€å·¥ä½œæµç¨‹ç®¡ç†",
    "problem_domain_quote": "enterprise application scenarios, enterprise deployment readiness, enterprise workflow automation",
    "problem_difficulty": "ä¸¤ä¸ªéš¾åº¦å±‚çº§ï¼šç®€å•UIäº¤äº’å’Œå¤æ‚å·¥ä½œæµç¨‹",
    "problem_difficulty_quote": "comprising 226 tasks across two difficulty tiers: simple UI interactions (136 tasks) and complex workflows (90 tasks)",
    "language": "ä¸ç‰¹å®šäºç¼–ç¨‹è¯­è¨€ï¼Œä¸»è¦å…³æ³¨UIäº¤äº’",
    "language_quote": NaN,
    "data_size": "226ä¸ªä»»åŠ¡ï¼Œå…¶ä¸­136ä¸ªç®€å•UIäº¤äº’ä»»åŠ¡ï¼Œ90ä¸ªå¤æ‚å·¥ä½œæµç¨‹ä»»åŠ¡",
    "data_size_quote": "comprising 226 tasks across two difficulty tiers: simple UI interactions (136 tasks) and complex workflows (90 tasks comprising 50 copy-paste/business-process tasks and 40 enterprise application tasks)",
    "source_type": "åŸºäºä¼ä¸šåº”ç”¨åœºæ™¯çš„ç³»ç»Ÿæ„å»ºï¼ŒåŒ…å«æ¨¡æ‹Ÿçš„ä¼ä¸šç³»ç»Ÿå·¥ä½œæµç¨‹",
    "source_type_quote": "faithful mocks of complex enterprise workflows to test coordination and operational reliability, including testing with workflows from SAP, Workday, and other enterprise systems",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.17131v1 [cs.SE] 21 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we designed UI-CUBE, a new benchmark tailored to enterprise-like conditions, and used it to evaluate our own CUA implementation",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "UIäº¤äº’è‡ªåŠ¨åŒ–ã€å·¥ä½œæµç¨‹æ‰§è¡Œ",
    "task_granularity_quote": "systematic coverage of atomic UI interactions to map interface-level capabilities, and faithful mocks of complex enterprise workflows to test coordination and operational reliability",
    "evaluation_metrics": "ä»»åŠ¡æˆåŠŸç‡",
    "evaluation_metrics_quote": "Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%",
    "input_modality": "UIç•Œé¢ã€ä»»åŠ¡æŒ‡ä»¤",
    "input_modality_quote": "systematic interface variation coverage, multi-resolution testing",
    "output_modality": "UIäº¤äº’åŠ¨ä½œã€å·¥ä½œæµç¨‹æ‰§è¡Œç»“æœ",
    "output_modality_quote": "automated validation of task success through the application state",
    "task_io_type": "ä»»åŠ¡æŒ‡ä»¤åˆ°UIäº¤äº’",
    "task_io_type_quote": "measuring task completion effectively, they provide limited assessment of enterprise deployment readiness",
    "execution_environment": "ä¼ä¸šè½¯ä»¶ç¯å¢ƒã€å¤šåˆ†è¾¨ç‡æ˜¾ç¤º",
    "execution_environment_quote": "All tasks are evaluated across multiple screen resolutions to assess agent consistency under different display conditions",
    "unique_features": "ä¸“æ³¨äºä¼ä¸šçº§éƒ¨ç½²å‡†å¤‡åº¦è¯„ä¼°ï¼Œæ­ç¤ºèƒ½åŠ›æ‚¬å´–ç°è±¡è€Œéæ¸è¿›æ€§èƒ½ä¸‹é™ï¼Œæä¾›ç³»ç»Ÿç•Œé¢åˆ†ç±»è¦†ç›–",
    "unique_features_quote": "reveals a sharp capability cliff rather than gradual performance degradation, systematic interface variation coverage, multi-resolution testing",
    "data_size_quantity": 226,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['ä¸ç‰¹å®šäºç¼–ç¨‹è¯­è¨€', 'ä¸»è¦å…³æ³¨UIäº¤äº’']",
    "dimension_normalized": "['åŠŸèƒ½æ­£ç¡®æ€§', 'æ“ä½œå¯é æ€§', 'ç•Œé¢é€‚åº”æ€§', 'å·¥ä½œæµåè°ƒèƒ½åŠ›']",
    "evaluation_method_normalized": "['ä»»åŠ¡æˆåŠŸç‡']",
    "problem_domain_normalized": "['ä¼ä¸šè½¯ä»¶è‡ªåŠ¨åŒ–', 'UIäº¤äº’', 'å·¥ä½œæµç¨‹ç®¡ç†']",
    "source_type_normalized": "['åŸºäºä¼ä¸šåº”ç”¨åœºæ™¯çš„ç³»ç»Ÿæ„å»º', 'åŒ…å«æ¨¡æ‹Ÿçš„ä¼ä¸šç³»ç»Ÿå·¥ä½œæµç¨‹']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.15755_output/content.md",
    "benchmark_name": "MyAntFarm.ai",
    "benchmark_name_quote": "To test this hypothesis, we present MyAntFarm.ai, a reproducible experimental framework enabling controlled comparison of three conditions",
    "dataset_url": "https://github.com/Phildram1/myantfarm-ai",
    "dataset_url_quote": "Source code, Docker configurations, and trial outputs are available at: https://github.com/Phildram1/myantfarm-ai",
    "task_description": "è¯„ä¼°å¤šæ™ºèƒ½ä½“LLMç¼–æ’åœ¨äº‹ä»¶å“åº”ä¸­çš„å†³ç­–æ”¯æŒè´¨é‡ï¼Œé€šè¿‡æ¨¡æ‹Ÿè®¤è¯æœåŠ¡å›å½’äº‹ä»¶æ¥æµ‹è¯•ç³»ç»Ÿç”Ÿæˆå¯æ‰§è¡Œå»ºè®®çš„èƒ½åŠ›",
    "task_description_quote": "multi-agent orchestration fundamentally transforms LLM-based incident response quality... achieve 100% actionable recommendation rate",
    "dimension": "å†³ç­–è´¨é‡ã€è¡ŒåŠ¨ç‰¹å¼‚æ€§ã€è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§ã€ç”Ÿäº§å°±ç»ªæ€§",
    "dimension_quote": "We introduce Decision Quality (DQ), a multi-dimensional metric capturing validity, specificity, and correctness",
    "evaluation_method": "ä½¿ç”¨å†³ç­–è´¨é‡(DQ)æŒ‡æ ‡ï¼Œç»“åˆæœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§å’Œæ­£ç¡®æ€§ä¸‰ä¸ªç»´åº¦è¿›è¡Œè‡ªåŠ¨åŒ–è¯„åˆ†",
    "evaluation_method_quote": "DQ measures actionability through three dimensions... Validity, Specificity, Correctness",
    "context_dependency": "å•äº‹ä»¶åœºæ™¯ï¼ŒåŸºäºç‰¹å®šçš„äº‹ä»¶é¥æµ‹æ•°æ®",
    "context_dependency_quote": "All 348 trials used identical context to isolate orchestration effects from scenario variability",
    "problem_domain": "è¿ç»´æ™ºèƒ½(AIOps)ã€äº‹ä»¶å“åº”ã€ç”Ÿäº§ç¯å¢ƒæ•…éšœè¯Šæ–­",
    "problem_domain_quote": "Modern operational teams face a critical gap between incident detection and actionable comprehension... incident response",
    "problem_difficulty": "ç”Ÿäº§çº§äº‹ä»¶å“åº”ï¼Œéœ€è¦å…·ä½“çš„å¯æ‰§è¡Œå‘½ä»¤",
    "problem_difficulty_quote": "time-critical operational contexts... production deployment",
    "language": "ä¸»è¦ä½¿ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œäº‹ä»¶æè¿°å’Œå“åº”ç”Ÿæˆ",
    "language_quote": "The LLM generates a single unstructured text response attempting to address all objectives",
    "data_size": "348æ¬¡è¯•éªŒï¼Œæ¯ä¸ªæ¡ä»¶116æ¬¡è¯•éªŒ",
    "data_size_quote": "Through 348 controlled trials... 116 trials per condition",
    "source_type": "äººå·¥æ„å»ºçš„æ¨¡æ‹Ÿäº‹ä»¶åœºæ™¯",
    "source_type_quote": "All 348 trials used identical context... Authentication service regression post-deployment",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.15755v1 [cs.AI] 19 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºçš„ç ”ç©¶æ¡†æ¶",
    "build_type_quote": "we present MyAntFarm.ai, a reproducible experimental framework",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "äº‹ä»¶å“åº”å†³ç­–æ”¯æŒï¼ŒåŒ…æ‹¬è¯Šæ–­ã€è§„åˆ’å’Œé£é™©è¯„ä¼°",
    "task_granularity_quote": "coordinating specialized LLM agents for diagnosis, planning, and risk assessment",
    "evaluation_metrics": "å†³ç­–è´¨é‡(DQ)ã€æ—¶é—´åˆ°å¯ç”¨ç†è§£(T2U)ã€æœ‰æ•ˆæ€§ã€ç‰¹å¼‚æ€§ã€æ­£ç¡®æ€§",
    "evaluation_metrics_quote": "We introduce Decision Quality (DQ), a multi-dimensional metric capturing validity, specificity, and correctness",
    "input_modality": "äº‹ä»¶é¥æµ‹æ•°æ®å’Œè‡ªç„¶è¯­è¨€æè¿°",
    "input_modality_quote": "Given the following telemetry: Service: auth-service v2.4.0, Error rate: 45%...",
    "output_modality": "ç»“æ„åŒ–çš„äº‹ä»¶ç®€æŠ¥ï¼ŒåŒ…å«æ ¹æœ¬åŸå› ã€å»ºè®®è¡ŒåŠ¨å’Œé£é™©è¯„ä¼°",
    "output_modality_quote": "A coordinator aggregates the three agent outputs into a structured incident brief containing root cause, recommended actions, and risk assessment",
    "task_io_type": "äº‹ä»¶æ•°æ®åˆ°å†³ç­–å»ºè®®",
    "task_io_type_quote": "bridging the gap between detection and actionable comprehension",
    "execution_environment": "å®¹å™¨åŒ–å¾®æœåŠ¡æ¡†æ¶ï¼Œä½¿ç”¨Docker Composeç¼–æ’",
    "execution_environment_quote": "MyAntFarm.ai consists of five containerized microservices orchestrated via Docker Compose",
    "unique_features": "ä¸“æ³¨äºå¤šæ™ºèƒ½ä½“ç¼–æ’åœ¨äº‹ä»¶å“åº”ä¸­çš„ç¡®å®šæ€§è´¨é‡ä¼˜åŠ¿ï¼Œå¼•å…¥å†³ç­–è´¨é‡(DQ)è¿™ä¸€æ–°çš„è¯„ä¼°æŒ‡æ ‡",
    "unique_features_quote": "multi-agent systems exhibit zero quality variance across all trials, making them production-ready... We introduce Decision Quality (DQ), a multi-dimensional metric capturing validity, specificity, and correctness",
    "data_size_quantity": 348,
    "data_size_unit": "æ¬¡è¯•éªŒ",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['è‡ªç„¶è¯­è¨€']",
    "dimension_normalized": "['å†³ç­–è´¨é‡', 'è¡ŒåŠ¨ç‰¹å¼‚æ€§', 'è§£å†³æ–¹æ¡ˆæ­£ç¡®æ€§', 'ç”Ÿäº§å°±ç»ªæ€§']",
    "evaluation_method_normalized": "['å†³ç­–è´¨é‡(DQ)', 'æ—¶é—´åˆ°å¯ç”¨ç†è§£(T2U)', 'æœ‰æ•ˆæ€§', 'ç‰¹å¼‚æ€§', 'æ­£ç¡®æ€§']",
    "problem_domain_normalized": "['è¿ç»´æ™ºèƒ½(AIOps)', 'äº‹ä»¶å“åº”', 'ç”Ÿäº§ç¯å¢ƒæ•…éšœè¯Šæ–­']",
    "source_type_normalized": "['äººå·¥æ„å»ºçš„æ¨¡æ‹Ÿäº‹ä»¶åœºæ™¯']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.16708_output/content.md",
    "benchmark_name": "CodeX-Verify",
    "benchmark_name_quote": "We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "å¤šæ™ºèƒ½ä½“ä»£ç éªŒè¯ç³»ç»Ÿï¼Œä½¿ç”¨å››ä¸ªä¸“é—¨åŒ–çš„æ™ºèƒ½ä½“æ¥æ£€æµ‹ä¸åŒç±»å‹çš„ä»£ç é”™è¯¯",
    "task_description_quote": "We built CodeX-Verify, a system that runs four specialized agents in parallel: Correctness (logic errors, edge cases, exception handling), Security (OWASP Top 10, CWE patterns, secrets), Performance (algorithmic complexity, resource leaks), and Style (maintainability, documentation). Each agent looks for different bug types.",
    "dimension": "ä»£ç æ­£ç¡®æ€§ã€å®‰å…¨æ€§ã€æ€§èƒ½å’Œé£æ ¼å››ä¸ªç»´åº¦çš„é”™è¯¯æ£€æµ‹",
    "dimension_quote": "Correctness (logic errors, edge cases, exception handling), Security (OWASP Top 10, CWE patterns, secrets), Performance (algorithmic complexity, resource leaks), and Style (maintainability, documentation)",
    "evaluation_method": "åœ¨99ä¸ªå¸¦æœ‰éªŒè¯æ ‡ç­¾çš„ä»£ç æ ·æœ¬ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæµ‹é‡çœŸé˜³æ€§ç‡å’Œå‡†ç¡®ç‡",
    "evaluation_method_quote": "Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€ä»£ç éªŒè¯ã€æ¼æ´æ£€æµ‹",
    "problem_domain_quote": "Multi-Agent Code Verification with Compound Vulnerability Detection",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": "99ä¸ªä»£ç æ ·æœ¬ï¼Œè¦†ç›–16ä¸ªé”™è¯¯ç±»åˆ«",
    "data_size_quote": "Dataset of 99 code samples with verified labels covering 16 bug categories",
    "source_type": "æ¥è‡ªçœŸå®SWE-benchå¤±è´¥çš„ä»£ç æ ·æœ¬",
    "source_type_quote": "covering 16 bug categories from real SWE-bench failures",
    "last_updated": 2025,
    "last_updated_quote": "October 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We built CodeX-Verify",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "å¼€æºå‘å¸ƒ",
    "dataset_license_quote": "released open-source",
    "task_granularity": "ä»£ç éªŒè¯å’Œé”™è¯¯æ£€æµ‹",
    "task_granularity_quote": "detect different types of bugs",
    "evaluation_metrics": "çœŸé˜³æ€§ç‡ã€å‡†ç¡®ç‡ã€å‡é˜³æ€§ç‡",
    "evaluation_metrics_quote": "catches 76.1% of bugs, achieving 76.1% TPR with 68.7% accuracy (Â±9.1% CI)",
    "input_modality": "ä»£ç ",
    "input_modality_quote": "code samples",
    "output_modality": "é”™è¯¯æ£€æµ‹ç»“æœ",
    "output_modality_quote": "detect different types of bugs",
    "task_io_type": "ä»£ç åˆ°é”™è¯¯æ£€æµ‹ç»“æœ",
    "task_io_type_quote": "analyzes code c âˆˆC through domain-specific function Ï•i : C â†’Oi, producing observation Ai = Ï•i(c) and decision Di âˆˆ{0, 1}",
    "execution_environment": "æ— éœ€æ‰§è¡Œä»£ç çš„é™æ€åˆ†æ",
    "execution_environment_quote": "running faster and without executing code",
    "unique_features": "å¤šæ™ºèƒ½ä½“ååŒéªŒè¯ã€å¤åˆæ¼æ´é£é™©å»ºæ¨¡ã€æ•°å­¦ç†è®ºè¯æ˜ã€æµ‹è¯•15ç§æ™ºèƒ½ä½“ç»„åˆ",
    "unique_features_quote": "We tested all 15 combinations of agents: single agents (4 configs), pairs (6 configs), triples (4 configs), and the full system. We formalize how multiple vulnerabilities in the same code create exponentially more risk. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent.",
    "data_size_quantity": 99,
    "data_size_unit": "ä¸ªä»£ç æ ·æœ¬",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç æ­£ç¡®æ€§', 'å®‰å…¨æ€§', 'æ€§èƒ½', 'é£æ ¼']",
    "evaluation_method_normalized": "['çœŸé˜³æ€§ç‡', 'å‡†ç¡®ç‡', 'å‡é˜³æ€§ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç éªŒè¯', 'æ¼æ´æ£€æµ‹']",
    "source_type_normalized": "['æ¥è‡ªçœŸå®SWE-benchå¤±è´¥çš„ä»£ç æ ·æœ¬']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.21509_output/content.md",
    "benchmark_name": "SV-LIB 1.0",
    "benchmark_name_quote": "SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks",
    "dataset_url": "https://gitlab.com/sosy-lab/benchmarking/sv-lib",
    "dataset_url_quote": "https://gitlab.com/sosy-lab/benchmarking/sv-lib",
    "task_description": "è½¯ä»¶éªŒè¯ä»»åŠ¡çš„äº¤æ¢æ ¼å¼å’Œä¸­é—´è¯­è¨€ï¼ŒåŒ…æ‹¬ç¨‹åºã€è§„èŒƒå’ŒéªŒè¯è§è¯",
    "task_description_quote": "we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses.",
    "dimension": "è½¯ä»¶éªŒè¯ã€ç¨‹åºåˆ†æã€äº¤æ¢æ ¼å¼ã€éªŒè¯è§è¯",
    "dimension_quote": "Additional Key Words and Phrases: Software verification, Program analysis, Exchange format, Witness, Certifying Algorithm, Intermediate language, SV-LIB, SMT-LIB",
    "evaluation_method": NaN,
    "evaluation_method_quote": NaN,
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶éªŒè¯ã€ç¨‹åºåˆ†æ",
    "problem_domain_quote": "Additional Key Words and Phrases: Software verification, Program analysis, Exchange format, Witness, Certifying Algorithm, Intermediate language, SV-LIB, SMT-LIB",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "åŸºäºå‘½ä»¤å¼ç¼–ç¨‹è¯­è¨€æ¦‚å¿µï¼Œä½¿ç”¨SMT-LIBè¡¨ç¤ºè¡¨è¾¾å¼å’Œç±»å‹",
    "language_quote": "SV-LIB is based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2025-11-26",
    "last_updated_quote": "Version 1.0 SV-LIB 2025-11-26",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we propose SV-LIB, an exchange format and intermediate language for software-verification tasks",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "è½¯ä»¶éªŒè¯ä»»åŠ¡äº¤æ¢æ ¼å¼",
    "task_granularity_quote": "SV-LIB, an exchange format and intermediate language for software-verification tasks",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "ç¨‹åºã€è§„èŒƒå’ŒéªŒè¯è§è¯",
    "input_modality_quote": "including programs, specifications, and verification witnesses",
    "output_modality": "éªŒè¯ç»“æœå’Œè§è¯",
    "output_modality_quote": "SV-LIB defines a witness format for both correct and incorrect SV-LIB programs",
    "task_io_type": "è½¯ä»¶éªŒè¯ä»»åŠ¡åˆ°éªŒè¯ç»“æœ",
    "task_io_type_quote": "exchange format and intermediate language for software-verification tasks",
    "execution_environment": "åŸºäºSMTæ±‚è§£å™¨çš„éªŒè¯å·¥å…·åŸºç¡€è®¾æ–½",
    "execution_environment_quote": "This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already.",
    "unique_features": "å®šä¹‰äº†æ­£ç¡®å’Œé”™è¯¯ç¨‹åºçš„è§è¯æ ¼å¼ï¼Œæ”¯æŒç‹¬ç«‹è§è¯éªŒè¯å™¨ï¼Œå¯é‡ç”¨éªŒè¯å™¨ä½œä¸ºè§è¯éªŒè¯å™¨",
    "unique_features_quote": "Furthermore, SV-LIB defines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 11,
    "last_updated_day": 26,
    "language_normalized": "['SMT-LIB']",
    "dimension_normalized": "['è½¯ä»¶éªŒè¯', 'ç¨‹åºåˆ†æ', 'äº¤æ¢æ ¼å¼', 'éªŒè¯è§è¯']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['è½¯ä»¶éªŒè¯', 'ç¨‹åºåˆ†æ']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.15817_output/content.md",
    "benchmark_name": "CodeSmellEval",
    "benchmark_name_quote": "The CodeSmellEval benchmark [37] proposed the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating specific types of smell.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆä»£ç çš„ç»“æ„è´¨é‡ï¼Œç‰¹åˆ«æ˜¯å…¶äº§ç”Ÿä»£ç å¼‚å‘³ï¼ˆå³å½±å“å¯è¯»æ€§ã€å¯ç»´æŠ¤æ€§æˆ–è®¾è®¡å®Œæ•´æ€§çš„æ¨¡å¼ï¼‰çš„å€¾å‘æ€§ã€‚",
    "task_description_quote": "This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality.",
    "dimension": "ä»£ç ç»“æ„è´¨é‡ï¼Œå…·ä½“ä¸ºä»£ç å¼‚å‘³çš„å€¾å‘æ€§ã€‚",
    "dimension_quote": "We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality.",
    "evaluation_method": "ä½¿ç”¨å€¾å‘æ€§å¼‚å‘³åˆ†æ•°ï¼ˆPSCï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ¨¡å‹åœ¨è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­é¢„æµ‹çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œæ¦‚ç‡çš„æ¦‚ç‡åº¦é‡ã€‚",
    "evaluation_method_quote": "The Propensity Smelly Score (PSC) is a probabilistic metric that estimates the likelihood that a LLM generates a specific type of code smell [37]. It relies on the next-token probabilities predicted by the model during autoregressive generation.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œä»£ç è´¨é‡è¯„ä¼°ã€‚",
    "problem_domain_quote": "Code smells offer a foundation for evaluating these broader quality concerns. They capture recurring design and implementation issues that affect readability, complexity, and long-term maintainability [8, 23, 36].",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Pythonï¼ˆä»ç¤ºä¾‹ä»£ç ç‰‡æ®µæ¨æ–­ï¼‰",
    "language_quote": "Figure 1: Propensity Smelly Score (SCM) Computation. The python snippet at the bottom contains two code smells: C0103 (i.e., invalid-name) and C0415 (i.e., import-outside-toplevel).",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2026",
    "last_updated_quote": "In 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE â€™26), April 12â€“18, 2026, Rio de Janeiro, Brazil.",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆåŸºäºå…ˆå‰å·¥ä½œï¼‰",
    "build_type_quote": "The CodeSmellEval benchmark [37] proposed the Propensity Smelly Score (PSC)...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç¦æ­¢æ¼”ç» 4.0 å›½é™…è®¸å¯åè®®",
    "dataset_license_quote": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "LLMs have rapidly moved from experimental tools to everyday assistants in software engineering (SE) [38]. Developers rely on them for many tasks such as code completion [5, 16, 41], summarization [1], program repair [11], clone detection [40] and test generation [39].",
    "evaluation_metrics": "å€¾å‘æ€§å¼‚å‘³åˆ†æ•°ï¼ˆPSCï¼‰",
    "evaluation_metrics_quote": "We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types...",
    "input_modality": NaN,
    "input_modality_quote": NaN,
    "output_modality": "ä»£ç ",
    "output_modality_quote": "This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code.",
    "task_io_type": NaN,
    "task_io_type_quote": NaN,
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "è¯¥åŸºå‡†ä¸“æ³¨äºä»£ç å¼‚å‘³ï¼ˆç»“æ„è´¨é‡é—®é¢˜ï¼‰çš„å€¾å‘æ€§è¯„ä¼°ï¼Œè€Œéä¼ ç»Ÿçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚å®ƒå¼•å…¥äº†ä¸€ä¸ªæ¦‚ç‡æ€§åº¦é‡ï¼ˆPSCï¼‰ï¼Œç”¨äºä¼°è®¡æ¨¡å‹ç”Ÿæˆç‰¹å®šç±»å‹å¼‚å‘³çš„å¯èƒ½æ€§ï¼Œå¹¶å¯ä½œä¸ºå› æœåˆ†æçš„å·¥å…·æ¥ç†è§£ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å½±å“å› ç´ ã€‚",
    "unique_features_quote": "Instead of treating PSC as a standalone benchmark metric, we use it as an instrument for reasoning about the factors that influence smell propensity in LLM-generated code. Our study begins by assessing the robustness and explanatory value of PSC, establishing that it provides a stable foundation for causal analysis.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç»“æ„è´¨é‡', 'ä»£ç å¼‚å‘³çš„å€¾å‘æ€§']",
    "evaluation_method_normalized": "['å€¾å‘æ€§å¼‚å‘³åˆ†æ•°ï¼ˆPSCï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç è´¨é‡è¯„ä¼°']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "CC-BY-SA 4.0",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.03421_output/content.md",
    "benchmark_name": "BugT",
    "benchmark_name_quote": "This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",
    "dataset_url": "https://github.com/Xucranger/PLofLBFL",
    "dataset_url_quote": "To facilitate future study, we share our source code and experimental data in a GitHub repository 1. 1https://github.com/Xucranger/PLofLBFL",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆå­¦è€…ç¨‹åºæ•…éšœå®šä½ä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚",
    "task_description_quote": "This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets.",
    "dimension": "æ•…éšœå®šä½çš„å‡†ç¡®æ€§ã€æ•ˆç‡ã€å¯ç”¨æ€§ï¼Œä»¥åŠå¯¹é—®é¢˜éš¾åº¦çš„é²æ£’æ€§ã€‚",
    "dimension_quote": "To investigate these issues, we aim to empirically assess the performance of different LLMs across various datasets, evaluating their accuracy, efficiency, and usability in fault localization for novice programs.",
    "evaluation_method": "åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°LLMsçš„æ•…éšœå®šä½æ€§èƒ½ï¼Œå¹¶ä¸ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚SBFLã€MBFLï¼‰è¿›è¡Œæ¯”è¾ƒã€‚",
    "evaluation_method_quote": "This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets. All closed-source LLMs outperform traditional SBFL and MBFL methods...",
    "context_dependency": "é’ˆå¯¹åŒ…å«é”™è¯¯çš„åˆå­¦è€…ç¨‹åºä»£ç ç‰‡æ®µã€‚",
    "context_dependency_quote": "Novice Program [18] refers to code written by novice programmers, usually in the early stages of learning programming. These programs usually contain multiple errors...",
    "problem_domain": "ç¼–ç¨‹æ•™è‚²ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åˆå­¦è€…çš„æ•…éšœå®šä½å’Œè°ƒè¯•ã€‚",
    "problem_domain_quote": "Novice programming assistance [19] is a crucial educational approach aimed at helping students develop their fundamental coding skills and enhance problem-solving abilities.",
    "problem_difficulty": "åŒ…å«ä¸åŒéš¾åº¦çº§åˆ«çš„é—®é¢˜ï¼Œä»ç®€å•åˆ°å¤æ‚ã€‚",
    "problem_difficulty_quote": "LLM accuracy decreases as problem difficulty increases in Codeflaws and Condefects, but top models maintain high accuracy even at peak difficulty in BugT, suggesting its lower complexity.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "è‡ªå»ºçš„åŒ…å«çœŸå®ç¼–ç¨‹é”™è¯¯çš„æ•°æ®é›†ã€‚",
    "source_type_quote": "We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMsâ€™ capabilities.",
    "last_updated": "2025",
    "last_updated_quote": "Preprint submitted to Journal of LATEX Templates December 4, 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We introduce a new self-created dataset with real programming faults...",
    "contamination_status": "ä¸“é—¨è®¾è®¡ä»¥å‡è½»æ•°æ®æ³„éœ²é—®é¢˜ã€‚",
    "contamination_status_quote": "...with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ•…éšœå®šä½",
    "task_granularity_quote": "This study evaluates the fault localization performance...",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "åŒ…å«é”™è¯¯çš„ä»£ç ",
    "input_modality_quote": "Novice Program [18] refers to code written by novice programmers, usually in the early stages of learning programming. These programs usually contain multiple errors...",
    "output_modality": "æ•…éšœä½ç½®å’Œè§£é‡Š",
    "output_modality_quote": "LLM generated fault explanations demonstrate significant value for novice programmer assistance...",
    "task_io_type": "ä»£ç åˆ°æ–‡æœ¬ï¼ˆæ•…éšœä½ç½®/è§£é‡Šï¼‰",
    "task_io_type_quote": "LLM generated fault explanations demonstrate significant value for novice programmer assistance...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“é—¨ä¸ºåˆå­¦è€…ç¼–ç¨‹æ•™è‚²è®¾è®¡ï¼Œæ—¨åœ¨å‡è½»æ•°æ®æ³„éœ²é—®é¢˜ï¼Œå¹¶åŒ…å«çœŸå®ç¼–ç¨‹é”™è¯¯ã€‚",
    "unique_features_quote": "...with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ...We introduce a new self-created dataset with real programming faults...",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['æ•…éšœå®šä½çš„å‡†ç¡®æ€§', 'æ•ˆç‡', 'å¯ç”¨æ€§', 'å¯¹é—®é¢˜éš¾åº¦çš„é²æ£’æ€§']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['ç¼–ç¨‹æ•™è‚²', 'é’ˆå¯¹åˆå­¦è€…çš„æ•…éšœå®šä½å’Œè°ƒè¯•']",
    "source_type_normalized": "['è‡ªå»ºçš„åŒ…å«çœŸå®ç¼–ç¨‹é”™è¯¯çš„æ•°æ®é›†']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "ä»£ç åˆ°æ–‡æœ¬",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.05073_output/content.md",
    "benchmark_name": "Comprehensive Verilog Design Problems (CVDP)",
    "benchmark_name_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€è§„èŒƒç”Ÿæˆç»è¿‡éªŒè¯çš„å¯„å­˜å™¨ä¼ è¾“çº§ï¼ˆRTLï¼‰ç¡¬ä»¶è®¾è®¡å®ç°ã€‚",
    "task_description_quote": "transform design intent from the CVDP dataset into verified register transfer level (RTL) implementations.",
    "dimension": "ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆå’Œä»£ç ç†è§£ã€‚",
    "dimension_quote": "We evaluate SLM models (1.7Bâ€“20B) across code generation and comprehension, establishing baseline performance.",
    "evaluation_method": "ä½¿ç”¨åŸºäºCocoTBçš„æµ‹è¯•å¥—ä»¶è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "evaluation_method_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "context_dependency": "å•æ¨¡å—è®¾è®¡é—®é¢˜ï¼ŒåŒ…å«æ¥å£è§„èŒƒå’ŒåŠŸèƒ½è¦æ±‚ã€‚",
    "context_dependency_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "problem_domain": "ç¡¬ä»¶è®¾è®¡ï¼Œå…·ä½“ä¸ºVerilogç¡¬ä»¶æè¿°è¯­è¨€ç¼–ç¨‹ã€‚",
    "problem_domain_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "problem_difficulty": "ä»£è¡¨ç°å®ä¸–ç•Œå¤æ‚æ€§çš„ç”Ÿäº§çº§IPå—ã€‚",
    "problem_difficulty_quote": "Derived from production IP blocks, it represents realistic complexity.",
    "language": "Verilogç¡¬ä»¶æè¿°è¯­è¨€ã€‚",
    "language_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "data_size": "åŒ…å«336ä¸ªé—®é¢˜ã€‚",
    "data_size_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "source_type": "æºè‡ªNVIDIAçš„ç”Ÿäº§IPå—ã€‚",
    "source_type_quote": "Derived from production IP blocks, it represents realistic complexity.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±NVIDIAå¼€å‘ï¼‰ã€‚",
    "build_type_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è§„èŒƒç”ŸæˆRTLä»£ç ï¼‰ã€‚",
    "task_granularity_quote": "transform design intent from the CVDP dataset into verified register transfer level (RTL) implementations.",
    "evaluation_metrics": "é€šè¿‡ç‡ï¼ˆpass rateï¼‰ã€‚",
    "evaluation_metrics_quote": "State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot), highlighting substantial improvement opportunity.",
    "input_modality": "è‡ªç„¶è¯­è¨€è§„èŒƒã€æ¨¡å—æ¥å£å’ŒåŠŸèƒ½è¦æ±‚ã€‚",
    "input_modality_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "output_modality": "Verilog RTLä»£ç ã€‚",
    "output_modality_quote": "transform design intent from the CVDP dataset into verified register transfer level (RTL) implementations.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ã€‚",
    "task_io_type_quote": "transform design intent from the CVDP dataset into verified register transfer level (RTL) implementations.",
    "execution_environment": "åŸºäºCocoTBçš„æµ‹è¯•å¥—ä»¶ã€‚",
    "execution_environment_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "unique_features": "ä¸“æ³¨äºç¡¬ä»¶è®¾è®¡ï¼ˆVerilogï¼‰çš„åŸºå‡†ï¼Œé—®é¢˜æºè‡ªç°å®ä¸–ç•Œçš„ç”Ÿäº§IPå—ï¼Œæ¶µç›–ç®—æœ¯è¿ç®—ã€æ§åˆ¶é€»è¾‘ã€å†…å­˜ç³»ç»Ÿç­‰å¤šç§è®¾è®¡ç±»åˆ«ã€‚",
    "unique_features_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs. Derived from production IP blocks, it represents realistic complexity.",
    "data_size_quantity": 336,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Verilogç¡¬ä»¶æè¿°è¯­è¨€']",
    "dimension_normalized": "['ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–èƒ½åŠ›', 'ä»£ç ç”Ÿæˆ', 'ä»£ç ç†è§£']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡ï¼ˆpass rateï¼‰']",
    "problem_domain_normalized": "['ç¡¬ä»¶è®¾è®¡', 'Verilogç¡¬ä»¶æè¿°è¯­è¨€ç¼–ç¨‹']",
    "source_type_normalized": "['æºè‡ªNVIDIAçš„ç”Ÿäº§IPå—']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.16787_output/content.md",
    "benchmark_name": "BLP-2025 Shared Task on Code Generation from Bangla Instructions",
    "benchmark_name_quote": "This paper presents the winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ ¹æ®ç»™å®šçš„å­ŸåŠ æ‹‰è¯­æŒ‡ä»¤ç”ŸæˆPythonä»£ç ã€‚æŒ‡ä»¤åŒ…å«å­ŸåŠ æ‹‰è¯­æè¿°ã€å‡½æ•°åå’Œå‚æ•°åã€‚",
    "task_description_quote": "The goal of the shared task is to generate Python code from a given Bangla instruction. The instruction itself contains the given Bangla instruction, function name, and argument names.",
    "dimension": "ä»éè‹±è¯­ï¼ˆå­ŸåŠ æ‹‰è¯­ï¼‰è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "This imbalance narrows access to program-synthesis tools for non-English speakers and limits our understanding of how linguistic factors â€“ such as morphology, script variation, and code-mixing â€“ impact the path from instructions to executable programs.",
    "evaluation_method": "ä½¿ç”¨pytesté£æ ¼çš„å•å…ƒæµ‹è¯•è¯„ä¼°ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Œé‡‡ç”¨Pass@1æŒ‡æ ‡",
    "evaluation_method_quote": "The candidate program is then executed against the provided unit tests (pytest-style, assert-based). ... For this task, the evaluation metric is Pass@1 (Chen et al., 2021).",
    "context_dependency": "å•å‡½æ•°ç”Ÿæˆï¼Œä¾èµ–æŒ‡ä»¤å’Œå•å…ƒæµ‹è¯•",
    "context_dependency_quote": "A candidate program must define the function precisely as provided by the function name since pytest is designed with the same function names.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜",
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "This paper presents the winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions, which consists of a multi-agent pipeline. First, a code-generation agent produces an initial solution from the input instruction.",
    "data_size": "å¼€å‘é›†400ä¸ªå­ŸåŠ æ‹‰è¯­æŒ‡ä»¤ï¼Œæµ‹è¯•é›†500ä¸ªå­ŸåŠ æ‹‰è¯­æŒ‡ä»¤",
    "data_size_quote": "Development dataset Our development set consists of 400 Bangla instructions paired with function names, each accompanied by three unit tests. ... Test dataset The test set contains 500 Bangla instructions with function names, each accompanied by a single unit test",
    "source_type": "ç”±ä»»åŠ¡ç»„ç»‡è€…æä¾›",
    "source_type_quote": "The organizers provide this dataset (Raihan et al., 2025a).",
    "last_updated": "2025",
    "last_updated_quote": "BLP-2025 Shared Task on Code Generation from Bangla Instructions",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆå…±äº«ä»»åŠ¡ç»„ç»‡è€…ï¼‰",
    "build_type_quote": "The organizers provide this dataset (Raihan et al., 2025a).",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "The goal of the shared task is to generate Python code from a given Bangla instruction.",
    "evaluation_metrics": "Pass@1",
    "evaluation_metrics_quote": "For this task, the evaluation metric is Pass@1 (Chen et al., 2021).",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆå­ŸåŠ æ‹‰è¯­ï¼‰",
    "input_modality_quote": "The goal of the shared task is to generate Python code from a given Bangla instruction.",
    "output_modality": "ä»£ç ï¼ˆPythonï¼‰",
    "output_modality_quote": "The goal of the shared task is to generate Python code from a given Bangla instruction.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "The goal of the shared task is to generate Python code from a given Bangla instruction.",
    "execution_environment": "pytestæµ‹è¯•ç¯å¢ƒ",
    "execution_environment_quote": "The candidate program is then executed against the provided unit tests (pytest-style, assert-based).",
    "unique_features": "ä¸“æ³¨äºå­ŸåŠ æ‹‰è¯­åˆ°Pythonä»£ç ç”Ÿæˆï¼Œå¡«è¡¥éè‹±è¯­ä»£ç ç”Ÿæˆè¯„æµ‹çš„ç©ºç™½",
    "unique_features_quote": "However, most benchmarks and systems remain vastly English-centric (Jiang et al., 2024). ... Bangla â€“ spoken by over 270 million people worldwide â€“ is an example of an inadequately supported language in this area.",
    "data_size_quantity": 900,
    "data_size_unit": "ä¸ªæŒ‡ä»¤",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»éè‹±è¯­ï¼ˆå­ŸåŠ æ‹‰è¯­ï¼‰è‡ªç„¶è¯­è¨€æŒ‡ä»¤ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['Pass@1']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "['ç”±ä»»åŠ¡ç»„ç»‡è€…æä¾›']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.05908_output/content.md",
    "benchmark_name": "DNext",
    "benchmark_name_quote": "Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "åœ¨å¤šä»“åº“å¾®æœåŠ¡æ¶æ„ä¸­ï¼Œæ ¹æ®è‡ªç„¶è¯­è¨€é”™è¯¯æŠ¥å‘Šå®šä½é”™è¯¯æ‰€åœ¨çš„ä»£ç æ–‡ä»¶ã€‚",
    "task_description_quote": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code... We propose reframing this as a natural language reasoning task... performing NL-to-NL search instead of cross-modal retrieval.",
    "dimension": "é”™è¯¯å®šä½çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šä»“åº“ç¯å¢ƒä¸‹çš„æœç´¢ç©ºé—´è·¯ç”±å’Œåˆ†å±‚å®šä½èƒ½åŠ›ã€‚",
    "dimension_quote": "Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories.",
    "evaluation_method": "ä½¿ç”¨Pass@kå’ŒRecall@kæŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œå…¶ä¸­k=10ç”¨äºæ–‡ä»¶å®šä½ï¼Œk=3ç”¨äºä»“åº“è·¯ç”±ã€‚åŒæ—¶ä½¿ç”¨å¹³å‡å€’æ•°æ’åï¼ˆMRRï¼‰ã€‚",
    "evaluation_method_quote": "We evaluate performance using standard metrics: Pass@k and Recall@k. Pass@k measures the proportion of bug reports where at least one correct file is found in the top-ğ‘˜ results. Recall@k measures the fraction of all correct files for a given bug that are successfully retrieved within the top-ğ‘˜ results. Given that the maximum number of modified files for any bug in our dataset is 10 (average 7.2), we set ğ‘˜= 10 as a fair and comprehensive threshold... For the preliminary search space routing phase, we use a tighter ğ‘˜= 3.",
    "context_dependency": "å¤šä»“åº“ã€å¤šæ–‡ä»¶é¡¹ç›®ï¼Œæ¶‰åŠæ–‡ä»¶ã€ç›®å½•å’Œä»“åº“ä¸‰ä¸ªå±‚æ¬¡ã€‚",
    "context_dependency_quote": "Our approach builds context-aware summaries at file, directory, and repository levels... This structured repository â†’directory â†’file search path provides an inherently transparent and auditable reasoning process.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œç‰¹åˆ«æ˜¯ç”µä¿¡é¢†åŸŸçš„å¾®æœåŠ¡æ¶æ„ç³»ç»Ÿã€‚",
    "problem_domain_quote": "Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",
    "problem_difficulty": "å·¥ä¸šçº§ï¼ŒåŒ…å«çœŸå®ã€å˜ˆæ‚çš„é”™è¯¯æŠ¥å‘Šï¼Œè§„æ¨¡å¤§ä¸”å¤æ‚ã€‚",
    "problem_difficulty_quote": "Its scale (see Table 1) and use of real-world, often noisy, bug reports provide a challenging benchmark that standard academic datasets cannot replicate.",
    "language": "Java",
    "language_quote": "Programming Language: Java",
    "data_size": "åŒ…å«46ä¸ªä»“åº“ï¼Œ7077ä¸ªä»£ç æ–‡ä»¶ï¼Œçº¦110ä¸‡è¡Œç‰©ç†ä»£ç ï¼Œ87ä¸ªé”™è¯¯å·¥å•ï¼Œå¹³å‡æ¯ä¸ªå·¥å•æ¶‰åŠ7.2ä¸ªé”™è¯¯æ–‡ä»¶ã€‚",
    "data_size_quote": "Number of Repositories: 46, Total Number of Code Files: 7,077, Total Physical Lines of Code: âˆ¼1.1M, Number of Bug Tickets: 87, Average Buggy Files per Ticket: 7.2",
    "source_type": "ä¸“æœ‰çš„å·¥ä¸šçº§å¾®æœåŠ¡ç³»ç»Ÿï¼Œé”™è¯¯å·¥å•çš„åŸºå‡†äº‹å®æ˜¯è§£å†³è¯¥é—®é¢˜çš„æ‹‰å–è¯·æ±‚ä¸­ä¿®æ”¹çš„æ–‡ä»¶é›†ã€‚",
    "source_type_quote": "Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system... The ground truth for each bug ticket is the set of files modified in the pull request that resolved the issue.",
    "last_updated": "2026",
    "last_updated_quote": "ICSE 2026, Rio de Janeiro, Brazil",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆä¸“æœ‰å·¥ä¸šç³»ç»Ÿï¼‰",
    "build_type_quote": "a proprietary, industrial-scale microservice system",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç æ£€ç´¢/å®šä½",
    "task_granularity_quote": "Bug Localization, Code Retrieval",
    "evaluation_metrics": "Pass@10, Recall@10, MRR (ç”¨äºæ–‡ä»¶å®šä½)ï¼›Pass@3, Recall@3, MRR (ç”¨äºä»“åº“è·¯ç”±)",
    "evaluation_metrics_quote": "We evaluate performance using standard metrics: Pass@k and Recall@k... we set ğ‘˜= 10... For the preliminary search space routing phase, we use a tighter ğ‘˜= 3.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆé”™è¯¯æŠ¥å‘Šï¼‰",
    "input_modality_quote": "natural language bug reports",
    "output_modality": "ä»£ç æ–‡ä»¶åˆ—è¡¨ï¼ˆæ’åï¼‰",
    "output_modality_quote": "produces the final ranked list of files most likely to be the source of the bug.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆæ£€ç´¢ï¼‰",
    "task_io_type_quote": "reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“ä¸ºå¤šä»“åº“å¾®æœåŠ¡æ¶æ„ä¸­çš„é”™è¯¯å®šä½è®¾è®¡ï¼Œä½¿ç”¨åˆ†å±‚è‡ªç„¶è¯­è¨€æ‘˜è¦ï¼ˆæ–‡ä»¶ã€ç›®å½•ã€ä»“åº“çº§ï¼‰å°†é—®é¢˜é‡æ„ä¸ºNL-to-NLæ¨ç†ä»»åŠ¡ï¼Œå¹¶æä¾›å¯è§£é‡Šçš„ä»“åº“â†’ç›®å½•â†’æ–‡ä»¶æœç´¢è·¯å¾„ã€‚",
    "unique_features_quote": "Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories... This structured repository â†’directory â†’file search path provides an inherently transparent and auditable reasoning process.",
    "data_size_quantity": 1100000,
    "data_size_unit": "è¡Œ",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['é”™è¯¯å®šä½çš„å‡†ç¡®æ€§', 'é”™è¯¯å®šä½çš„æ•ˆç‡', 'å¤šä»“åº“ç¯å¢ƒä¸‹çš„æœç´¢ç©ºé—´è·¯ç”±', 'åˆ†å±‚å®šä½èƒ½åŠ›']",
    "evaluation_method_normalized": "['Pass@10', 'Recall@10', 'MRR', 'Pass@3', 'Recall@3', 'MRR']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ç”µä¿¡é¢†åŸŸçš„å¾®æœåŠ¡æ¶æ„ç³»ç»Ÿ']",
    "source_type_normalized": "['ä¸“æœ‰çš„å·¥ä¸šçº§å¾®æœåŠ¡ç³»ç»Ÿ', 'é”™è¯¯å·¥å•çš„åŸºå‡†äº‹å®æ˜¯è§£å†³è¯¥é—®é¢˜çš„æ‹‰å–è¯·æ±‚ä¸­ä¿®æ”¹çš„æ–‡ä»¶é›†']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  }
]