[
  {
    "source_paper": "content.md",
    "benchmark_name": "HumanEval-x, CodefuseEval",
    "benchmark_name_quote": "Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodefuseEval for Chinese prompts.",
    "dataset_url": "https://github.com/codefuse-ai/codefuse-evaluation",
    "dataset_url_quote": "Moreover, we developed and open-sourced a more comprehensive benchmark, named CodefuseEval3, to support for a broader range of programming scenarios involving Chinese inputs.",
    "task_description": "代码生成、代码翻译、代码注释、测试用例生成等多语言代码相关任务",
    "task_description_quote": "In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",
    "dimension": "多语言代码理解能力、功能正确性",
    "dimension_quote": "However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied.",
    "evaluation_method": "pass@1指标、人工反馈评估",
    "evaluation_method_quote": "The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "软件工程、编程开发",
    "problem_domain_quote": "Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "支持40多种编程语言，包括Java、Python、C++、JavaScript等",
    "language_quote": "It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages.",
    "data_size": "HumanEval基准测试，包含164个Python编程问题",
    "data_size_quote": "The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%",
    "source_type": "行业标准基准测试和专门设计的中文提示评估集",
    "source_type_quote": "Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodefuseEval for Chinese prompts.",
    "last_updated": 2024,
    "last_updated_quote": "ICSE-SEIP '24, April 14–20, 2024, Lisbon, Portugal",
    "build_type": "官方自建",
    "build_type_quote": "Moreover, we developed and open-sourced a more comprehensive benchmark, named CodefuseEval3, to support for a broader range of programming scenarios involving Chinese inputs.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "代码生成、代码翻译、代码注释、测试用例生成",
    "task_granularity_quote": "In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%",
    "input_modality": "自然语言（英文和中文提示）",
    "input_modality_quote": "It is specifically designed for code-related tasks with both English and Chinese prompts",
    "output_modality": "代码",
    "output_modality_quote": "In practical scenarios, such as code generation, code translation, code comments, and testcase generation",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "专门针对中文提示优化的多语言代码评估基准，支持40多种编程语言",
    "unique_features_quote": "It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages.",
    "data_size_quantity": 164,
    "data_size_unit": "个问题",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java', 'Python', 'C++', 'JavaScript', '多语言']",
    "dimension_normalized": "['多语言代码理解能力', '功能正确性']",
    "evaluation_method_normalized": "['pass@1指标', '人工反馈评估']",
    "problem_domain_normalized": "['软件工程', '编程开发']",
    "source_type_normalized": "['行业标准基准测试', '专门设计的中文提示评估集']",
    "problem_difficulty_normalized": "未知",
    "context_dependency_normalized": "未知",
    "task_granularity_normalized": "代码生成",
    "input_modality_normalized": "自然语言",
    "output_modality_normalized": "代码",
    "execution_environment_normalized": "未知",
    "dataset_license_normalized": "未知",
    "contamination_status_normalized": "未知"
  }
]