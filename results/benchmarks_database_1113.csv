source_paper,benchmark_name,benchmark_name_quote,dataset_url,dataset_url_quote,task_description,task_description_quote,dimension,dimension_quote,evaluation_method,evaluation_method_quote,context_dependency,context_dependency_quote,problem_domain,problem_domain_quote,problem_difficulty,problem_difficulty_quote,language,language_quote,data_size,data_size_quote,source_type,source_type_quote,last_updated,last_updated_quote,build_type,build_type_quote,contamination_status,contamination_status_quote,dataset_license,dataset_license_quote,task_granularity,task_granularity_quote,evaluation_metrics,evaluation_metrics_quote,input_modality,input_modality_quote,output_modality,output_modality_quote,task_io_type,task_io_type_quote,execution_environment,execution_environment_quote,unique_features,unique_features_quote
2511.15757_output/content.md,RGym,"In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware.",,,Linux内核自动程序修复评估，专注于内核空间调试和修复的复杂性,Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on user-space applications and overlook the complexities of kernel-space debugging and repair.,Linux内核程序修复能力评估，包括定位、补丁生成、验证和成本/延迟考虑,"These characteristics make the kernel an ideal stress test for evaluating LLM-based APR, from localization to patch generation, validation, and cost/latency consideration.",通过编译修补后的内核、运行概念验证程序并报告结果来评估补丁质量,"RGym overall compiles patched kernels, runs PoCs, and reports results.",Linux内核级别的复杂依赖关系，包括大规模、深度依赖、普遍并发和硬件底层交互,"with its massive scale, deep dependency, and pervasive concurrency and low-level interactions with hardware.",操作系统内核开发，特别是Linux内核内存安全漏洞修复,"The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions.",高难度内核级bug修复，包含内存损坏等最严重类型的bug,"filtering to KASAN bugs [13], which represent the most severe types of bugs (memory corruption)",C语言（Linux内核开发）,"From 6,088 Syzbot bugs, we retain those with fix commits, reproducers, crash reports, and kernel configs",包含143个已验证的内核bug的数据集,We test on a filtered and verified dataset of 143 bugs.,来自Syzkaller内核模糊测试器和Syzbot自动崩溃报告系统的真实内核bug,"Syzkaller [6], a coverage-guided kernel fuzzer, together with Syzbot [5], an automated online crash reporting system developed by Google, provides a valuable ecosystem that makes kernel-bug collection possible",2025,arXiv:2511.15757v1 [cs.SE] 19 Nov 2025,官方自建，基于真实内核bug构建,We organize a dataset of 143 kernel bugs from Syzbot into an easily consumable format and verified the reproducibility of the bug on the patch parent.,,,,,代码修复，特别是内核内存安全漏洞修复,automated program repair (APR) in the Linux kernel space,补丁通过率（pass rate）、错误补丁率（bad patch rate）、每bug平均成本,Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug.,崩溃报告、调用栈、bug诱导提交、内核配置,"inputs (patch, commit, source, config, compiler, cores, timeout, metadata)",内核补丁代码,"The LLM lists candidate functions, receives their definitions, and returns their patched definitions.",崩溃信息到内核补丁代码,Our APR generates a patch via the Simple Agent or Function Exploration Agent and tests it with RGym.,使用docker捆绑作业依赖和QEMU虚拟机，在本地硬件上运行,RGym runs locally using docker to bundle job dependencies and QEMU for VMs.,轻量级、平台无关的Linux内核APR评估框架，专注于本地商品硬件运行，解决了kGym对GCP的硬依赖问题,"we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. RGym solves the compiler and dependency problem by smartly switching build dependencies using docker images depending on the kernel version or compiler string provided in the kernel configuration."
2511.16005_output/content.md,MultiSWE-bench-CPP,"Evaluated on the MultiSWE-bench-CPP benchmark, InfCode-C++ achieves a resolution rate of 25.58%",https://github.com/Tokfinity/InfCode,"To support the research community and facilitate future work, we release InfCode-C++ and our evaluation benchmark as an open-source project at https://github.com/Tokfinity/InfCode",C++软件问题解决，从自然语言问题描述生成修复补丁,"Given a software repository 𝐶 and a natural-language issue description 𝐷, the objective is to synthesize a patch 𝑝 such that the updated repository 𝐶′ = 𝐶 ⊕ 𝑝 satisfies the behavioral requirements expressed in 𝐷 while preserving the original functionality",代码修复能力、上下文检索准确性、结构分析能力,"Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution",通过回归测试套件验证补丁正确性，使用解决率作为评估指标,"A correct patch must satisfy: 𝑡(𝐶′) = 𝑡(𝐶), ∀𝑡∈𝑇 and ∃𝑡𝐷: 𝑡𝐷(𝐶′) = PASS, where 𝑡𝐷 is a failing test case that captures the erroneous behavior described in 𝐷",仓库级别，涉及多文件C++项目,"repository-level issue resolution... in large, statically typed C++ repositories",软件工程，C++程序修复,"Automated software issue resolution stands as a critical challenge in software engineering... resolving issues in large-scale, high-performance systems written in C++",真实世界工程级难度,"thousands of real-world GitHub issues... a formidable, unsolved challenge",C++,specifically designed for C++ projects... the first C++-aware autonomous system for end-to-end issue resolution,MultiSWE-bench的C++子集,Evaluated on the MultiSWE-bench-CPP benchmark... on the C++ subset of MultiSWE-bench,真实世界的GitHub问题,thousands of real-world GitHub issues,2025,"2025. InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution. 1, 1 (November 2025)",基于现有基准构建的子集,the C++ subset of MultiSWE-bench,,,,,代码修复,issue resolution... synthesize a patch,解决率,achieves a resolution rate of 25.58%,自然语言问题描述,natural-language issue description 𝐷,代码补丁,synthesize a patch 𝑝,文本到代码,from a natural language issue description... synthesize a patch,Docker环境,Repo Codebase Docker Env,专门针对C++语言的复杂性设计，包含语义检索和AST结构化查询,"the first C++-aware autonomous system... combines two complementary retrieval mechanisms—semantic code-intent retrieval and deterministic AST-structured querying—to construct accurate, language-aware context for repair"
2511.17131_output/content.md,UI-CUBE (UiPath Computer Use BEnchmark),"We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs.",https://github.com/UiPath/uipath_enterprise_benchmark,GitHub: https://github.com/UiPath/uipath_enterprise_benchmark,评估计算机使用代理在企业环境中的能力，包括简单的UI交互和复杂的工作流程自动化,Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks),功能正确性、操作可靠性、界面适应性、工作流协调能力,measuring not only functional accuracy but also operational reliability—providing clearer insight into how CUAs can move from experimental demos toward dependable enterprise-grade tools,通过应用程序状态的自动化验证来评估任务成功，包括多分辨率测试和系统界面变化覆盖,"with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state",多步骤工作流程、跨应用程序协调,"complex workflows including copy-paste tasks and enterprise application scenarios, multi-step processes where reliability is as critical as task completion",企业软件自动化、UI交互、工作流程管理,"enterprise application scenarios, enterprise deployment readiness, enterprise workflow automation",两个难度层级：简单UI交互和复杂工作流程,comprising 226 tasks across two difficulty tiers: simple UI interactions (136 tasks) and complex workflows (90 tasks),不特定于编程语言，主要关注UI交互,,226个任务，其中136个简单UI交互任务，90个复杂工作流程任务,comprising 226 tasks across two difficulty tiers: simple UI interactions (136 tasks) and complex workflows (90 tasks comprising 50 copy-paste/business-process tasks and 40 enterprise application tasks),基于企业应用场景的系统构建，包含模拟的企业系统工作流程,"faithful mocks of complex enterprise workflows to test coordination and operational reliability, including testing with workflows from SAP, Workday, and other enterprise systems",2025,arXiv:2511.17131v1 [cs.SE] 21 Nov 2025,官方自建,"we designed UI-CUBE, a new benchmark tailored to enterprise-like conditions, and used it to evaluate our own CUA implementation",,,,,UI交互自动化、工作流程执行,"systematic coverage of atomic UI interactions to map interface-level capabilities, and faithful mocks of complex enterprise workflows to test coordination and operational reliability",任务成功率,"Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%",UI界面、任务指令,"systematic interface variation coverage, multi-resolution testing",UI交互动作、工作流程执行结果,automated validation of task success through the application state,任务指令到UI交互,"measuring task completion effectively, they provide limited assessment of enterprise deployment readiness",企业软件环境、多分辨率显示,All tasks are evaluated across multiple screen resolutions to assess agent consistency under different display conditions,专注于企业级部署准备度评估，揭示能力悬崖现象而非渐进性能下降，提供系统界面分类覆盖,"reveals a sharp capability cliff rather than gradual performance degradation, systematic interface variation coverage, multi-resolution testing"
2511.15755_output/content.md,MyAntFarm.ai,"To test this hypothesis, we present MyAntFarm.ai, a reproducible experimental framework enabling controlled comparison of three conditions",https://github.com/Phildram1/myantfarm-ai,"Source code, Docker configurations, and trial outputs are available at: https://github.com/Phildram1/myantfarm-ai",评估多智能体LLM编排在事件响应中的决策支持质量，通过模拟认证服务回归事件来测试系统生成可执行建议的能力,multi-agent orchestration fundamentally transforms LLM-based incident response quality... achieve 100% actionable recommendation rate,决策质量、行动特异性、解决方案正确性、生产就绪性,"We introduce Decision Quality (DQ), a multi-dimensional metric capturing validity, specificity, and correctness",使用决策质量(DQ)指标，结合有效性、特异性和正确性三个维度进行自动化评分,"DQ measures actionability through three dimensions... Validity, Specificity, Correctness",单事件场景，基于特定的事件遥测数据,All 348 trials used identical context to isolate orchestration effects from scenario variability,运维智能(AIOps)、事件响应、生产环境故障诊断,Modern operational teams face a critical gap between incident detection and actionable comprehension... incident response,生产级事件响应，需要具体的可执行命令,time-critical operational contexts... production deployment,主要使用自然语言进行事件描述和响应生成,The LLM generates a single unstructured text response attempting to address all objectives,348次试验，每个条件116次试验,Through 348 controlled trials... 116 trials per condition,人工构建的模拟事件场景,All 348 trials used identical context... Authentication service regression post-deployment,2025,arXiv:2511.15755v1 [cs.AI] 19 Nov 2025,官方自建的研究框架,"we present MyAntFarm.ai, a reproducible experimental framework",,,,,事件响应决策支持，包括诊断、规划和风险评估,"coordinating specialized LLM agents for diagnosis, planning, and risk assessment",决策质量(DQ)、时间到可用理解(T2U)、有效性、特异性、正确性,"We introduce Decision Quality (DQ), a multi-dimensional metric capturing validity, specificity, and correctness",事件遥测数据和自然语言描述,"Given the following telemetry: Service: auth-service v2.4.0, Error rate: 45%...",结构化的事件简报，包含根本原因、建议行动和风险评估,"A coordinator aggregates the three agent outputs into a structured incident brief containing root cause, recommended actions, and risk assessment",事件数据到决策建议,bridging the gap between detection and actionable comprehension,容器化微服务框架，使用Docker Compose编排,MyAntFarm.ai consists of five containerized microservices orchestrated via Docker Compose,专注于多智能体编排在事件响应中的确定性质量优势，引入决策质量(DQ)这一新的评估指标,"multi-agent systems exhibit zero quality variance across all trials, making them production-ready... We introduce Decision Quality (DQ), a multi-dimensional metric capturing validity, specificity, and correctness"
2511.16708_output/content.md,CodeX-Verify,"We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs.",,,多智能体代码验证系统，使用四个专门化的智能体来检测不同类型的代码错误,"We built CodeX-Verify, a system that runs four specialized agents in parallel: Correctness (logic errors, edge cases, exception handling), Security (OWASP Top 10, CWE patterns, secrets), Performance (algorithmic complexity, resource leaks), and Style (maintainability, documentation). Each agent looks for different bug types.",代码正确性、安全性、性能和风格四个维度的错误检测,"Correctness (logic errors, edge cases, exception handling), Security (OWASP Top 10, CWE patterns, secrets), Performance (algorithmic complexity, resource leaks), and Style (maintainability, documentation)",在99个带有验证标签的代码样本上进行测试，测量真阳性率和准确率,"Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution.",,,软件工程、代码验证、漏洞检测,Multi-Agent Code Verification with Compound Vulnerability Detection,,,,,99个代码样本，覆盖16个错误类别,Dataset of 99 code samples with verified labels covering 16 bug categories,来自真实SWE-bench失败的代码样本,covering 16 bug categories from real SWE-bench failures,2025,October 2025,官方自建,We built CodeX-Verify,,,开源发布,released open-source,代码验证和错误检测,detect different types of bugs,真阳性率、准确率、假阳性率,"catches 76.1% of bugs, achieving 76.1% TPR with 68.7% accuracy (±9.1% CI)",代码,code samples,错误检测结果,detect different types of bugs,代码到错误检测结果,"analyzes code c ∈C through domain-specific function ϕi : C →Oi, producing observation Ai = ϕi(c) and decision Di ∈{0, 1}",无需执行代码的静态分析,running faster and without executing code,多智能体协同验证、复合漏洞风险建模、数学理论证明、测试15种智能体组合,"We tested all 15 combinations of agents: single agents (4 configs), pairs (6 configs), triples (4 configs), and the full system. We formalize how multiple vulnerabilities in the same code create exponentially more risk. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent."
2511.21509_output/content.md,SV-LIB 1.0,SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks,https://gitlab.com/sosy-lab/benchmarking/sv-lib,https://gitlab.com/sosy-lab/benchmarking/sv-lib,软件验证任务的交换格式和中间语言，包括程序、规范和验证见证,"we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses.",软件验证、程序分析、交换格式、验证见证,"Additional Key Words and Phrases: Software verification, Program analysis, Exchange format, Witness, Certifying Algorithm, Intermediate language, SV-LIB, SMT-LIB",,,,,软件验证、程序分析,"Additional Key Words and Phrases: Software verification, Program analysis, Exchange format, Witness, Certifying Algorithm, Intermediate language, SV-LIB, SMT-LIB",,,基于命令式编程语言概念，使用SMT-LIB表示表达式和类型,SV-LIB is based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program.,,,,,2025-11-26,Version 1.0 SV-LIB 2025-11-26,官方自建,"we propose SV-LIB, an exchange format and intermediate language for software-verification tasks",,,,,软件验证任务交换格式,"SV-LIB, an exchange format and intermediate language for software-verification tasks",,,程序、规范和验证见证,"including programs, specifications, and verification witnesses",验证结果和见证,SV-LIB defines a witness format for both correct and incorrect SV-LIB programs,软件验证任务到验证结果,exchange format and intermediate language for software-verification tasks,基于SMT求解器的验证工具基础设施,"This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already.",定义了正确和错误程序的见证格式，支持独立见证验证器，可重用验证器作为见证验证器,"Furthermore, SV-LIB defines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses."
2511.15817_output/content.md,CodeSmellEval,"The CodeSmellEval benchmark [37] proposed the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating specific types of smell.",,,评估大型语言模型生成代码的结构质量，特别是其产生代码异味（即影响可读性、可维护性或设计完整性的模式）的倾向性。,"This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality.",代码结构质量，具体为代码异味的倾向性。,"We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality.",使用倾向性异味分数（PSC），这是一个基于模型在自回归生成过程中预测的下一个令牌概率的概率度量。,The Propensity Smelly Score (PSC) is a probabilistic metric that estimates the likelihood that a LLM generates a specific type of code smell [37]. It relies on the next-token probabilities predicted by the model during autoregressive generation.,,,软件工程，代码质量评估。,"Code smells offer a foundation for evaluating these broader quality concerns. They capture recurring design and implementation issues that affect readability, complexity, and long-term maintainability [8, 23, 36].",,,Python（从示例代码片段推断）,"Figure 1: Propensity Smelly Score (SCM) Computation. The python snippet at the bottom contains two code smells: C0103 (i.e., invalid-name) and C0415 (i.e., import-outside-toplevel).",,,,,2026,"In 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE ’26), April 12–18, 2026, Rio de Janeiro, Brazil.",官方自建（基于先前工作）,The CodeSmellEval benchmark [37] proposed the Propensity Smelly Score (PSC)...,,,知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议,This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.,代码生成,"LLMs have rapidly moved from experimental tools to everyday assistants in software engineering (SE) [38]. Developers rely on them for many tasks such as code completion [5, 16, 41], summarization [1], program repair [11], clone detection [40] and test generation [39].",倾向性异味分数（PSC）,"We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types...",,,代码,"This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code.",,,,,该基准专注于代码异味（结构质量问题）的倾向性评估，而非传统的功能正确性。它引入了一个概率性度量（PSC），用于估计模型生成特定类型异味的可能性，并可作为因果分析的工具来理解生成过程中的影响因素。,"Instead of treating PSC as a standalone benchmark metric, we use it as an instrument for reasoning about the factors that influence smell propensity in LLM-generated code. Our study begins by assessing the robustness and explanatory value of PSC, establishing that it provides a stable foundation for causal analysis."
2512.03421_output/content.md,BugT,"This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",https://github.com/Xucranger/PLofLBFL,"To facilitate future study, we share our source code and experimental data in a GitHub repository 1. 1https://github.com/Xucranger/PLofLBFL",评估大型语言模型在初学者程序故障定位任务中的性能。,"This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets.",故障定位的准确性、效率、可用性，以及对问题难度的鲁棒性。,"To investigate these issues, we aim to empirically assess the performance of different LLMs across various datasets, evaluating their accuracy, efficiency, and usability in fault localization for novice programs.",在多个数据集上评估LLMs的故障定位性能，并与传统方法（如SBFL、MBFL）进行比较。,"This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets. All closed-source LLMs outperform traditional SBFL and MBFL methods...",针对包含错误的初学者程序代码片段。,"Novice Program [18] refers to code written by novice programmers, usually in the early stages of learning programming. These programs usually contain multiple errors...",编程教育，特别是针对初学者的故障定位和调试。,Novice programming assistance [19] is a crucial educational approach aimed at helping students develop their fundamental coding skills and enhance problem-solving abilities.,包含不同难度级别的问题，从简单到复杂。,"LLM accuracy decreases as problem difficulty increases in Codeflaws and Condefects, but top models maintain high accuracy even at peak difficulty in BugT, suggesting its lower complexity.",,,,,自建的包含真实编程错误的数据集。,"We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMs’ capabilities.",2025,"Preprint submitted to Journal of LATEX Templates December 4, 2025",官方自建,We introduce a new self-created dataset with real programming faults...,专门设计以减轻数据泄露问题。,...with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.,,,故障定位,This study evaluates the fault localization performance...,,,包含错误的代码,"Novice Program [18] refers to code written by novice programmers, usually in the early stages of learning programming. These programs usually contain multiple errors...",故障位置和解释,LLM generated fault explanations demonstrate significant value for novice programmer assistance...,代码到文本（故障位置/解释）,LLM generated fault explanations demonstrate significant value for novice programmer assistance...,,,专门为初学者编程教育设计，旨在减轻数据泄露问题，并包含真实编程错误。,...with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ...We introduce a new self-created dataset with real programming faults...
2512.05073_output/content.md,Comprehensive Verilog Design Problems (CVDP),"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",,,从自然语言规范生成经过验证的寄存器传输级（RTL）硬件设计实现。,transform design intent from the CVDP dataset into verified register transfer level (RTL) implementations.,硬件设计自动化能力，包括代码生成和代码理解。,"We evaluate SLM models (1.7B–20B) across code generation and comprehension, establishing baseline performance.",使用基于CocoTB的测试套件评估功能正确性。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",单模块设计问题，包含接口规范和功能要求。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",硬件设计，具体为Verilog硬件描述语言编程。,"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",代表现实世界复杂性的生产级IP块。,"Derived from production IP blocks, it represents realistic complexity.",Verilog硬件描述语言。,"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",包含336个问题。,"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",源自NVIDIA的生产IP块。,"Derived from production IP blocks, it represents realistic complexity.",,,官方自建（由NVIDIA开发）。,"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",,,,,代码生成（从规范生成RTL代码）。,transform design intent from the CVDP dataset into verified register transfer level (RTL) implementations.,通过率（pass rate）。,"State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot), highlighting substantial improvement opportunity.",自然语言规范、模块接口和功能要求。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",Verilog RTL代码。,transform design intent from the CVDP dataset into verified register transfer level (RTL) implementations.,文本到代码。,transform design intent from the CVDP dataset into verified register transfer level (RTL) implementations.,基于CocoTB的测试套件。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",专注于硬件设计（Verilog）的基准，问题源自现实世界的生产IP块，涵盖算术运算、控制逻辑、内存系统等多种设计类别。,"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs. Derived from production IP blocks, it represents realistic complexity."
2511.16787_output/content.md,BLP-2025 Shared Task on Code Generation from Bangla Instructions,This paper presents the winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions,,,根据给定的孟加拉语指令生成Python代码。指令包含孟加拉语描述、函数名和参数名。,"The goal of the shared task is to generate Python code from a given Bangla instruction. The instruction itself contains the given Bangla instruction, function name, and argument names.",从非英语（孟加拉语）自然语言指令生成代码的功能正确性,"This imbalance narrows access to program-synthesis tools for non-English speakers and limits our understanding of how linguistic factors – such as morphology, script variation, and code-mixing – impact the path from instructions to executable programs.",使用pytest风格的单元测试评估生成代码的功能正确性，采用Pass@1指标,"The candidate program is then executed against the provided unit tests (pytest-style, assert-based). ... For this task, the evaluation metric is Pass@1 (Chen et al., 2021).",单函数生成，依赖指令和单元测试,A candidate program must define the function precisely as provided by the function name since pytest is designed with the same function names.,通用编程问题,,,,Python,"This paper presents the winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions, which consists of a multi-agent pipeline. First, a code-generation agent produces an initial solution from the input instruction.",开发集400个孟加拉语指令，测试集500个孟加拉语指令,"Development dataset Our development set consists of 400 Bangla instructions paired with function names, each accompanied by three unit tests. ... Test dataset The test set contains 500 Bangla instructions with function names, each accompanied by a single unit test",由任务组织者提供,"The organizers provide this dataset (Raihan et al., 2025a).",2025,BLP-2025 Shared Task on Code Generation from Bangla Instructions,官方自建（共享任务组织者）,"The organizers provide this dataset (Raihan et al., 2025a).",,,,,代码生成,The goal of the shared task is to generate Python code from a given Bangla instruction.,Pass@1,"For this task, the evaluation metric is Pass@1 (Chen et al., 2021).",自然语言（孟加拉语）,The goal of the shared task is to generate Python code from a given Bangla instruction.,代码（Python）,The goal of the shared task is to generate Python code from a given Bangla instruction.,文本到代码,The goal of the shared task is to generate Python code from a given Bangla instruction.,pytest测试环境,"The candidate program is then executed against the provided unit tests (pytest-style, assert-based).",专注于孟加拉语到Python代码生成，填补非英语代码生成评测的空白,"However, most benchmarks and systems remain vastly English-centric (Jiang et al., 2024). ... Bangla – spoken by over 270 million people worldwide – is an example of an inadequately supported language in this area."
2512.05908_output/content.md,DNext,"Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",,,在多仓库微服务架构中，根据自然语言错误报告定位错误所在的代码文件。,Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code... We propose reframing this as a natural language reasoning task... performing NL-to-NL search instead of cross-modal retrieval.,错误定位的准确性和效率，特别是在多仓库环境下的搜索空间路由和分层定位能力。,"Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories.",使用Pass@k和Recall@k指标进行评估，其中k=10用于文件定位，k=3用于仓库路由。同时使用平均倒数排名（MRR）。,"We evaluate performance using standard metrics: Pass@k and Recall@k. Pass@k measures the proportion of bug reports where at least one correct file is found in the top-𝑘 results. Recall@k measures the fraction of all correct files for a given bug that are successfully retrieved within the top-𝑘 results. Given that the maximum number of modified files for any bug in our dataset is 10 (average 7.2), we set 𝑘= 10 as a fair and comprehensive threshold... For the preliminary search space routing phase, we use a tighter 𝑘= 3.",多仓库、多文件项目，涉及文件、目录和仓库三个层次。,"Our approach builds context-aware summaries at file, directory, and repository levels... This structured repository →directory →file search path provides an inherently transparent and auditable reasoning process.",软件工程，特别是电信领域的微服务架构系统。,"Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",工业级，包含真实、嘈杂的错误报告，规模大且复杂。,"Its scale (see Table 1) and use of real-world, often noisy, bug reports provide a challenging benchmark that standard academic datasets cannot replicate.",Java,Programming Language: Java,包含46个仓库，7077个代码文件，约110万行物理代码，87个错误工单，平均每个工单涉及7.2个错误文件。,"Number of Repositories: 46, Total Number of Code Files: 7,077, Total Physical Lines of Code: ∼1.1M, Number of Bug Tickets: 87, Average Buggy Files per Ticket: 7.2",专有的工业级微服务系统，错误工单的基准事实是解决该问题的拉取请求中修改的文件集。,"Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system... The ground truth for each bug ticket is the set of files modified in the pull request that resolved the issue.",2026,"ICSE 2026, Rio de Janeiro, Brazil",官方自建（专有工业系统）,"a proprietary, industrial-scale microservice system",,,,,代码检索/定位,"Bug Localization, Code Retrieval","Pass@10, Recall@10, MRR (用于文件定位)；Pass@3, Recall@3, MRR (用于仓库路由)","We evaluate performance using standard metrics: Pass@k and Recall@k... we set 𝑘= 10... For the preliminary search space routing phase, we use a tighter 𝑘= 3.",自然语言（错误报告）,natural language bug reports,代码文件列表（排名）,produces the final ranked list of files most likely to be the source of the bug.,文本到代码（检索）,reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval.,,,专为多仓库微服务架构中的错误定位设计，使用分层自然语言摘要（文件、目录、仓库级）将问题重构为NL-to-NL推理任务，并提供可解释的仓库→目录→文件搜索路径。,"Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories... This structured repository →directory →file search path provides an inherently transparent and auditable reasoning process."
2512.08867_output/content.md,SimpleDevQA,"we introduce SimpleDevQA, a multilingual Dev Knowledge QA benchmark derived from real user dialogues.",,,开发知识问答，旨在为软件开发过程中提出的知识寻求问题提供准确的自然语言答案。,The Development Knowledge Question Answering (Dev Knowledge QA) task aims to address knowledge-seeking questions posed during the software development process by providing accurate and relevant natural language answers.,评估大型语言模型对软件开发知识的理解能力。,to assess LLMs’ understanding capability of software development knowledge in real programming scenarios.,通过提示一个独立的评判LLM，将模型的预测与参考答案进行比较以验证正确性。,we evaluate SimpleDevQA by prompting a separate judge LLM with the model’s prediction and the reference answer to verify correctness.,,,软件开发，涵盖底层系统、数据库、网络协议、算法原理等广泛知识，而不仅仅是代码理解。,"practical software development demands a much broader understanding, encompassing knowledge of underlying systems, databases, network protocols, algorithmic principles, etc.",,,英语、中文和俄语。,"This dataset contains 2,740 QA pairs in three languages (English, Chinese, and Russian)","包含2,740个开发知识问答对。","This dataset contains 2,740 QA pairs",源自真实的用户对话（来自WildChat数据集）和网络检索的参考文档。,"a Dev Knowledge QA benchmark built from real developer dialogues; Using these topics, we then retrieve relevant reference documents from the web",2025,Publication date: December 2025.,官方自建，通过一个三阶段的数据构建流程将真实对话转化为基准。,we design and implement a three-phase data construction pipeline to convert real-world SE-related conversations into a Dev Knowledge QA benchmark.,,,,,开发知识问答,Development Knowledge Question Answering (Dev Knowledge QA) task,准确性（通过LLM评判验证正确性）,verify correctness,自然语言问题,questions inquiring about development knowledge,自然语言答案,providing accurate and relevant natural language answers,文本到文本（自然语言问题到自然语言答案）,knowledge-seeking questions... providing accurate and relevant natural language answers,,,1. 源自真实的开发者对话，反映真实的开发者任务需求和查询模式。2. 专注于具有单一、明确、可验证答案的问题，使评估更准确简单。3. 涵盖广泛的软件开发知识，而不仅仅是代码理解。4. 每个问答对都附有多个网络检索的参考，可用于验证答案的事实准确性。,"SimpleDevQA, a Dev Knowledge QA benchmark built from real developer dialogues, to address the aforementioned problems; The benchmark focuses on questions with single, correct answers, making evaluation more accurate and simple; each QA pair is also accompanied by multiple web-retrieved references, which can be used to verify the factual accuracy of the answer."
2512.10713_output/content.md,PACIFIC,"We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs...",,,评估大型语言模型（LLM）的顺序指令遵循和代码干运行（即模拟代码执行）能力。模型需要根据给定的初始输入和一系列指令，执行指令并生成最终输出。,...a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs... The model under evaluation is tasked with executing the instructions on the given input and producing the final output in the specified format.,指令遵循的精确性、代码干运行（模拟执行）能力、处理多步任务的能力,...rigorously assess sequential instruction-following and code dry-running capabilities in LLMs... Our results highlight the importance of code instruction following and dry-running ability as core competencies for LLM-based code assistants...,基于规则的确定性评估，通过将模型输出与参考代码执行生成的预期结果进行简单比较。,"PACIFIC benchmarks allow deterministic evaluation, relying on simple output comparison against expected results generated via reference code... To ensure transparency, reproducibility, and efficiency, PACIFIC employs a fully rule-based evaluation pipeline.",多步顺序指令，指令可以串联形成复杂的多步任务。,"These instructions can be concatenated to form complex multi-step tasks... Multiple instructions can then be composed into a pipeline, where each instruction receives as input the output produced by the preceding one.",计算机科学基础、算法与字符串操作,"We note that the instructions which are the framework’s building blocks are made to be easily dry-run by a first year computer science major... (Example instructions: 'next_perfect_square', 'shift_back', prime number, day of week calculation)",可控制难度，范围从易到极具挑战性。难度维度包括指令数量和预期输出长度。,...while allowing control over benchmark difficulty... We introduce an approach to control sample difficulty... The difficulty dimensions are: (1) LLM input: the number of instructions in each sample (prompt). (2) LLM expected output: the expected length of each sample’s output.,支持多种编程语言，包括Python、Java和C++。,"Currently, instructions are implemented in multiple programming languages, including Python, Java, and C++.",规模可扩展，可根据参数（如每个编程语言的样本数）生成大量多样化的基准测试。,"To produce meaningful results, the framework must support large-scale and diverse benchmark generation... • Samples per Programming Language: The number of unique samples to generate for each supported language.",通过框架自动生成，指令池作为构建块。,"PACIFIC, a framework designed to automatically generate benchmarks... Instructions serve as the fundamental building blocks of PACIFIC.",2026,"AI-SQE, April 14, 2026, Rio de Janeiro, Brazil",官方自建（由IBM Research团队提出）,"IBM Research Haifa, Israel {Itay.Dreyfuss,Antonio.Abu.Nassar,Samuel.Ackerman,Axel.Bendavid}@ibm.com",抗污染设计，框架可轻松生成新的基准测试变体以减轻训练数据污染风险。,"...our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations... Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations.",,,代码生成与推理（根据指令序列生成最终输出）,The model under evaluation is tasked with executing the instructions on the given input and producing the final output in the specified format.,Prompt-Level Accuracy（提示级准确率）,"Inspired by the metric design proposed in [21], we compute two metrics: • Prompt-Level Acc",自然语言指令或代码片段，以及初始输入（数字或字符串）。,• Instruction Type: Either Code (instructions as code snippets) or NL (instructions expressed in natural language). (1) An initial input — either a number or a string. (2) A sequence of instructions — operations to be applied to the input.,代码执行后的最终输出值（数字或字符串）。,The model under evaluation is tasked with executing the instructions on the given input and producing the final output in the specified format.,文本（指令）到代码（推理结果），或代码到代码。,The model under evaluation is tasked with executing the instructions on the given input and producing the final output... (Instructions can be in Code or NL form),干运行（模拟执行），无需实际执行环境或外部工具。,The framework emphasizes the ability of LLMs to simulate code execution (i.e. dry-running) without relying on external tools or agentic mechanisms.,1. 专注于评估代码干运行（模拟执行）能力，而非实际工具使用。2. 框架可自动生成基准测试，支持难度控制和抗污染设计。3. 提供确定性的、基于规则而非LLM评判的评估方法。,"In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM’s intrinsic ability to reason through code behavior step-by-step without execution (dry running)... PACIFIC benchmarks allow deterministic evaluation, relying on simple output comparison against expected results generated via reference code, without requiring tool use or LLM-as-a-judge paradigms... The framework must provide explicit mechanisms for controlling the difficulty of the generated benchmarks... To produce meaningful results, the framework must support large-scale and diverse benchmark generation. Furthermore, it should allow rapid creation of alternative benchmark versions to mitigate training data contamination risks."
2512.10485_output/content.md,VentiVul,"We constructed a new, small-scale evaluation dataset, which we name VentiVul.",https://github.com/Chaomeng-Lu/A-Practical-Evaluation-of-Deep-Learning-Models-and-LLMs-for-Vulnerability-Detection.git,"All of our datasets, code, and results are available at the following link1. 1https://github.com/Chaomeng-Lu/A-Practical-Evaluation-of-Deep-Learning-Models-and-LLMs-for-Vulnerability-Detection.git",检测软件源代码中的安全漏洞。,Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets... Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code...,漏洞检测的泛化能力、鲁棒性、在真实部署场景下的有效性。,"Our primary motivation arises from a critical observation about existing research. DL–based vulnerability detection models have seldom been rigorously evaluated in deployment-oriented, realistic settings involving novel or previously unseen vulnerabilities.",在时间分布外（OOD）数据集上进行评估，使用Whole-File和Function-Pair两种新颖的评估模式。,"We manually construct a small but high-quality out-of-distribution vulnerability dataset and design a deployment-oriented evaluation framework introducing two novel modes, Whole-File and Function-Pair.",函数级和文件级。,"For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits. These were carefully aligned to form function pairs... To provide additional context and challenge for model evaluation, we also included non-vulnerable functions from the same files...",软件安全、漏洞检测。,"Software vulnerabilities remain a persistent and critical threat in modern computing environments... In response, the research community and industry have turned to automation, leading to the development of a variety of techniques for detecting vulnerabilities automatically...",真实世界、新近披露的、复杂的漏洞。,"Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably.",C/C++。,"Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database... These pairs span 21 distinct .c source files...",包含来自20个Linux CVE的25个函数对（每个包含漏洞版本和修复版本），以及来自相同文件的835个无关函数。,"In total, we collected 25 function pairs (each consisting of a vulnerable and a patched version) from the 20 selected Linux CVEs. These pairs span 21 distinct .c source files and include 835 unrelated functions from the same files...",从2025年5月披露的Linux内核CVE报告中手动收集，基于明确的修复提交。,"Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database. Each report was then manually examined to identify those containing explicit fix commits with detailed code changes.",2025年5月,"Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database.",作者手动构建。,We manually construct a small but high-quality out-of-distribution vulnerability dataset...,设计为时间分布外（OOD）数据集，旨在最小化训练数据污染。,"When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset...",,,漏洞检测（二分类：漏洞/非漏洞）。,"For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits. These were carefully aligned to form function pairs, labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",,,源代码。,"For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits.",二分类标签（漏洞/非漏洞）。,"...labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",代码到标签。,"For each CVE, we extracted the vulnerable and patched versions of affected functions... labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",,,专注于评估模型在真实、部署导向场景下的泛化能力，特别是对时间分布外（OOD）和新近漏洞的检测。数据集基于2025年5月真实Linux内核CVE手动构建，包含Whole-File和Function-Pair两种评估模式。,"The novelty of this work lies in its comprehensive and deployment-oriented perspective... We manually construct a small but high-quality out-of-distribution vulnerability dataset and design a deployment-oriented evaluation framework introducing two novel modes, Whole-File and Function-Pair."
2512.15699_output/content.md,FrontierCS,"We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science...",Frontier-CS GitHub,Code and Data: Frontier-CS GitHub,评测LLM解决开放式计算机科学问题的能力，这些问题没有已知的封闭形式或确定性最优解。模型需要实现可执行程序来解决问题，而非直接输出答案。,"FrontierCS, a coding benchmark that evaluates LLMs on solving open-ended computer science problems, where no known closed-form or deterministic optimal solution exists in practice. Unlike math or reasoning benchmarks that require a direct answer (e.g., AIME), FrontierCS requires models to implement executable programs to solve the problem (e.g., LiveCodeBench).",开放式推理、算法实现、解决方案质量、跨领域问题解决能力,FrontierCS provides a benchmark at the frontier of computer-science difficulty. ... stress-testing a model’s ability to perform deep open-ended reasoning and discover nontrivial optimization strategies.,通过自动评估器运行生成的程序，根据任务特定指标（如打包密度、运行时间、内存使用）在资源限制下对输出进行确定性验证和定量评分，而非简单的通过/失败。,"Each FrontierCS task can be solved by submitting executable code: the evaluator runs the program on generated instances and scores its outputs by task-specific metrics under resource limits (e.g., time and memory usage). ... Solutions with runtime limit can be automatically checked for validity and assigned a numeric score that reflects the quality of the solution, rather than a simple pass-or-fail.",单任务问题，模型根据问题规范（及所需的I/O或API存根）生成独立的求解程序。,The model is prompted with the problem specification (and any required I/O or API stubs) and must produce a self-contained solver program.,计算机科学，包含算法优化（如NP-hard变体）和跨领域研究问题（操作系统、高性能计算、人工智能、数据库、编程语言、安全）。,"FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. ... Research Problems, spanning six major CS domains: OS (Operating Systems), HPC (High-Performance Computing), AI (Artificial Intelligence research tasks), DB (Databases), PL (Programming Languages), and Security (cybersecurity and vulnerability analysis).",前沿级，问题最优解未知或计算上不可行，旨在挑战计算机科学前沿难度。,FrontierCS provides a benchmark at the frontier of computer-science difficulty. ... The global optimum is unknown to compute over all problem instances...,未明确指定，但任务要求提交可执行代码，推断主要涉及通用编程语言。,,包含156个开放式问题，其中算法问题107个，研究问题49个。,FrontierCS consists 156 problems across two tracks: Algorithmic Problems and Research Problems. The Algorithmic Problems track contains 107 problems... The Research Problems track contains 49 problems...,由专家（包括CS博士和顶级竞赛编程参与者及出题者）设计和评审。算法问题改编自编程竞赛，研究问题源自真实世界计算机科学研究问题。,"designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. ... The Algorithmic Problems track contains 107 problems adapted from programming contests... The Research Problems track contains 49 problems sourced from real-world computer science research questions...",2025-12-17 (arXiv版本),arXiv:2512.15699v1  [cs.LG]  17 Dec 2025,官方自建，由多所大学的研究团队和专家顾问共同构建。,"Contributors... Advisors... Affiliations... (列出了UC Berkeley, Princeton University等多所机构的研究人员)",抗污染设计，通过参数化问题生成器产生大量可变难度的实例，使用新鲜的、未见过的测试用例来防止泄露和过拟合。,"Parametric Problem Generator: The task specification induces a large, variable-difficulty space of instances, enabling fresh, unseen test cases to prevent leak and overfitting.",,,代码生成，生成完整的求解程序。,Models solve these tasks by implementing executable programs rather than outputting a direct answer.,任务特定的定量评分指标（如打包密度），而非单一指标。,scored its outputs by task-specific metrics... assigned a numeric score that reflects the quality of the solution,自然语言（问题规范）及可能的代码存根。,The model is prompted with the problem specification (and any required I/O or API stubs)...,代码（自包含的求解程序）。,...must produce a self-contained solver program.,文本到代码,The model is prompted with the problem specification... and must produce a self-contained solver program.,在资源限制（如时间和内存使用）下运行，具体环境未明确说明。,"the evaluator runs the program on generated instances and scores its outputs by task-specific metrics under resource limits (e.g., time and memory usage).",专注于开放式、未解决、可验证且多样化的问题。问题的最优解未知，但解决方案的质量可以客观评估。包含算法和研究两条轨道，鼓励在开放式环境中进行迭代改进而非追求确定性最优解。,"FrontierCS, an unsolved, open-ended, verifiable, and diverse benchmark for computer science tasks. ... Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. ... The design of FrontierCS encourages iterative improvement in an open-ended landscape rather than aiming for a deterministic optimal solution, since none of its problems have known practical optima."
2512.20482_output/content.md,SWELOCMULTI,"We create SWELOCMULTI, a large-scale multilingual dataset curated specifically for issue localization.",https://github.com/SalesforceAIResearch/SweRank,1Code and models will be released here: https://github.com/com/SalesforceAIResearch/SweRank,软件问题定位，即根据自然语言错误描述或功能请求，在代码库中定位需要修改的相关函数。,"Maintaining large-scale, multilingual code-bases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified.",多语言代码排名能力、多轮迭代推理能力、问题定位准确性,"SWERANK+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization",在问题定位基准上进行实验，评估定位准确性（如Accuracy@10），并与基线方法进行比较。,"Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SWERANK-MULTI, while SWERANKAGENT further improves localization over single-pass ranking.",多文件、多语言代码库。,modern code repositories grow in size and complexity to encompass thousands of files across multiple programming languages,软件工程、软件维护、问题定位,The maintenance of large-scale software systems constitutes a significant and ever-growing portion of the software development lifecycle. A persistent bottleneck in this process is software issue localization,现实世界企业级软件系统的复杂问题定位,"real-world enterprise systems are inherently multilingual, comprising interconnected components written in diverse programming languages... complex issues that demand iterative reasoning or involve changes dispersed across multiple, loosely coupled functions.","JavaScript, Java, TypeScript, Ruby, Rust, Go, PHP, C, C++, Python","SWELOCMULTI extends the data collection and filtering pipeline of SWELOC to encompass JavaScript, Java, TypeScript, Ruby, Rust, Go, PHP, C, and C++.",大规模数据集，包含来自4060个仓库、56279个拉取请求的155663个训练实例。,Total 4060 56279 155663,从GitHub上流行的开源仓库中提取，筛选条件包括：目标语言代码占比至少40%、超过1000星、过去六个月内有提交。数据来自与GitHub问题明确关联并包含测试文件修改的拉取请求。,"Following SWERANK's methodology, we identify popular open-source repositories on GitHub for each language, filtering for repositories with at least 40% code in the target language, over 1,000 stars, and at least one commit in the preceding six months. From this curated set, we extract pull requests (PRs) explicitly linked to GitHub issues that include test file modifications.",2025,arXiv:2512.20482v1 [cs.SE] 23 Dec 2025,官方自建,"We create SWELOCMULTI, a large-scale multilingual dataset curated specifically for issue localization.",,,,,代码排名/检索,reformulates issue localization as a retrieve-and-rerank problem,Accuracy@10,Figure 1: Comparison of function localization accuracy@10 against the SWERANK baseline.,自然语言,mapping natural-language error descriptions to the relevant functions,代码（函数）排名列表,identifying where in a codebase a fix should be applied,文本到代码排名,"mapping natural language descriptions... to specific code elements including files, modules, or functions.",,,首个专注于多语言软件问题定位的基准数据集，覆盖10种流行编程语言，并支持多轮代理式搜索进行迭代推理。,"We present the first framework to address issue localization in a multilingual setting. ... We propose SWERANKAGENT, an iterative, multi-turn localization framework that further improves over single-pass ranking."
2512.21238_output/content.md,Basket,"• Basket, a Bloom’s taxonomy-guided frAmework for Software security Knowledge EvaluaTion. This framework allows the systematic assessment of LLMs’ capabilities with respect to software security knowledge (Section 3).",,,评估大型语言模型对软件安全知识的理解、应用和扩展能力，涵盖从事实回忆到高阶推理和创造性设计的六个认知层次。,"To answer this question, we developed Basket, a framework that enables the systematic assessment of LLMs guided by Bloom’s Taxonomy [22], a widely adopted framework in education research, which categorizes learning objectives across six cognitive levels: remembering, understanding, applying, analyzing, evaluating, and creating. ... to comprehensively assess how well LLMs comprehend, apply, and extend the software security knowledge boundary.",软件安全知识理解，涵盖六个认知维度：记忆、理解、应用、分析、评估和创造。,"We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating.",使用多种数据集进行评估，包括精心设计的多项选择题、易受攻击的代码片段、课程评估、真实世界案例研究和基于项目的开放式创建任务。,"Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",,,软件安全,Assessing the Software Security Comprehension of Large Language Models,涵盖从低级认知任务（如回忆事实）到高级认知任务（如推理、架构评估和安全系统创建）的多个难度层次。,"Results show that while LLMs perform well on lower-level cognitive tasks, such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation.",,,,,多种来源：包括精心设计的多项选择题、易受攻击的代码片段（SALLM）、课程评估、真实世界案例研究（XBOW）和基于项目的创建任务。,"Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",,,官方自建,"To answer this question, we developed Basket, a framework that enables the systematic assessment of LLMs guided by Bloom’s Taxonomy [22]...",,,,,知识评估与理解，涵盖从事实回忆到创造性设计的多种任务粒度。,"Rather than introducing yet another benchmark for a single security task, we used a combination of curated multiple-choice questions, vulnerable code snippets, course assessments, real-world case studies, and open-ended project tasks...",基于认知水平的可靠性能（知识边界）和准确性。,"Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance.",自然语言问题、代码片段、案例研究描述等。,"Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",自然语言答案、代码、设计等。,"This way, such a framework allows us not only to measure an LLM’s factual recall abilities but also to probe deeper into its reasoning, diagnostic ability, and creativity in secure software development tasks.",文本到文本、代码到文本、文本到代码等，取决于具体任务。,"Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",,,基于布鲁姆分类法的认知层次评估框架，旨在评估LLM的软件安全知识边界和识别其系统性误解模式，而非仅针对单一安全任务。,"Rather than introducing yet another benchmark for a single security task, we used a combination of curated multiple-choice questions, vulnerable code snippets, course assessments, real-world case studies, and open-ended project tasks to comprehensively assess how well LLMs comprehend, apply, and extend the software security knowledge boundary. ... we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identified 51 recurring misconception patterns made by LLMs across Bloom’s levels."
2512.21236_output/content.md,SPELL,"To address this gap, we propose SPELL, a comprehensive testing framework for LLM developers and the Secure Team, specifically designed to evaluate the weakness of security alignment in malicious code generation.",,,评估大型语言模型在恶意代码生成任务中的安全对齐弱点。,"To address this gap, we propose SPELL, a comprehensive testing framework for LLM developers and the Secure Team, specifically designed to evaluate the weakness of security alignment in malicious code generation.",安全对齐、恶意代码生成能力、越狱攻击成功率。,specifically designed to evaluate the weakness of security alignment in malicious code generation.,通过系统构建越狱提示词，评估模型生成恶意代码的攻击成功率，并使用最先进的检测系统确认生成的代码是否为恶意。,"Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts... Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL’s effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%...",,,网络安全、恶意软件生成。,"malicious code generation... including malware, ransomware, and other security threats.",,,,,,,从先验知识数据集中提取句子智能组合。,Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset...,2025-12-24,arXiv:2512.21236v1  [cs.CR]  24 Dec 2025,官方自建,"we propose SPELL, a comprehensive testing framework...",,,,,代码生成,malicious code generation,攻击成功率,"achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories.",自然语言（越狱提示词）,systematically constructs jailbreaking prompts,代码,generate malicious code,文本到代码,generate malicious code,,,首个动态发现和组合提示词组件的自动化框架，用于恶意代码生成评估；采用分时选择策略，平衡对新攻击模式的探索和对成功技术的利用；评估了八类恶意代码；在真实IDE环境中验证。,"We propose SPELL, the first automated framework that dynamically discovers and combines prompt components for malicious code generation... Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques... across eight malicious code categories... successful real-world deployment in production IDE environments."
