import os
import json
import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“
from openai import OpenAI
import pandas as pd
from pathlib import Path
import time
from typing import Set, List, Dict, Any

# --- 1. é…ç½® (å·²æ”¹è¿›) ---
# æœ€ä½³å®è·µï¼šä»ç¯å¢ƒå˜é‡è¯»å–API Keyï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç ã€‚
# è¯·åœ¨è¿è¡Œè„šæœ¬å‰åœ¨ç»ˆç«¯è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
# (PowerShell): $env:DEEPSEEK_API_KEY="sk-..."
# (CMD): set DEEPSEEK_API_KEY=sk-...
# (Linux/macOS): export DEEPSEEK_API_KEY="sk-..."
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
if not DEEPSEEK_API_KEY:
    raise ValueError("é”™è¯¯ï¼šè¯·è®¾ç½® 'DEEPSEEK_API_KEY' ç¯å¢ƒå˜é‡ã€‚")

client = OpenAI(
    api_key=DEEPSEEK_API_KEY,
    base_url="https://api.deepseek.com"
)


# --- 2. æ–°å¢å‡½æ•° (å‡å°‘Token) ---
def preprocess_text(full_text: str) -> str:
    """
    åœ¨å°†æ–‡æœ¬å‘é€ç»™LLMä¹‹å‰å¯¹å…¶è¿›è¡Œé¢„å¤„ç†ã€‚
    1. åˆ é™¤å‚è€ƒæ–‡çŒ®ã€é™„å½•ç­‰å™ªéŸ³éƒ¨åˆ†ã€‚
    2. æˆªå–æ ¸å¿ƒéƒ¨åˆ†ï¼ˆä¾‹å¦‚å‰5000å­—ç¬¦ï¼‰ä»¥èŠ‚çœTokenã€‚
    """
    # å®šä¹‰åœæ­¢è¯/ç« èŠ‚æ ‡é¢˜ï¼ŒåŒ¹é…åˆ°è¿™äº›è¯æ—¶ï¼Œåç»­å†…å®¹å°†è¢«æˆªæ–­
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œ(?im) æ ‡å¿—è¡¨ç¤ºå¿½ç•¥å¤§å°å†™(i)å’Œå¤šè¡Œæ¨¡å¼(m)
    # \s* è¡¨ç¤ºåŒ¹é…ä»»æ„ç©ºç™½ç¬¦ï¼ˆåŒ…æ‹¬æ¢è¡Œï¼‰
    stop_patterns = [
        r"^\s*References\s*$",
        r"^\s*Bibliography\s*$",
        r"^\s*Acknowledgements\s*$",
        r"^\s*Acknowledgments\s*$",
        r"^\s*Appendix\s*$",
        r"^\s*é™„å½•\s*$",
        r"^\s*å‚è€ƒæ–‡çŒ®\s*$"
    ]
    
    # å°†æ‰€æœ‰æ¨¡å¼åˆå¹¶ä¸ºä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼
    # (?:...) æ˜¯ä¸€ä¸ªéæ•è·ç»„
    stop_regex = re.compile(r"(\n\s*(?:" + "|".join(stop_patterns) + r")\s*\n)", re.IGNORECASE | re.MULTILINE)
    
    # æŒ‰ç¬¬ä¸€ä¸ªåŒ¹é…åˆ°çš„åœæ­¢è¯åˆ†å‰²æ–‡æœ¬ï¼Œåªä¿ç•™[0]ï¼ˆå³åœæ­¢è¯ä¹‹å‰çš„å†…å®¹ï¼‰
    cleaned_text = stop_regex.split(full_text, maxsplit=1)[0]
    
    # æˆªå–å‰16000ä¸ªå­—ç¬¦ï¼Œè¿™é€šå¸¸è¶³ä»¥è¦†ç›–æ‘˜è¦ã€å¼•è¨€å’Œæ–¹æ³•éƒ¨åˆ†
    # 16000å­—ç¬¦å¤§çº¦å¯¹åº” 3000-4000 ä¸ªTokenï¼Œæ˜¯ä¸€ä¸ªåˆç†çš„èŒƒå›´
    snippet = cleaned_text[:16000]  # è¿™é‡Œä½¿ç”¨16000å­—ç¬¦ä»¥é€‚åº”æ›´é•¿çš„æ–‡æœ¬éœ€æ±‚
    
    if len(snippet) < len(full_text) and len(snippet) > 0:
        print(f"æ–‡æœ¬å·²é¢„å¤„ç†ï¼šä» {len(full_text)} å­—ç¬¦ç¼©å‡åˆ° {len(snippet)} å­—ç¬¦ (å·²ç§»é™¤å‚è€ƒæ–‡çŒ®ç­‰)ã€‚")
    elif len(snippet) == 0 and len(full_text) > 0:
         print(f"è­¦å‘Šï¼šé¢„å¤„ç†åæ–‡æœ¬ä¸ºç©ºï¼å¯èƒ½åˆ‡åˆ†é€»è¾‘æœ‰è¯¯ã€‚å°†ä½¿ç”¨åŸå§‹æ–‡æœ¬çš„å‰5000å­—ç¬¦ã€‚")
         snippet = full_text[:5000] # Fallback
    else:
        print(f"æ–‡æœ¬å·²é¢„å¤„ç†ï¼šä¿ç•™åŸæ–‡ {len(snippet)} å­—ç¬¦ã€‚")
    
    return snippet


# --- 3. æ ¸å¿ƒå‡½æ•° (æ›´ä¸°å¯Œç»´åº¦ + æº¯æº) ---
def build_benchmark_finder_prompt(text_content: str) -> tuple[str, str]:
    """
    æ„å»ºä¸€ä¸ªç²¾ç¡®çš„æç¤ºï¼ŒæŒ‡å¯¼LLMæå– *è¯¦ç»†* ä¿¡æ¯å¹¶ *é™„å¸¦åŸæ–‡ä½ç½®*ã€‚
    å¢åŠ äº†é’ˆå¯¹â€œæ•°æ®é›†åŸå§‹è®ºæ–‡ vs åº”ç”¨è®ºæ–‡â€çš„è¾¨æé€»è¾‘ã€‚
    """
    system_prompt = """
    ä½ æ˜¯ä¸€ä½æå…¶ç»†è‡´çš„ç§‘ç ”åŠ©ç†ï¼Œä¸“æ³¨äºä»å­¦æœ¯è®ºæ–‡ä¸­æå–AIä»£ç è¯„æµ‹åŸºå‡†çš„å…ƒæ•°æ®ã€‚

    ### ğŸš¨ æ ¸å¿ƒåŸåˆ™ï¼šåŒºåˆ†â€œæ•°æ®é›†åŸç”Ÿå±æ€§â€ä¸â€œæ¨¡å‹å®éªŒè®¾ç½®â€ (CRITICAL)
    ä½ çš„é¦–è¦ä»»åŠ¡æ˜¯åˆ¤æ–­ï¼š**è¿™ç¯‡è®ºæ–‡æ˜¯â€œæå‡º/å‘å¸ƒâ€äº†è¿™ä¸ªæ•°æ®é›†ï¼Œè¿˜æ˜¯ä»…ä»…â€œä½¿ç”¨â€å®ƒï¼Ÿ**
    
    1. **å¦‚æœæ˜¯åŸå§‹å‘å¸ƒè®ºæ–‡ (Original Paper)**ï¼š
       - æå–è¯¥æ•°æ®é›†çš„æ‰€æœ‰å®šä¹‰ç‰¹å¾ã€‚
    
    2. **å¦‚æœæ˜¯åº”ç”¨/è¯„æµ‹è®ºæ–‡ (Evaluation Paper)**ï¼š
       - è¿™ç§æƒ…å†µæå…¶å®¹æ˜“å‡ºé”™ï¼**ä¸¥ç¦**å°†è®ºæ–‡ä¸­**æ¨¡å‹çš„å±€é™æ€§**æˆ–**ç‰¹å®šå®éªŒçš„è®¾ç½®**è¯¯è®¤ä¸ºæ˜¯æ•°æ®é›†çš„å±æ€§ã€‚
       - **é”™è¯¯ç¤ºä¾‹**ï¼šè®ºæ–‡è¯´ "We evaluated our model SteloCoder on the Python subset of XLCoST"ï¼ˆæˆ‘ä»¬åœ¨XLCoSTçš„Pythonå­é›†ä¸Šæµ‹è¯•äº†æ¨¡å‹ï¼‰ã€‚
         - **é”™è¯¯æå–**ï¼šlanguage: "Python" (è¿™æ˜¯å®éªŒè®¾ç½®ï¼Œä¸æ˜¯æ•°æ®é›†å…¨è²Œ)ã€‚
         - **æ­£ç¡®è¡Œä¸º**ï¼šå¯»æ‰¾æ–‡ä¸­æ˜¯å¦æœ‰æè¿°æ•°æ®é›†å®Œæ•´æƒ…å†µçš„å¥å­ï¼ˆå¦‚ "XLCoST covers 7 languages..."ï¼‰ã€‚å¦‚æœæ–‡ä¸­å®Œå…¨æœªæåŠæ•°æ®é›†çš„å…¨è²Œï¼Œä»…æåˆ°äº†å®éªŒå­é›†ï¼Œè¯·åœ¨ value ä¸­æ˜ç¡®å¤‡æ³¨ **"æ–‡ä¸­ä»…æåŠå®éªŒå­é›†(Python)ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†"**ï¼Œæˆ–è€…å°†è¯¥å­—æ®µç•™ä¸º nullï¼Œä¸è¦ç¼–é€ ã€‚

    ä½ çš„ä»»åŠ¡æ˜¯ï¼šé’ˆå¯¹æä¾›çš„è®ºæ–‡æ–‡æœ¬ï¼Œæ‰¾å‡ºå…¶ä¸­æè¿°çš„ *æ¯ä¸€ä¸ª* å­—æ®µï¼Œå¹¶åŒæ—¶æä¾›ï¼š
    1. æå–çš„ **å€¼ (value)**ã€‚
    2. æ”¯æŒè¯¥å€¼çš„ **åŸæ–‡å¼•è¿° (source_quote)**ï¼Œå³ä½ ä»æ–‡æœ¬ä¸­çœ‹åˆ°è¯¥ä¿¡æ¯çš„ä¾æ®ã€‚

    ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„åµŒå¥—JSONæ ¼å¼è¿”å›ã€‚å¦‚æœæŸä¸ªå­—æ®µçš„ä¿¡æ¯åœ¨æ–‡ä¸­ *æ˜ç¡®* æ‰¾ä¸åˆ°ï¼Œè¯·å°† "value" å’Œ "source_quote" å‡è®¾ä¸º nullã€‚

    **éœ€è¦æå–çš„JSONç»“æ„:**
    {
      "benchmark_name": {
        "value": "è¯„æµ‹åŸºå‡†çš„å®˜æ–¹åç§° (ä¾‹å¦‚ 'HumanEval', 'MBPP')",
        "source_quote": "åŸæ–‡ä¸­å®šä¹‰æˆ–æåˆ°è¯¥åç§°çš„å¥å­ (ä¾‹å¦‚ 'We introduce HumanEval, a new evaluation set...')"
      },
      "is_original_proposal": {
        "value": "ã€å¸ƒå°”å€¼æˆ–è¯´æ˜ã€‘è¿™ç¯‡è®ºæ–‡æ˜¯å¦æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡ï¼Ÿ(ä¾‹å¦‚ 'Yes' æˆ– 'No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹')",
        "source_quote": "åŸæ–‡ä¸­è¡¨æ˜èº«ä»½çš„å¥å­ (ä¾‹å¦‚ 'We propose a new dataset...' æˆ– 'We evaluate on XLCoST...')"
      },
      "dataset_url": {
        "value": "å®˜ç½‘é“¾æ¥æˆ–ä»£ç ä»“åº“URL (ä¾‹å¦‚ 'https://github.com/openai/human-eval')",
        "source_quote": "åŸæ–‡ä¸­æä¾›è¯¥URLçš„å¥å­ (ä¾‹å¦‚ 'The full dataset is available at https://github.com/...')"
      },
      "task_description": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘è¯¥è¯„æµ‹åŸºå‡†æ—¨åœ¨è§£å†³çš„ä¸»è¦ä»»åŠ¡ï¼ˆæŒ‡æ•°æ®é›†æœ¬èº«è®¾è®¡çš„ç”¨é€”ï¼Œè€Œéæœ¬æ–‡æ¨¡å‹åšçš„ä»»åŠ¡ï¼‰",
        "source_quote": "åŸæ–‡ä¸­æè¿°å…¶ä»»åŠ¡çš„å¥å­ (ä¾‹å¦‚ '...measure functional correctness for synthesizing programs from docstrings.')"
      },
      "dimension": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘è¯„æµ‹ç»´åº¦æˆ–å…³æ³¨ç‚¹ (ä¾‹å¦‚ 'ä»£ç ç”Ÿæˆæ­£ç¡®æ€§', 'å¤šè¯­è¨€èƒ½åŠ›')",
        "source_quote": "åŸæ–‡ä¸­æè¿°å…¶è¯„æµ‹ç»´åº¦çš„å¥å­"
      },
      "evaluation_method": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘è¯„ä¼°æ–¹æ³• (ä¾‹å¦‚ 'pass@k', 'æ‰§è¡Œå•å…ƒæµ‹è¯•', 'ä»£ç BLEUåˆ†æ•°')",
        "source_quote": "åŸæ–‡ä¸­æè¿°å…¶å¦‚ä½•è¯„ä¼°çš„å¥å­ (ä¾‹å¦‚ 'We use the pass@k metric, where k=1, 10, 100.')"
      },
      "context_dependency": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘ä¸Šä¸‹æ–‡ä¾èµ–èŒƒå›´ (ä¾‹å¦‚ 'å•å‡½æ•°', 'å¤šæ–‡ä»¶é¡¹ç›®')",
        "source_quote": "åŸæ–‡ä¸­æè¿°å…¶ä¸Šä¸‹æ–‡éœ€æ±‚çš„å¥å­ (ä¾‹å¦‚ 'The problems are single-function...')"
      },
      "problem_domain": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘é—®é¢˜æ‰€å±ä¸“ä¸šé¢†åŸŸ (ä¾‹å¦‚ 'ç®—æ³•', 'Webå¼€å‘', 'æ•°æ®ç§‘å­¦')",
        "source_quote": "åŸæ–‡ä¸­æè¿°å…¶é—®é¢˜ç±»å‹çš„å¥å­ (ä¾‹å¦‚ '...tasks are algorithmic in nature...')"
      },
      "problem_difficulty": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘ä»»åŠ¡éš¾åº¦ (ä¾‹å¦‚ 'å…¥é—¨çº§', 'ç«èµ›çº§', 'å·¥ç¨‹çº§')",
        "source_quote": "åŸæ–‡ä¸­æè¿°å…¶éš¾åº¦çš„å¥å­ (ä¾‹å¦‚ '...consists of 974 entry-level tasks...')"
      },
      "language": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘æ¶‰åŠçš„ä¸»è¦ç¼–ç¨‹è¯­è¨€",
        "source_quote": "åŸæ–‡ä¸­æåˆ°è¯­è¨€çš„å¥å­ (ä¾‹å¦‚ 'The dataset consists of 164 Python programming problems.')"
      },
      "data_size": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘æ•°æ®é›†è§„æ¨¡æè¿°",
        "source_quote": "åŸæ–‡ä¸­æåˆ°æ•°æ®è§„æ¨¡çš„å¥å­ (ä¾‹å¦‚ 'It includes 974 entry-level tasks...')"
      },
      "source_type": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘æ•°æ®æ¥æºæè¿°",
        "source_quote": "åŸæ–‡ä¸­æè¿°æ•°æ®æ¥æºçš„å¥å­ (ä¾‹å¦‚ 'Tasks were constructed manually...')"
      },
      "last_updated": {
        "value": "æœ€åæ›´æ–°æˆ–å‘å¸ƒæ—¶é—´ (ä¾‹å¦‚ '2021', '2024.09')",
        "source_quote": "åŸæ–‡ä¸­æåˆ°æ—¥æœŸçš„å¥å­ (ä¾‹å¦‚ 'The dataset was collected in September 2024...')"
      },
      "build_type": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘æ„å»ºç±»å‹ (ä¾‹å¦‚ 'å®˜æ–¹è‡ªå»º', 'ç¤¾åŒºè´¡çŒ®')",
        "source_quote": "åŸæ–‡ä¸­æè¿°æ„å»ºè€…èº«ä»½çš„å¥å­"
      },
      "contamination_status": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘æ•°æ®æ±¡æŸ“çŠ¶æ€ (ä¾‹å¦‚ 'é«˜æ±¡æŸ“é£é™©', 'æŠ—æ±¡æŸ“è®¾è®¡')",
        "source_quote": "åŸæ–‡ä¸­è®¨è®ºæ•°æ®æ±¡æŸ“æˆ–æ–°é²œåº¦çš„å¥å­ (ä¾‹å¦‚ '...designed to be contamination-free...')"
      },
      "dataset_license": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘æ•°æ®é›†çš„è®¸å¯è¯ (ä¾‹å¦‚ 'MIT', 'ä»…ä¾›å­¦æœ¯ç ”ç©¶')",
        "source_quote": "åŸæ–‡ä¸­æåŠè®¸å¯è¯çš„å¥å­ (ä¾‹å¦‚ 'The dataset is released under the MIT License.')"
      },
      "task_granularity": { 
        "value": "ã€ä¸­æ–‡æè¿°ã€‘(ä¾‹å¦‚ 'ä»£ç ç”Ÿæˆ', 'ä»£ç è¡¥å…¨', 'ä»£ç ä¿®å¤')", 
        "source_quote": "åŸæ–‡ä¸­æè¿°ä»»åŠ¡ç²’åº¦çš„å¥å­ (ä¾‹å¦‚ 'This task involves generating the full function body...')" 
      },
      "evaluation_metrics": { 
        "value": "ã€ä¸­æ–‡æè¿°ã€‘(ä¾‹å¦‚ 'pass@1, pass@10', 'CodeBLEU')", 
        "source_quote": "åŸæ–‡ä¸­æåˆ°å…·ä½“è¯„ä¼°æŒ‡æ ‡çš„å¥å­ (ä¾‹å¦‚ 'We evaluate functional correctness using pass@1.')" 
      },
      "input_modality": { 
        "value": "ã€ä¸­æ–‡æè¿°ã€‘(ä¾‹å¦‚ 'è‡ªç„¶è¯­è¨€', 'ä»£ç ä¸è‡ªç„¶è¯­è¨€')", 
        "source_quote": "åŸæ–‡ä¸­æè¿°è¾“å…¥ç±»å‹çš„å¥å­ (ä¾‹å¦‚ 'The input is a natural language docstring...')" 
      },
      "output_modality": { 
        "value": "ã€ä¸­æ–‡æè¿°ã€‘(ä¾‹å¦‚ 'ä»£ç ', 'è‡ªç„¶è¯­è¨€')", 
        "source_quote": "åŸæ–‡ä¸­æè¿°æœŸæœ›è¾“å‡ºçš„å¥å­ (ä¾‹å¦‚ 'The model is expected to output a block of Python code.')" 
      },
      "task_io_type": { 
        "value": "ã€ä¸­æ–‡æè¿°ã€‘ä»»åŠ¡çš„è¾“å…¥è¾“å‡ºç±»å‹ (ä¾‹å¦‚ 'æ–‡æœ¬åˆ°ä»£ç ', 'ä»£ç åˆ°ä»£ç ','ä»£ç åˆ°æ–‡æœ¬')", 
        "source_quote": "åŸæ–‡ä¸­æè¿°è¾“å…¥è¾“å‡ºæ¨¡æ€çš„å¥å­ (ä¾‹å¦‚ '...synthesizing programs from docstrings.')" 
      },
      "execution_environment": { 
        "value": "ã€ä¸­æ–‡æè¿°ã€‘(ä¾‹å¦‚ 'æ ‡å‡†åº“', 'éœ€è¦ç‰¹å®šä¾èµ–')", 
        "source_quote": "åŸæ–‡ä¸­æè¿°æ‰§è¡Œç¯å¢ƒçš„å¥å­ (ä¾‹å¦‚ 'The generated code is executed in a sandboxed environment with no external libraries.')" 
      }
      "unique_features": {
        "value": "ã€è¯·ç”¨ä¸­æ–‡æè¿°ã€‘è¯¥åŸºå‡†çš„ç‹¬ç‰¹ä¹‹å¤„æˆ–é¢å¤–ä¿¡æ¯ (å³'é¢å¤–åˆ—')",
        "source_quote": "åŸæ–‡ä¸­æè¿°å…¶ç‰¹æ®Šæ€§çš„å¥å­ (ä¾‹å¦‚ 'Unlike previous benchmarks, RMCBench focuses on malicious code generation.')"
      }
    }
    """
    user_prompt = f"**è¯·åˆ†æä»¥ä¸‹è®ºæ–‡æ–‡æœ¬ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§JSONç»“æ„æå–æ‰€æœ‰å­—æ®µåŠå…¶åŸæ–‡å¼•è¿°:**\n---\n{text_content}\n---"
    return system_prompt, user_prompt


# --- 4. æ ¸å¿ƒå‡½æ•°ï¼šè°ƒç”¨API (ä¿æŒä¸å˜) ---
def find_benchmark_info_in_text(text: str) -> dict:
    """
    ä½¿ç”¨ DeepSeek æ¨¡å‹ä»è®ºæ–‡æ–‡æœ¬ä¸­æŸ¥æ‰¾ä»£ç è¯„æµ‹åŸºå‡†è¯¦ç»†ä¿¡æ¯ã€‚
    """
    if not text.strip():
        print("è­¦å‘Šï¼šè¾“å…¥çš„æ–‡æœ¬ä¸ºç©ºã€‚")
        return {}

    system_prompt, user_prompt = build_benchmark_finder_prompt(text)

    try:
        response = client.chat.completions.create(
            model="deepseek-chat", # ç¡®ä¿ä½ ä½¿ç”¨çš„æ˜¯æœ‰æƒé™ä¸”èƒ½åŠ›è¶³å¤Ÿçš„æ¨¡å‹
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0, # æå–ä»»åŠ¡ä½¿ç”¨ä½æ¸©
            stream=False
        )
        response_content = response.choices[0].message.content
        cleaned_response = response_content.strip().replace('```json', '').replace('```', '').strip()
        if cleaned_response.startswith('"') and cleaned_response.endswith('"'):
             cleaned_response = cleaned_response[1:-1].replace('\\"', '"')

        extracted_json = json.loads(cleaned_response)
        
        # æ£€æŸ¥æœ€å…³é”®çš„å­—æ®µ
        if not extracted_json.get("benchmark_name") or not extracted_json.get("benchmark_name").get("value"):
             print("è­¦å‘Šï¼šæœªèƒ½ä»æ–‡æœ¬ä¸­æ˜ç¡®æå–åˆ° benchmark_name çš„å€¼ã€‚")
        return extracted_json

    except json.JSONDecodeError as json_err:
        print(f"JSONè§£æé”™è¯¯: {json_err}")
        print("--- åŸå§‹æ¨¡å‹è¾“å‡º ---")
        print(response_content if 'response_content' in locals() else "N/A")
        print("--------------------")
        return {"error": "JSONDecodeError", "original_response": response_content if 'response_content' in locals() else "N/A"}
    except Exception as e:
        print(f"å¤„ç†æ–‡æœ¬æ—¶å‘ç”ŸAPIæˆ–å…¶ä»–é”™è¯¯: {e}")
        return {"error": str(e), "original_response": "API call failed"}


# --- 5. æ–°å¢å‡½æ•° (åŠ å…¥åŸæ–‡ä½ç½®ï¼šæ‰å¹³åŒ–æ•°æ®ä»¥ä¾¿å­˜å…¥CSV) ---
def flatten_extracted_data(nested_data: dict, source_paper: str) -> dict:
    """
    å°†LLMè¿”å›çš„åµŒå¥—JSONæ‰å¹³åŒ–ï¼Œä»¥ä¾¿å­˜å…¥CSVã€‚
    ä¾‹å¦‚ï¼š{"benchmark_name": {"value": "...", "source_quote": "..."}}
    å˜ä¸ºï¼š{"benchmark_name": "...", "benchmark_name_quote": "..."}
    """
    flat_data = {"source_paper": source_paper}
    
    # æ³¨æ„ï¼šLLM è¿”å›çš„ true/false åœ¨ JSON ä¸­æ˜¯å¸ƒå°”å€¼
    is_original_item = nested_data.get("is_original_proposal", {})
    if isinstance(is_original_item, dict):
        flat_data["is_original_proposal"] = is_original_item.get("value")
        flat_data["is_original_proposal_quote"] = is_original_item.get("source_quote")
    else:
        flat_data["is_original_proposal"] = None
        flat_data["is_original_proposal_quote"] = None

    # éå†æ‰€æœ‰åœ¨promptä¸­å®šä¹‰çš„é”®
    all_fields = [
        "benchmark_name", "is_original_proposal","dataset_url", "task_description", "dimension",
        "evaluation_method", "context_dependency", "problem_domain", 
        "problem_difficulty", "language", "data_size", "source_type",
        "last_updated", "build_type", "contamination_status", 
        "dataset_license", "task_granularity", "evaluation_metrics", "input_modality",
        "output_modality", "task_io_type","execution_environment","unique_features"
    ]
    
    for field in all_fields:
        item = nested_data.get(field)
        
        if isinstance(item, dict):
            # æ­£å¸¸æƒ…å†µï¼šæå– value å’Œ source_quote
            flat_data[field] = item.get("value")
            flat_data[f"{field}_quote"] = item.get("source_quote")
        else:
            # å¼‚å¸¸æƒ…å†µï¼šLLMå¯èƒ½è¿”å›äº† null æˆ–å…¶ä»–éå­—å…¸ç»“æ„
            flat_data[field] = item # å¯èƒ½æ˜¯ null
            flat_data[f"{field}_quote"] = None
            
    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
    if "error" in nested_data:
        flat_data["error"] = nested_data.get("error")
        flat_data["original_response"] = nested_data.get("original_response")

    return flat_data

def load_existing_csv(csv_path: Path) -> (pd.DataFrame, Set[str]):
    """
    åŠ è½½ç°æœ‰çš„åŸå§‹CSVï¼Œè¿”å›DataFrameå’Œå·²å¤„ç†è¿‡çš„ 'source_paper' è·¯å¾„é›†åˆã€‚
    """
    if not csv_path.exists():
        print(f"æœªæ‰¾åˆ°ç°æœ‰CSVæ•°æ®åº“ï¼š{csv_path}ã€‚å°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")
        return pd.DataFrame(), set()
    try:
        df = pd.read_csv(csv_path)
        if 'source_paper' not in df.columns:
             print("è­¦å‘Šï¼šç°æœ‰CSVæ²¡æœ‰ 'source_paper' åˆ—ï¼Œå°†é‡æ–°å¤„ç†æ‰€æœ‰æ–‡ä»¶ã€‚")
             return pd.DataFrame(), set()
        # å°†è·¯å¾„è§„èŒƒåŒ–ä¸º posix æ ¼å¼ (/)ï¼Œä»¥ä¾¿è·¨å¹³å°æ¯”è¾ƒ
        processed_paths = set(Path(p).as_posix() for p in df['source_paper'].astype(str))
        print(f"æˆåŠŸåŠ è½½åŸå§‹CSVï¼Œå·²åŒ…å« {len(df)} æ¡è®°å½•ã€‚")
        return df, processed_paths
    except pd.errors.EmptyDataError:
        print(f"è­¦å‘Šï¼šåŸå§‹CSVæ–‡ä»¶ {csv_path} ä¸ºç©ºã€‚å°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")
        return pd.DataFrame(), set()
    except Exception as e:
        print(f"åŠ è½½ {csv_path} å¤±è´¥: {e}ã€‚å°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")
        return pd.DataFrame(), set()


# --- 6. ä¸»ç¨‹åº (å·²æ›´æ–°) ---
if __name__ == "__main__":
    try:
        script_path = Path(__file__).resolve()
        project_root = script_path.parent
    except NameError:
        # é€‚åº” .ipynb/.py äº¤äº’å¼ç¯å¢ƒ
        project_root = Path.cwd() 

    papers_folder = project_root / "new_papers_info" # ç°åœ¨æŒ‡å‘æ­£ç¡®çš„çˆ¶æ–‡ä»¶å¤¹
    results_folder = project_root / "results"
    results_folder.mkdir(exist_ok=True)

    # å®šä¹‰è¾“å‡ºæ–‡ä»¶å
    output_filename = results_folder / "benchmarks_database_1125.csv"

    df_existing, processed_paths = load_existing_csv(output_filename)
    print(f"å·²å¤„ç†çš„æ–‡ä»¶: {len(processed_paths)} ä¸ª")
    print(f"æ­£åœ¨æ–‡ä»¶å¤¹ '{papers_folder}' åŠå…¶æ‰€æœ‰å­æ–‡ä»¶å¤¹ä¸­é€’å½’æœç´¢ .md æ–‡ä»¶...")
    #æŸ¥æ‰¾ .md æ–‡ä»¶
    input_files = list(papers_folder.glob("**/*.md")) 

    if not input_files:
        print(f"é”™è¯¯ï¼šåœ¨ '{papers_folder}' åŠå…¶å­æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .md æ–‡ä»¶ã€‚")
        exit()

    # è¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„æ–°æ–‡ä»¶
    new_files_to_process = []
    for file_path in input_files:
        # å°†è·¯å¾„è§„èŒƒåŒ–ä¸º posix æ ¼å¼ (/)ï¼Œä»¥ä¾¿ä¸ set ä¸­çš„è·¯å¾„æ¯”è¾ƒ
        relative_path_str = file_path.relative_to(papers_folder).as_posix()
        
        if file_path.is_file() and relative_path_str not in processed_paths:
            new_files_to_process.append((file_path, relative_path_str))

    if not new_files_to_process:
        print("æ²¡æœ‰æ‰¾åˆ°éœ€è¦æå–çš„æ–°æ–‡ä»¶ã€‚æ‰€æœ‰æ–‡ä»¶å‡å·²å¤„ç†ã€‚")
        exit() # æ­£å¸¸é€€å‡º

    print(f"æˆåŠŸæ‰¾åˆ° {len(input_files)} ä¸ªæ€»æ–‡ä»¶ï¼Œå…¶ä¸­ {len(new_files_to_process)} ä¸ªæ˜¯æ–°æ–‡ä»¶ã€‚å‡†å¤‡å¼€å§‹å¤„ç†...")
          
    new_benchmarks_flat = [] # åªå­˜å‚¨æ–°æ–‡ä»¶çš„ç»“æœ

    # åªå¾ªç¯å¤„ç†æ–°æ–‡ä»¶
    for file_path, relative_path_str in new_files_to_process:
        
        print(f"\n--- æ­£åœ¨å¤„ç†æ–°æ–‡ä»¶: {relative_path_str} ---")
        try:
            full_text = file_path.read_text(encoding='utf-8')
            text_snippet = preprocess_text(full_text)
            nested_benchmark_info = find_benchmark_info_in_text(text_snippet)
            
            # ä½¿ç”¨ relative_path_str ç¡®ä¿è·¯å¾„æ ¼å¼ä¸€è‡´
            flat_benchmark_info = flatten_extracted_data(nested_benchmark_info, relative_path_str)
            
            # --- æ–°å¢è¿‡æ»¤é€»è¾‘ ---
            is_original = flat_benchmark_info.get("is_original_proposal")
            
            # ä¸¥æ ¼æ¨¡å¼ï¼šå¦‚æœä¸ç¡®å®šæˆ–ä¸æ˜¯åŸå§‹è®ºæ–‡ï¼Œåˆ™å‘å‡ºè­¦å‘Šå¹¶ä¸ä¿å­˜ï¼ˆæˆ–è€…ä¿å­˜ä½†ä½ çŸ¥é“å®ƒæ˜¯è„æ•°æ®ï¼‰
            if is_original is False:
                print(f"âš ï¸ è·³è¿‡: {relative_path_str} ä¼¼ä¹ä¸æ˜¯æ•°æ®é›†çš„åŸå§‹å‡ºå¤„ (is_original_proposal=False)ã€‚")
                print(f"   ç†ç”±: {flat_benchmark_info.get('is_original_proposal_quote')}")
                continue # è·³è¿‡å½“å‰å¾ªç¯ï¼Œä¸æ·»åŠ åˆ° new_benchmarks_flat

            # --------------------

            print("æå–ç»“æœ (æ‰å¹³åŒ–):", json.dumps(flat_benchmark_info, indent=2, ensure_ascii=False))

            if flat_benchmark_info.get("benchmark_name"):
                new_benchmarks_flat.append(flat_benchmark_info)
            elif "error" in flat_benchmark_info:
                 print(f"å¤„ç†æ–‡ä»¶ {relative_path_str} æ—¶é‡åˆ°é”™è¯¯ï¼Œå·²è·³è¿‡ã€‚")
            else:
                print(f"åœ¨æ–‡ä»¶ {relative_path_str} ä¸­æœªæå–åˆ° benchmark_nameï¼Œå·²è·³è¿‡ã€‚")
                 
        except Exception as e:
            print(f"è¯»å–æˆ–å¤„ç†æ–‡ä»¶ '{relative_path_str}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        
        time.sleep(1) # å¢åŠ é€Ÿç‡é™åˆ¶ï¼Œé˜²æ­¢APIè¿‡è½½

    # --- ä¿å­˜ç»“æœï¼ˆåˆå¹¶ï¼‰ ---
    if new_benchmarks_flat:
        df_new = pd.DataFrame(new_benchmarks_flat)
        
        # åˆå¹¶æ—§æ•°æ®å’Œæ–°æ•°æ®
        df_combined = pd.concat([df_existing, df_new], ignore_index=True)
        
        print(f"\nâœ… æˆåŠŸæå– {len(new_benchmarks_flat)} æ¡æ–°è®°å½•ã€‚")
    else:
        print("\nâŒ æœªèƒ½ä»æ–°æ–‡ä»¶ä¸­æˆåŠŸæå–ä»»ä½•æ•°æ®ã€‚")
        df_combined = df_existing # å³ä½¿æ²¡æœ‰æ–°æ•°æ®ï¼Œä¹Ÿè¦ç¡®ä¿åç»­æ­¥éª¤èƒ½è¿è¡Œ

    # --- ä¿å­˜ç»“æœ (åˆ—é¡ºåºå·²æ›´æ–°) ---
    if not df_combined.empty:
        
        # å®šä¹‰åŸºç¡€åˆ— (ä¸°å¯Œäº†ç»´åº¦)
        base_columns = [
            "benchmark_name", "is_original_proposal","dataset_url", "task_description", "dimension",
            "evaluation_method", "context_dependency", "problem_domain", 
            "problem_difficulty", "language", "data_size", "source_type",
            "last_updated", "build_type", "contamination_status", 
            "dataset_license", "task_granularity", "evaluation_metrics", "input_modality",
            "output_modality", "task_io_type", "execution_environment", "unique_features" # "é¢å¤–åˆ—"
        ]
        
        # åŠ¨æ€ç”Ÿæˆå¸¦å¼•è¿°çš„å®Œæ•´åˆ—åˆ—è¡¨ (å®ç°è¯·æ±‚ 3)
        desired_columns = ['source_paper']
        for col in base_columns:
            if col in df_combined.columns: # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                desired_columns.append(col)
                if f"{col}_quote" in df_combined.columns:
                    desired_columns.append(f"{col}_quote") # ä¸ºæ¯ä¸ªå­—æ®µæ·»åŠ å¼•è¿°åˆ—
        
        # æ’åº
        existing_columns_ordered = [col for col in desired_columns if col in df_combined.columns]
        other_columns = [col for col in df_combined.columns if col not in existing_columns_ordered]
        final_columns = existing_columns_ordered + other_columns
        # ç¡®ä¿åªä½¿ç”¨ df_combined ä¸­å®é™…å­˜åœ¨çš„åˆ—
        final_columns_existing = [col for col in final_columns if col in df_combined.columns]
        df_combined = df_combined[final_columns]

        df_combined.to_csv(output_filename, index=False, encoding='utf-8-sig')

        print(f"æ•°æ®é›†å·²æ›´æ–°å¹¶ä¿å­˜åˆ°: {output_filename}")
        print(f"æ•°æ®åº“æ€»æ¡ç›®æ•°: {len(df_combined)}")
        print("\nç»“æœé¢„è§ˆ (å‰5è¡Œï¼Œéƒ¨åˆ†æ ¸å¿ƒåˆ—):")
        preview_cols = ['source_paper', 'benchmark_name', 'dataset_url', 'task_description', 'evaluation_method', 'context_dependency']
        print(df_combined.head()[preview_cols].to_markdown(index=False))
    else:
        print(f"\nâŒ æœªèƒ½ä» {len(input_files)} ä¸ªè·¯å¾„ä¸­æˆåŠŸè¯†åˆ«å‡ºæœ‰æ•ˆçš„ä»£ç è¯„æµ‹åŸºå‡†ä¿¡æ¯ã€‚")
