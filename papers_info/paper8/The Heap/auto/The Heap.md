# The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models

Jonathan Katzy Razvan Mihai Popescu Arie van Deursen Maliheh Izadi Delft University of Technology Delft University of Technology Delft University of Technology Delft University of Technology Delft, The Netherlands Delft, The Netherlands Delft, The Netherlands Delft, The Netherlands 0009-0005-9574-2414 0009-0003-6251-770X 0000-0003-4850-3312 0000-0001-5093-5523

Abstract—The recent rise in the popularity of large language models has spurred the development of extensive code datasets needed to train them. This has left limited code available for collection and use in the downstream investigation of specific behaviors, or evaluation of large language models without suffering from data contamination. To address this problem, we release The Heap, a large multilingual dataset covering 57 programming languages that has been deduplicated with respect to other open datasets of code, enabling researchers to conduct fair evaluations of large language models without significant data cleaning overhead.

Index Terms—Dataset, Evaluation, Large Language Models, Open Science, Data Contamination, Multilingual

# I. INTRODUCTION

The data-intensive training process of Large Language Models (LLMs) has driven the release of numerous large-scale datasets, particularly for code, to facilitate the development of new models. This rapid increase in the amount of training data used to pre-train LLMs has resulted in extensive datasets covering almost all publicly available code [1]–[3].

To assess the success of such LLMs in downstream tasks, fresh data not seen during training is needed. Otherwise such evaluations are contaminated, possibly resulting in overly optimistic results. Unfortunately, obtaining such non-contaminated data is increasingly difficult. In fact, a recent study establishes that only $10 \%$ of investigations involving LLMs deduplicate their data with respect to the training data in order to avoid contamination [4].

To address this, we propose The Heap, a dataset of not previously used code that can be used for contamination-free multilingual evaluation of LLMs in downstream tasks. We address contamination in two ways. First, we select code with a non-permissive license, such as the GNU General Public License. Using such code for training is unattractive, as it may require the end user to publicly release all code in their code bases. Second, we pre-conduct computationally expensive near and exact deduplication, removing code that is used in other datasets widely used for training such as The Stack [1].

# II. COLLECTION

Using the search API, we collect our dataset from GitHub, a commonly used online platform for sharing code repositories. This collection process mimics the data distribution of other large-scale datasets [3], [5]–[8], minimizing the probability of including confounding factors in the dataset, such as drifts in the representations of data [9].

# A. Programming Languages

We aim to compile a representative dataset that encompasses a wide range of programming languages. To achieve this, we select languages based on several criteria. Our selection includes languages with diverse syntactic structures, such as LISP, C, Python, Haskell, and Assembly. We also select different programming paradigms, such as COBOL, Pascal, and C for procedural languages, Java, C#, Python, for objectoriented languages, and Haskell and Clojure for functional languages. To cover more specific use cases, we also include domain-specific languages such as Mathematica, Emacs-Lisp, and Coq. A complete list of all languages included in the dataset is presented in Table I.

# B. Query

We focus on repositories that have one of the targeted languages as the main language of the repository. We further select only repositories that are licensed under non-permissive licenses. We choose non-permissive licenses as an initial filter for repositories, as many large-scale datasets focus on exclusively unlicensed or permissively licensed code [2], [3], [5]. The reasons for the exclusion of non-permissively licensed code in other datasets come from potential licensing issues that may be related to the output of models trained on nonpermissively licensed data [10]. The Heap is not intended for pre-training models that are aimed at end users, but rather for exclusive use in a research setting. The inclusion of exclusively non-permissively licensed code has the added benefit that it acts as a deterrent for developers to train LLMs on The Heap, ensuring it remains a relevant source of data for downstream tasks. We provide an overview of the licenses used in this work in Table II.

# C. Scraping

For each programming language, we scrape up to 50,000 repositories or as many as are available. Our dataset contains code from repositories created between January 2008 and August 2024. For each selected language, we extract repositories sorted by star count in descending order; this has been used as a loose quality metric before [11]. To maximize extraction efficiency and avoid GitHub’s rate limits, we employ pagination and repository creation date filtering. When the number of repositories within a specified time frame exceeds the rate limit, we narrow the time interval and apply a tumbling window approach to ensure comprehensive coverage. We guide the file extraction based on a list of file extensions from The Stack [5].

TABLE I LANGUAGES INCLUDED IN THE DATASET   

<table><tr><td>Language</td><td>Repositories</td><td>Raw Files</td><td>Unique Files</td></tr><tr><td>Ada</td><td>676</td><td>41,367</td><td>35,425</td></tr><tr><td>Agda</td><td>142</td><td>5,483</td><td>5,113</td></tr><tr><td>ANTLR</td><td>101</td><td>564</td><td>541</td></tr><tr><td>Apex</td><td>253</td><td>17,833</td><td>7,641</td></tr><tr><td>Assembly</td><td>7,100</td><td>208,896</td><td>104,901</td></tr><tr><td>C</td><td>50,000</td><td>16,585,280</td><td>4,960,192</td></tr><tr><td>C#</td><td>50,000</td><td>5,906,716</td><td>3,770,829</td></tr><tr><td>C++</td><td>50,000</td><td>14,891,856</td><td>4,811,620</td></tr><tr><td>Clojure</td><td>27,107</td><td>380,567</td><td>273,181</td></tr><tr><td>Cobol</td><td>341</td><td>2,242</td><td>1,208</td></tr><tr><td>Common Lisp</td><td>796</td><td>45,083</td><td>16,968</td></tr><tr><td>Coq</td><td>477</td><td>54,137</td><td>26,175</td></tr><tr><td>Crystal</td><td>368</td><td>11,606</td><td>7,300</td></tr><tr><td>Cuda</td><td>1,191</td><td>26,948</td><td>13,359</td></tr><tr><td>D</td><td>1,185</td><td>185,630</td><td>126,111</td></tr><tr><td>Dart</td><td>11,907</td><td>484,935</td><td>413,203</td></tr><tr><td>EJS</td><td>1,475</td><td>15,513</td><td>12,884</td></tr><tr><td>Elixir</td><td>2,371</td><td>643,856</td><td>127,910</td></tr><tr><td>Emacs Lisp</td><td>377</td><td>8,260</td><td>7,963</td></tr><tr><td>Erlang</td><td>1,240</td><td>55,932</td><td>32,049</td></tr><tr><td>F#</td><td>876</td><td>22,152</td><td>16,015</td></tr><tr><td>Forth</td><td>222</td><td>28,287</td><td>7,932</td></tr><tr><td>Go</td><td>50,000</td><td>8,506,379</td><td>2,355,716</td></tr><tr><td>Groovy</td><td>2,198</td><td>60,299</td><td>48,353</td></tr><tr><td>Hack</td><td>1,379</td><td>84,916</td><td>37,405</td></tr><tr><td>Haskell</td><td>8,023</td><td>122,788</td><td>111,234</td></tr><tr><td>Java</td><td>50,000</td><td>6,989,601</td><td>5,197,338</td></tr><tr><td>JavaScript</td><td>50,000</td><td>8,289,901</td><td>3,393,747</td></tr><tr><td>Julia</td><td>2,859</td><td>46,284</td><td>38,381</td></tr><tr><td>Kotlin</td><td>21,665</td><td>1,467,343</td><td>1,045,396</td></tr><tr><td>Less</td><td>433</td><td>17,276</td><td>7,389</td></tr><tr><td>Lua</td><td>42,241</td><td>4,605,230</td><td>913,898</td></tr><tr><td>Mathematica</td><td>1,528</td><td>164,498</td><td>89,853</td></tr><tr><td>MATLAB</td><td>20,828</td><td>1,051,354</td><td>665,659</td></tr><tr><td>NetLogo</td><td>332</td><td>900</td><td>863</td></tr><tr><td>NewLisp</td><td>35</td><td>5,819</td><td>5,148</td></tr><tr><td>Nix</td><td>1,892</td><td>75,093</td><td>71,199</td></tr><tr><td>Objective-C</td><td>7,700</td><td>1,899,714</td><td>698,137</td></tr><tr><td>OCaml</td><td>1,961</td><td>121,890</td><td>69,171</td></tr><tr><td>Pascal</td><td>5,218</td><td>330,832</td><td>225,749</td></tr><tr><td>Perl</td><td>14,673</td><td>1,798,520</td><td>629,769</td></tr><tr><td>PHP</td><td>50,000</td><td>12,707,727</td><td>3,363,040</td></tr><tr><td>Processing</td><td>2,950</td><td>24,723</td><td>20,343</td></tr><tr><td>Prolog</td><td>1,071</td><td>38,995</td><td>20,279</td></tr><tr><td>Python</td><td>50,000</td><td>2,290,182</td><td>1,792,451</td></tr><tr><td>R</td><td>44,993</td><td>589,139</td><td>374,812</td></tr><tr><td>Raku</td><td>158</td><td>1,384</td><td>1,306</td></tr><tr><td>Ruby</td><td>13,378</td><td>1,579,655</td><td>794,364</td></tr><tr><td>Rust</td><td>42,847</td><td>2,496,177</td><td>844,258</td></tr><tr><td>Scala</td><td>5,893</td><td>749,370</td><td>224,021</td></tr><tr><td>Scheme</td><td>1,878</td><td>106,620</td><td>54,226</td></tr><tr><td>Scilab</td><td>199</td><td>4,531</td><td>4,084</td></tr><tr><td>SQL</td><td>130</td><td>47,185</td><td>41,178</td></tr><tr><td>Starlark</td><td>146</td><td>524</td><td>498</td></tr><tr><td>Swift</td><td>13,924</td><td>633,819</td><td>439,565</td></tr><tr><td>Vue</td><td>14,858</td><td>457,605</td><td>323,672</td></tr><tr><td>WebAssembly</td><td>68</td><td>834</td><td>587</td></tr><tr><td>Total</td><td>733,663</td><td>96,990,250</td><td>38,681,609</td></tr></table>

TABLE II COPYLEFT LICENSES INCLUDED IN THE DATASET.   

<table><tr><td rowspan=1 colspan=1>License</td><td rowspan=1 colspan=1>Family</td><td rowspan=1 colspan=1>Description1</td></tr><tr><td rowspan=1 colspan=1>CECILL-1.0CECILL-1.1CECILL-2.0CECILL-2.1CECILL-CEPL-1.0EPL-2.0LGPL-2.1LGPL-3.0MS-RLMPL-2.0</td><td rowspan=1 colspan=1>Weak Copyleft</td><td rowspan=1 colspan=1>Share changes and additions tothe licensed software whenredistributing.</td></tr><tr><td rowspan=1 colspan=1>GPL-2.0GPL-3.0</td><td rowspan=1 colspan=1>Strong Copyleft</td><td rowspan=1 colspan=1>Share larger programs built withthe licensed software whenredistributing. This extendsweak copyleft requirements.</td></tr><tr><td rowspan=1 colspan=1>AGPL-3.0EUPL-1.1EUPL-1.2OSL-3.0</td><td rowspan=1 colspan=1>Network Copyleft</td><td rowspan=1 colspan=1>Share larger programs built withthe licensed software whenredistributing or running it overa network. This extends strongcopyleft requirements.</td></tr></table>

# D. Cleaning

After collecting the data from online sources, we perform some cleaning steps. First, we exclude files containing fewer than 10 words or exceeding $1 0 \ \mathrm { { M B } }$ in size. We also remove exact duplicates from our own dataset. We use the same approach as the exact deduplication with respect to other datasets described in Section III-A.

# III. DEDUPLICATION

An important aspect of fairly evaluating downstream tasks is preventing data leakage [4]. This is often done through a deduplication process. Although there should be no overlap between our non-permissively licensed dataset and permissively licensed datasets due to our selection procedure, it does not completely prevent overlap [10].

Our deduplication strategy consists of exact deduplication and near deduplication. Before each deduplication strategy, we remove all comments (using a regex, based on the programming language) and whitespace from each file. This ensures that small changes to files, such as the removal of a license comment or changes in whitespace characters, still result in the detection of an exact duplicate. The final files included in The Heap are the unaltered versions scraped from GitHub.

![](images/ac85a7fd027267b4b058f0d371f3aa210bf89904513645dc77e15795008c36a6.jpg)  
Fig. 1. Example of final dataset structure for one entry

a) Exact Deduplication: For exact deduplication, we calculate the SHA-256 hash of each file to identify exact duplicates between The Heap and publicly available datasets. We selected this hash function for its low collision probability, which reduces the risk of false positives.

b) Near Deduplication: We also perform neardeduplication between our scraped dataset and the publicly available ones. To achieve this, we utilize the MinHash Locality-Sensitive Hashing (LSH) approach, implemented using the datasketch2 library. We apply the same SHA256 hashing function as before, with 128 permutations and a precision-recall weight distribution of $4 0 \% \mathrm { ~ - ~ } 6 0 \%$ These design choices help mitigate hash collisions while maintaining a balanced trade-off, hence favoring higher recall at the expense of a controlled increase in false positives (removing files that were not duplicates).

We use a shingle size of 7 characters, as code files typically use a smaller set of characters compared to large research articles, where $k \ = \ 9$ [12]. This reduces the likelihood of overly common shingles, which could otherwise inflate similarity scores, as would occur with smaller values of $k$ . Files with a Jaccard similarity above 0.7 are flagged as near duplicates, a threshold shown to be effective for duplicate detection [13].

TABLE III LIST OF PUBLICLY-AVAILABLE DATASETS USED FOR DEDUPLICATION   

<table><tr><td>Dataset</td><td>Source</td></tr><tr><td>The Stack V2 [3]</td><td>All permissively licensed and unlicensed fi les collected in the Software Heritage [14] archive.</td></tr><tr><td>The Stack [1]</td><td>All permissively licensed repositories collected in the GHArchive [15] and scraped from GitHub.</td></tr><tr><td>Red Pajama [2]</td><td>Repositories from the GitHub dataset hosted by Google BigQuery [16] licensed under MIT, BSD, or Apache licenses.</td></tr><tr><td>GitHub Code [8]</td><td>Repositories from the GitHub dataset hosted by Google BigQuery [16].</td></tr><tr><td>CodeParrot [7]</td><td>All Python files from the GitHub dataset hosted by Google BigQuery [16].</td></tr></table>

their specific requirements. Table I provides a comprehensive summary of the languages extracted. The third column lists the number of files collected after filtering based on file size and word count. The last column indicates the number of files obtained after removing exact duplicates within our dataset, with exact and near duplicates from other datasets flagged among the remaining files. For more detailed information on the dataset creation process, please refer to the dataset page3.

# A. Datasets

Our selection of datasets for deduplication is based on previously curated lists [10], with the addition of The Stack V2 [3], which is the only new dataset that has been released since the publication of previous works. We give an overview of all potential datasets in Table III. Due to the comment removal being based on the programming languages of the files, we are not able to infer the correct language for two datasets. The Pile [6], which has been removed and re-uploaded, has lost information about the programming language of a file. Furthermore, due to a known issue with the curation of CodeClippy4, the languages and names of files are misaligned in the dataset. We also exclude this dataset from deduplication. Although we could predict the languages used in the files in these datasets, the tools that provide this functionality do return incorrect predictions, which could result in a duplicate not being removed. As we aim to provide a guarantee that there is no data contamination in our dataset, we remove these two datasets from consideration.

We identify and flag duplicates between our dataset and all publicly available datasets to facilitate a more flexible approach to LLM evaluation, prioritizing both reproducibility and ease of use. This setup minimizes time and computational overhead by removing the burden of duplicate detection from researchers. Users can seamlessly filter data by language or by exact and near-duplicate files, tailoring the dataset to

# IV. LAYOUT

The Heap is organized into multiple subsets, each of them corresponding to one programming language. In each subset, the entries included in the dataset can be summarized into 3 groups: file content and metadata, quality indicators, and duplicates. We give an example of one entry in Figure 1.

a) File Content and Metadata: For the file content and metadata, we list the actual content of the file, which is the main information to be used in downstream tasks. We also include information about filename and path, as this has been included in the pre-training procedure of some LLMs [3], [11], [17].

b) Quality Indicators: To facilitate the selection of files for downstream use, we incorporated several quality indicators previously utilized in related works, ensuring the dataset can be easily filtered and selected. We included numerical statistics about the file such as the total lines, avg line length, max line length and alphanumeric fraction, as well as repository-wide statistics such as repo stars, repo forks, open issues and the extraction date of the repo. The repository star count will be artificially inflated for languages where more than 50, 000 repositories exist, due to the ordering of the repositories in the collection steps.

c) Duplicates: As we deduplicate The Heap with respect to a number of other publicly available datasets, we incorporate two columns for every dataset. One column contains a Boolean value, whether there is an exact duplicate of the given file in the dataset, and the other column contains a Boolean value describing whether there is a near duplicate of the given file in the dataset. We choose not to remove files but to use a Boolean mask in order to maximize the amount of available data for each available dataset.

# V. FUTURE IMPROVEMENTS

In future iterations of this dataset, several potential improvements could be made. These include enhancing the deduplication process, releases of new training datasets, providing detailed information about the natural languages represented in the dataset, and tracking the evolution of codebases.

a) New Datasets: The main goal of this dataset is to reduce the burden of deduplicating a dataset used for downstream tasks for future research. This is only effective if the dataset is deduplicated against all available datasets. As new datasets are released we intend to pass them through the same pipeline to ensure The Heap remains relevant for the future.

b) Deduplication: We addressed the deduplication of datasets using two widely adopted methods: exact deduplication based on hashing and near-deduplication leveraging locality-sensitive hashing. However, there is limited research on what constitutes an effective deduplication strategy. There could be issues with duplicates at a lower granularity level than file-based deduplications, as well as possible issues with the provenance of code fragments. Once studies are conducted on the impact of various deduplication approaches, we plan to incorporate these strategies as a new entry in the dataset.

c) Cleaning: We include all files that we scraped that were not duplicates, while this gives us a dataset of deduplicated files, there is still the question of file “quality”. In NLP research, keywords have been used for filtering websites, such as lorem ipsum or TODOs [18], and code datasets have been cleaned of autogenerated files using a similar approach [3]. We believe that this may also affect the quality of code datasets. Specifically, languages that rely heavily on boiler plating, such as Java, may benefit from removing certain common phrases from their corpus. This will be included as a further filtering step in a future release of the dataset.

d) Topic Modeling: While languages can be used to loosely select an area that is being analyzed (Mathematica for mathematics, or JavaScript for web-based projects), many languages can be used in multiple specializations/areas. Adopting the FineWeb topic modeling approach for code datasets would create interesting annotations for the code files, as well as show any form of topic-based imbalances in the dataset.

e) Natural Language: An under-explored research area involves the presence of multiple natural languages within code. As natural languages are often mixed within one file [19], we plan to adopt a Parts of Speech-like tagging [20] system for the natural languages present in each file. This can give information about the performance of code LLMs when the code is not in English. This will both help the development of non-English code LLMs, as well as aid English-focused LLMs, as they can be evaluated on only English.

# VI. LIMITATIONS/CHALLENGES

The limitations and challenges faced by this dataset are twofold. First, other actors may decide to train their models on this data, removing the benefits, and second, developers may object to their code being present in this dataset. We address these problems as follows.

a) Training: In order to use The Heap for a fair evaluation of an LLM, the researcher must be sure that the target LLM has not been trained on The Heap. Aside from our deduplication ensuring this fact for current existing LLMs, our collection process also adds a layer of protection from the inclusion of The Heap in the training procedure. The trend of training LLMs has shifted to only training on permissively licensed data, which would exclude The Heap. Furthermore, the restriction of The Heap to research only, alleviates the problems with author attribution in LLM generations as trained models are not intended to be used by end-users [10], [21].

Furthermore, existing works such as membership inference attacks, have been extended to the scale of entire datasets [22]. This should make it possible in the near future to retroactively test for the inclusion of The Heap in the training procedures of a model.

b) Ethics: With the rapid rise of public repositories being used to train code language models, many authors of older repositories were unaware that their code could be utilized for such purposes, leaving them unable to opt-out. Moreover, there is currently no consensus on how developers can opt in or out of having their code included in datasets. We acknowledge these ethical concerns regarding the use of code in deep learning practices and offer the ability for repository owners to opt out of having their code included in our dataset. Although this approach is not ideal, as it places the burden of exclusion on the authors, it aligns with the current best practices [3].

# VII. CONCLUSION

We present The Heap, a multilingual dataset of source code that we deduplicated against datasets commonly used in the (pre-)training of large language models. The Heap enables researchers to conduct investigations into the behavior and performance of code large language models without the need to perform extensive deduplication with other datasets. This addresses the shortcomings of LLM investigations not testing for data leakage in $90 \%$ of all investigations [4] allowing for more robust conclusions to be made.

We release the dataset (only for research purposes) and outline a road map for future features such as natural language annotation, topic annotations, and further cleaning procedures to be incorporated into the dataset, to make higher-quality evaluations easier and more available for all researchers.

# REFERENCES

[1] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu ˜noz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code, 2022.   
[2] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.   
[3] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024.   
[4] Antonio Vitale, Rocco Oliveto, and Simone Scalabrino. A catalog of data smells for coding tasks. ACM Trans. Softw. Eng. Methodol., December 2024. Just Accepted.   
[5] Denis Kocetkov, Raymond Li, Loubna Ben allal, Jia LI, Chenghao Mou, Yacine Jernite, Margaret Mitchell, Carlos Mu ˜noz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro Von Werra, and Harm de Vries. The stack: 3 TB of permissively licensed source code. Transactions on Machine Learning Research, 2023.   
[6] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   
[7] Thomas Wolf, Leandro von Werra, and Lewis Tunstall. Codeparrot dataset. https://huggingface.co/datasets/transformersbook/codeparrot.   
[8] github-code. https://huggingface.co/datasets/codeparrot/github-code.   
[9] Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Vi´egas, and Martin Wattenberg. An interpretability illusion for bert. arXiv preprint arXiv:2104.07143, 2021.   
[10] Jonathan Katzy, Razvan Popescu, Arie Van Deursen, and Maliheh Izadi. An exploratory investigation into code license infringements in large language model training datasets. In Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering, FORGE ’24, page 74–85, New York, NY, USA, 2024. Association for Computing Machinery.   
[11] Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason T Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu ˜noz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra, and Harm de Vries. Starcoder: may the source be with you! Transactions on Machine Learning Research, 2023. Reproducibility Certification.   
[12] Jure Leskovec, Anand Rajaraman, and Jeffrey David Ullman. Mining of Massive Datasets. Cambridge University Press, USA, 2nd edition, 2014.   
[13] Miltiadis Allamanis. The adverse effects of code duplication in machine learning models of code. In Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, Onward! 2019, page 143–153, New York, NY, USA, 2019. Association for Computing Machinery.   
[14] software heritage. https://docs.softwareheritage.org/index.html.   
[15] gharchive. https://www.gharchive.org/.   
[16] google bigquery. https://cloud.google.com/bigquery/public-data.   
[17] CodeGemma Team, Heri Zhao, Jeffrey Hui, Joshua Howland, Nam Nguyen, Siqi Zuo, Andrea Hu, Christopher A Choquette-Choo, Jingyue Shen, Joe Kelley, et al. Codegemma: Open code models based on gemma. arXiv preprint arXiv:2406.11409, 2024.   
[18] Jesse Dodge, Maarten Sap, Ana Marasovi´c, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. arXiv preprint arXiv:2104.08758, 2021.   
[19] Timo Pawelka and Elmar Juergens. Is this code written in english? a study of the natural language of comments and identifiers in practice. In 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME), pages 401–410. IEEE, 2015.   
[20] Alebachew Chiche and Betselot Yitagesu. Part of speech tagging: a systematic review of deep learning and machine learning approaches. Journal of Big Data, 9(1):10, 2022.   
[21] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. The data provenance initiative: A large scale audit of dataset licensing & attribution in ai. arXiv preprint arXiv:2310.16787, 2023.   
[22] Pratyush Maini, Hengrui Jia, Nicolas Papernot, and Adam Dziedzic. Llm dataset inference: Did you train on my dataset? arXiv preprint arXiv:2406.06443, 2024.