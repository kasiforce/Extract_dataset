{"id": "2511.17368", "title": "Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software", "abstract": "Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.", "arxiv_url": "https://arxiv.org/abs/2511.17368", "authors": ["Eric L. Melin", "Ahmed Musa Awon", "Nasir U. Eisty", "Neil A. Ernst", "Shurui Zhou"], "first_author": "Eric L. Melin", "primary_category": "cs.SE", "tag": ["Code Quality Analysis"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17368v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-11-24 16:01:06"}
{"id": "2511.17330", "title": "Agentic Program Verification", "abstract": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.   In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.   Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.", "arxiv_url": "https://arxiv.org/abs/2511.17330", "authors": ["Haoxin Tu", "Huan Zhao", "Yahui Song", "Mehtab Zafar", "Ruijie Meng", "Abhik Roychoudhury"], "first_author": "Haoxin Tu", "primary_category": "cs.SE", "tag": ["Program Verification"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17330v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-11-24 16:01:18"}
{"id": "2511.17262", "title": "SlsReuse: LLM-Powered Serverless Function Reuse", "abstract": "Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.   This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.", "arxiv_url": "https://arxiv.org/abs/2511.17262", "authors": ["Jinfeng Wen", "Yuehan Sun"], "first_author": "Jinfeng Wen", "primary_category": "cs.SE", "tag": ["Code Retrieval and Reuse"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17262v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-11-24 16:01:47"}
{"id": "2511.17027", "title": "ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting", "abstract": "Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.", "arxiv_url": "https://arxiv.org/abs/2511.17027", "authors": ["Zhijie Chen", "Xiang Chen", "Ziming Li", "Jiacheng Xue", "Chaoyang Gao"], "first_author": "Zhijie Chen", "primary_category": "cs.SE", "tag": ["Code Prompting"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17027v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-11-24 16:02:30"}
{"id": "2511.16858", "title": "Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair", "abstract": "Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.", "arxiv_url": "https://arxiv.org/abs/2511.16858", "authors": ["Toufique Ahmed", "Jatin Ganhotra", "Avraham Shinnar", "Martin Hirzel"], "first_author": "Toufique Ahmed", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16858v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-11-24 16:02:47"}
{"id": "2511.16787", "title": "NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction to Python Code Generation", "abstract": "This paper presents JGU Mainz's winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions. We propose a multi-agent-based pipeline. First, a code-generation agent produces an initial solution from the input instruction. The candidate program is then executed against the provided unit tests (pytest-style, assert-based). Only the failing cases are forwarded to a debugger agent, which reruns the tests, extracts error traces, and, conditioning on the error messages, the current program, and the relevant test cases, generates a revised solution. Using this approach, our submission achieved first place in the shared task with a $Pass@1$ score of 95.4. We also make our code public.", "arxiv_url": "https://arxiv.org/abs/2511.16787", "authors": ["Hossain Shaikh Saadi", "Faria Alam", "Mario Sanz-Guerrero", "Minh Duc Bui", "Manuel Mager", "Katharina von der Wense"], "first_author": "Hossain Shaikh Saadi", "primary_category": "cs.CL", "tag": ["Code Translation"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16787v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-11-24 16:02:59"}
{"id": "2511.16395", "title": "CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference", "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs). However, their inherent tendency toward hallucination often introduces functional errors into the generated HDL designs. To address this issue, we propose the framework CorrectHDL that leverages high-level synthesis (HLS) results as functional references to correct potential errors in LLM-generated HDL designs.The input to the proposed framework is a C/C++ program that specifies the target circuit's functionality. The program is provided to an LLM to directly generate an HDL design, whose syntax errors are repaired using a Retrieval-Augmented Generation (RAG) mechanism. The functional correctness of the LLM-generated circuit is iteratively improved by comparing its simulated behavior with an HLS reference design produced by conventional HLS tools, which ensures the functional correctness of the result but can lead to suboptimal area and power efficiency. Experimental results demonstrate that circuits generated by the proposed framework achieve significantly better area and power efficiency than conventional HLS designs and approach the quality of human-engineered circuits. Meanwhile, the correctness of the resulting HDL implementation is maintained, highlighting the effectiveness and potential of agentic HDL design leveraging the generative capabilities of LLMs and the rigor of traditional correctness-driven IC design flows.", "arxiv_url": "https://arxiv.org/abs/2511.16395", "authors": ["Kangwei Xu", "Grace Li Zhang", "Ulf Schlichtmann", "Bing Li"], "first_author": "Kangwei Xu", "primary_category": "cs.AI", "tag": ["Code Translation"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16395v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-11-24 16:03:33"}
{"id": "2511.16383", "title": "An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models", "abstract": "Recently, using Large Language Models (LLMs) to generate optimization models from natural language descriptions has became increasingly popular. However, a major open question is how to validate that the generated models are correct and satisfy the requirements defined in the natural language description. In this work, we propose a novel agent-based method for automatic validation of optimization models that builds upon and extends methods from software testing to address optimization modeling . This method consists of several agents that initially generate a problem-level testing API, then generate tests utilizing this API, and, lastly, generate mutations specific to the optimization model (a well-known software testing technique assessing the fault detection power of the test suite). In this work, we detail this validation framework and show, through experiments, the high quality of validation provided by this agent ensemble in terms of the well-known software testing measure called mutation coverage.", "arxiv_url": "https://arxiv.org/abs/2511.16383", "authors": ["Alexander Zadorojniy", "Segev Wasserkrug", "Eitan Farchi"], "first_author": "Alexander Zadorojniy", "primary_category": "cs.AI", "tag": ["Code Testing"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16383v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-11-24 16:03:44"}
{"id": "2511.16224", "title": "Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts", "abstract": "Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.", "arxiv_url": "https://arxiv.org/abs/2511.16224", "authors": ["Francesco Salzano", "Simone Scalabrino", "Rocco Oliveto", "Remo Pareschi"], "first_author": "Francesco Salzano", "primary_category": "cs.SE", "tag": ["Code Completion"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16224v2", "published": "2025-11-20", "update_time": "2025-11-21", "download_time": "2025-11-24 16:03:59"}
{"id": "2511.16123", "title": "Domain-constrained Synthesis of Inconsistent Key Aspects in Textual Vulnerability Descriptions", "abstract": "Textual Vulnerability Descriptions (TVDs) are crucial for security analysts to understand and address software vulnerabilities. However, the key aspect inconsistencies in TVDs from different repositories pose challenges for achieving a comprehensive understanding of vulnerabilities. Existing approaches aim to mitigate inconsistencies by aligning TVDs with external knowledge bases, but they often discard valuable information and fail to synthesize comprehensive representations. In this paper, we propose a domain-constrained LLM-based synthesis framework for unifying key aspects of TVDs. Our framework consists of three stages: 1) Extraction, guided by rule-based templates to ensure all critical details are captured; 2) Self-evaluation, using domain-specific anchor words to assess semantic variability across sources; and 3) Fusion, leveraging information entropy to reconcile inconsistencies and prioritize relevant details. This framework improves synthesis performance, increasing the F1 score for key aspect augmentation from 0.82 to 0.87, while enhancing comprehension and efficiency by over 30\\%. We further develop Digest Labels, a practical tool for visualizing TVDs, which human evaluations show significantly boosts usability.", "arxiv_url": "https://arxiv.org/abs/2511.16123", "authors": ["Linyi Han", "Shidong Pan", "Zhenchang Xing", "Sofonias Yitagesu", "Xiaowang Zhang", "Zhiyong Feng", "Jiamou Sun", "Qing Huang"], "first_author": "Linyi Han", "primary_category": "cs.SE", "tag": ["Code Summarization"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16123v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-11-24 16:04:14"}
{"id": "2511.16092", "title": "The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report", "abstract": "Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report", "arxiv_url": "https://arxiv.org/abs/2511.16092", "authors": ["Xing Hu", "Raula Gaikovina Kula", "Christoph Treude"], "first_author": "Xing Hu", "primary_category": "cs.SE", "tag": ["AI-Assisted Development Environments"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16092v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-11-24 16:04:23"}
{"id": "2511.16708", "title": "Multi-Agent Code Verification with Compound Vulnerability Detection", "abstract": "LLMs generate buggy code: 29.6% of SWE-bench \"solved\" patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, confirmed by measuring agent correlation of p = 0.05--0.25. We also show that multiple vulnerabilities in the same code create exponentially more risk than previously thought--SQL injection plus exposed credentials creates 15x more danger (risk 300 vs. 20) than traditional models predict. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution. We tested 15 different agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with gains of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4. The best two-agent combination reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.", "arxiv_url": "https://arxiv.org/abs/2511.16708", "authors": ["Shreshth Rajan"], "first_author": "Shreshth Rajan", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16708v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-11-24 16:04:36"}
{"id": "2511.16005", "title": "InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution", "abstract": "Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \\texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.", "arxiv_url": "https://arxiv.org/abs/2511.16005", "authors": ["Qingao Dong", "Mengfei Wang", "Hengzhi Zhang", "Zhichao Li", "Yuan Yuan", "Mu Li", "Xiang Gao", "Hailong Sun", "Chunming Hu", "Weifeng Lv"], "first_author": "Qingao Dong", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16005v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-11-24 16:04:48"}
{"id": "2511.16004", "title": "InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution", "abstract": "Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.", "arxiv_url": "https://arxiv.org/abs/2511.16004", "authors": ["KeFan Li", "Mengfei Wang", "Hengzhi Zhang", "Zhichao Li", "Yuan Yuan", "Mu Li", "Xiang Gao", "Hailong Sun", "Chunming Hu", "Weifeng Lv"], "first_author": "KeFan Li", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16004v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-11-24 16:04:59"}
{"id": "2511.15817", "title": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code", "abstract": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.   This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.", "arxiv_url": "https://arxiv.org/abs/2511.15817", "authors": ["Alejandro Velasco", "Daniel Rodriguez-Cardenas", "Dipin Khati", "David N. Palacio", "Luftar Rahman Alif", "Denys Poshyvanyk"], "first_author": "Alejandro Velasco", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15817v2", "published": "2025-11-19", "update_time": "2025-11-21", "download_time": "2025-11-24 16:05:36"}
{"id": "2511.15665", "title": "Quantum-Guided Test Case Minimization for LLM-Based Code Generation", "abstract": "Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.", "arxiv_url": "https://arxiv.org/abs/2511.15665", "authors": ["Huixiang Zhang", "Mahzabeen Emu"], "first_author": "Huixiang Zhang", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": false, "conference": "This is a preprint version, full paper has been accepted in IEEE CASCON 2025 and...", "pdf_url": "https://arxiv.org/pdf/2511.15665v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-11-24 16:05:46"}
{"id": "2511.15403", "title": "MutDafny: A Mutation-Based Approach to Assess Dafny Specifications", "abstract": "This paper explores the use of mutation testing to reveal weaknesses in formal specifications written in Dafny. In verification-aware programming languages, such as Dafny, despite their critical role, specifications are as prone to errors as implementations. Flaws in specs can result in formally verified programs that deviate from the intended behavior.   We present MutDafny, a tool that increases the reliability of Dafny specifications by automatically signaling potential weaknesses. Using a mutation testing approach, we introduce faults (mutations) into the code and rely on formal specifications for detecting them. If a program with a mutant verifies, this may indicate a weakness in the specification. We extensively analyze mutation operators from popular tools, identifying the ones applicable to Dafny. In addition, we synthesize new operators tailored for Dafny from bugfix commits in publicly available Dafny projects on GitHub. Drawing from both, we equipped our tool with a total of 32 mutation operators. We evaluate MutDafny's effectiveness and efficiency in a dataset of 794 real-world Dafny programs and we manually analyze a subset of the resulting undetected mutants, identifying five weak real-world specifications (on average, one at every 241 lines of code) that would benefit from strengthening.", "arxiv_url": "https://arxiv.org/abs/2511.15403", "authors": ["Isabel Amaral", "Alexandra Mendes", "Jos√© Campos"], "first_author": "Isabel Amaral", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15403v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-11-24 16:06:15"}
{"id": "2511.15293", "title": "A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development", "abstract": "Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.", "arxiv_url": "https://arxiv.org/abs/2511.15293", "authors": ["Jia Li", "Zhi Jin", "Kechi Zhang", "Huangzhao Zhang", "Jiaru Qian", "Tiankuo Zhao"], "first_author": "Jia Li", "primary_category": "cs.SE", "tag": ["Automated Software Development"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15293v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-11-24 16:06:36"}
{"id": "2511.15757", "title": "Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym", "abstract": "Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on userspace applications and overlook the complexities of kernel-space debugging and repair. The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions. Prior efforts such as KGym and CrashFixer have highlighted the difficulty of APR in this domain, reporting low success rates or relying on costly and complex pipelines and pricey cloud infrastructure. In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. Built on RGym, we propose a simple yet effective APR pipeline leveraging specialized localization techniques (e.g., call stacks and blamed commits) to overcome the unrealistic usage of oracles in KGym. We test on a filtered and verified dataset of 143 bugs. Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug. We further conduct an ablation study to analyze contributions from our proposed localization strategy, prompt structure, and model choice, and demonstrate that feedback-based retries can significantly enhance success rates.", "arxiv_url": "https://arxiv.org/abs/2511.15757", "authors": ["Kareem Shehada", "Yifan Wu", "Wyatt D. Feng", "Adithya Iyer", "Gryphon Kumfert", "Yangruibo Ding", "Zhiyun Qian"], "first_author": "Kareem Shehada", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": true, "conference": "NeurIPS 2025", "pdf_url": "https://arxiv.org/pdf/2511.15757v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-11-24 16:06:58"}
{"id": "2511.15229", "title": "From Code Smells to Best Practices: Tackling Resource Leaks in PyTorch, TensorFlow, and Keras", "abstract": "Much of the existing ML research focuses on model performance metrics, leaving limited attention to the long-term sustainability and resource efficiency of ML applications. While high performance is essential, ensuring efficient resource management is equally critical for robust deployment. This study addresses this gap by systematically identifying code smells that lead to resource leaks in ML applications. We conducted an empirical investigation of developer discussions and real-world code snippets from PyTorch, TensorFlow, and Keras. The analysis identified 30 PyTorch-related smells and 16 TensorFlow/Keras smells linked to resource leaks. These smells were categorized in two ways: (1) based on their root causes, and (2) as general ML smells with framework-specific characteristics. For each smell, we derived at least one best practice, resulting in 50 recommended coding patterns aimed at reducing resource leakage and improving efficiency. To ensure the validity of our findings, we employed a three-phase validation process involving independent analysis by three authors followed by consensus discussions. This is the first comprehensive study to examine resource-leak-inducing code smells across major ML frameworks and to present actionable best practices for mitigating them. The contributions support developers in building more efficient and sustainable ML applications and offer a structured view of the underlying causes of resource leaks.", "arxiv_url": "https://arxiv.org/abs/2511.15229", "authors": ["Bashar Abdallah", "Martyna E. Wojciechowska", "Gustavo Santos", "Edmand Yu", "Maxime Lamothe", "Alain Abran", "Mohammad Hamdaqa"], "first_author": "Bashar Abdallah", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15229v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-11-24 16:07:09"}
{"id": "2511.17417", "title": "CREST: Improving Interpretability and Effectiveness of Troubleshooting at Ericsson through Criterion-Specific Trouble Report Retrieval", "abstract": "The rapid evolution of the telecommunication industry necessitates efficient troubleshooting processes to maintain network reliability, software maintainability, and service quality. Trouble Reports (TRs), which document issues in Ericsson's production system, play a critical role in facilitating the timely resolution of software faults. However, the complexity and volume of TR data, along with the presence of diverse criteria that reflect different aspects of each fault, present challenges for retrieval systems. Building on prior work at Ericsson, which utilized a two-stage workflow, comprising Initial Retrieval (IR) and Re-Ranking (RR) stages, this study investigates different TR observation criteria and their impact on the performance of retrieval models. We propose \\textbf{CREST} (\\textbf{C}riteria-specific \\textbf{R}etrieval via \\textbf{E}nsemble of \\textbf{S}pecialized \\textbf{T}R models), a criterion-driven retrieval approach that leverages specialized models for different TR fields to improve both effectiveness and interpretability, thereby enabling quicker fault resolution and supporting software maintenance. CREST utilizes specialized models trained on specific TR criteria and aggregates their outputs to capture diverse and complementary signals. This approach leads to enhanced retrieval accuracy, better calibration of predicted scores, and improved interpretability by providing relevance scores for each criterion, helping users understand why specific TRs were retrieved. Using a subset of Ericsson's internal TRs, this research demonstrates that criterion-specific models significantly outperform a single model approach across key evaluation metrics. This highlights the importance of all targeted criteria used in this study for optimizing the performance of retrieval systems.", "arxiv_url": "https://arxiv.org/abs/2511.17417", "authors": ["Soroush Javdan", "Pragash Krishnamoorthy", "Olga Baysal"], "first_author": "Soroush Javdan", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17417v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-11-24 09:54:12"}
{"id": "2511.17131", "title": "UI-CUBE: Enterprise-Grade Computer Use Agent Benchmarking Beyond Task Accuracy to Operational Reliability", "abstract": "While current Computer Use Agent (CUA) benchmarks measure task completion effectively, they provide limited assessment of enterprise deployment readiness, emphasizing functional correctness over the operational reliability required for production systems. We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs. Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks), with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state. Evaluation of five state-of-the-art models reveals a sharp capability cliff rather than gradual performance degradation. Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%. Human evaluators with no prior application experience achieve only 61.2% on complex tasks despite near-perfect performance on simple tasks, establishing realistic performance ceilings. This discontinuous performance pattern -- where agents achieve 68-87% of human performance on simple tasks but only 15-32% on complex workflows -- indicates fundamental architectural limitations in memory management, hierarchical planning, and state coordination rather than incremental capability gaps addressable through better training or prompting. UI-CUBE functions as an enterprise-readiness diagnostic, revealing that while current CUAs can manipulate individual interface elements, they cannot yet function as reliable workflow automation tools. These findings provide architectural insights essential for developing production-ready CUAs capable of managing complex, multi-step enterprise processes.", "arxiv_url": "https://arxiv.org/abs/2511.17131", "authors": ["Horia Cristescu", "Charles Park", "Trong Canh Nguyen", "Sergiu Talmacel", "Alexandru-Gabriel Ilie", "Stefan Adam"], "first_author": "Horia Cristescu", "primary_category": "cs.SE", "tag": ["Agentic UI Automation Evaluation"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17131v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-11-24 09:55:38"}
{"id": "2511.15168", "title": "Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework", "abstract": "Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.", "arxiv_url": "https://arxiv.org/abs/2511.15168", "authors": ["Nguyen-Khang Le", "Hiep Nguyen", "Ngoc-Minh Nguyen", "Son T. Luu", "Trung Vo", "Quan Minh Bui", "Shoshin Nomura", "Le-Minh Nguyen"], "first_author": "Nguyen-Khang Le", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": true, "conference": "Proceedings of KSE 2025", "pdf_url": "https://arxiv.org/pdf/2511.15168v2", "published": "2025-11-19", "update_time": "2025-11-20", "download_time": "2025-11-25 01:01:11"}
{"id": "2511.15755", "title": "Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response", "abstract": "Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.", "arxiv_url": "https://arxiv.org/abs/2511.15755", "authors": ["Philip Drammeh"], "first_author": "Philip Drammeh", "primary_category": "cs.AI", "tag": ["Code Debug"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15755v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-11-25 01:01:22"}
{"id": "2511.19427", "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering", "abstract": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.", "arxiv_url": "https://arxiv.org/abs/2511.19427", "authors": ["Jayanaka L. Dantanarayana", "Savini Kashmira", "Thakee Nathees", "Zichen Zhang", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "first_author": "Jayanaka L. Dantanarayana", "primary_category": "cs.SE", "tag": ["Code Prompting"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19427v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-11-26 00:56:39"}
{"id": "2511.20617", "title": "Translating Large-Scale C Repositories to Idiomatic Rust", "abstract": "Existing C to Rust translation techniques fail to balance quality and scalability: transpilation-based approaches scale to large projects but produce code with poor safety, idiomaticity, and readability. In contrast, LLM-based techniques are prohibitively expensive due to their reliance on frontier models (without which they cannot reliably generate compilable translations), thus limiting scalability. This paper proposes Rustine, a fully automated pipeline for effective and efficient repository-level C to idiomatic safe Rust translation. Evaluating on a diverse set of 23 C programs, ranging from 27 to 13,200 lines of code, Rustine can generate fully compilable Rust code for all and achieve 87% functional equivalence (passing 1,063,099 assertions out of 1,221,192 in test suites with average function and line coverage of 74.7% and 72.2%). Compared to six prior repository-level C to Rust translation techniques, the translations by Rustine are overall safer (fewer raw pointers, pointer arithmetic, and unsafe constructs), more idiomatic (fewer Rust linter violations), and more readable. When the translations cannot pass all tests to fulfill functional equivalence, human developers were able to complete the task in 4.5 hours, on average, using Rustine as debugging support.", "arxiv_url": "https://arxiv.org/abs/2511.20617", "authors": ["Saman Dehghan", "Tianran Sun", "Tianxiang Wu", "Zihan Li", "Reyhaneh Jabbarvand"], "first_author": "Saman Dehghan", "primary_category": "cs.SE", "tag": ["Code Translation"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.20617v1", "published": "2025-11-25", "update_time": "2025-11-25", "download_time": "2025-11-27 00:55:36"}
{"id": "2511.21382", "title": "Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead", "abstract": "Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.", "arxiv_url": "https://arxiv.org/abs/2511.21382", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "first_author": "Bei Chu", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21382v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-11-28 00:54:27"}
{"id": "2511.21380", "title": "Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions", "abstract": "Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.", "arxiv_url": "https://arxiv.org/abs/2511.21380", "authors": ["Jingyi Chen", "Xiaoyan Guo", "Songqiang Chen", "Shing-Chi Cheung", "Jiasi Shen"], "first_author": "Jingyi Chen", "primary_category": "cs.SE", "tag": ["Code Editing"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21380v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-11-29 00:55:02"}
{"id": "2511.21197", "title": "Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools", "abstract": "AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \\textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \\textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.", "arxiv_url": "https://arxiv.org/abs/2511.21197", "authors": ["Paolo Buono", "Mary Cerullo", "Stefano Cirillo", "Giuseppe Desolda", "Francesco Greco", "Emanuela Guglielmi", "Grazia Margarella", "Giuseppe Polese", "Simone Scalabrino", "Cesare Tucci"], "first_author": "Paolo Buono", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21197v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-11-30 01:03:31"}
{"id": "2511.21509", "title": "SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks", "abstract": "In the past two decades, significant research and development effort went into the development of verification tools for individual languages, such asC, C++, and Java. Many of the used verification approaches are in fact language-agnostic and it would be beneficial for the technology transfer to allow for using the implementations also for other programming and modeling languages. To address the problem, we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses. SV-LIBis based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program. This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already. Furthermore, SV-LIBdefines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses. This paper presents version 1.0 of the SV-LIBformat, including its design goals, the syntax, and informal semantics. Formal semantics and further extensions to concurrency are planned for future versions.", "arxiv_url": "https://arxiv.org/abs/2511.21509", "authors": ["Dirk Beyer", "Gidon Ernst", "Martin Jon√°≈°", "Marian Lingsch-Rosenfeld"], "first_author": "Dirk Beyer", "primary_category": "cs.PL", "tag": ["Code Verification"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21509v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-01 01:07:41"}
{"id": "2511.23408", "title": "Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities", "abstract": "Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.", "arxiv_url": "https://arxiv.org/abs/2511.23408", "authors": ["Aayush Garg", "Zanis Ali Khan", "Renzo Degiovanni", "Qiang Tang"], "first_author": "Aayush Garg", "primary_category": "cs.CR", "tag": ["Code Debug"], "benchmark": false, "conference": "41st ACM/SIGAPP Symposium on Applied Computing (SAC) Smarter Engineering-Building AI and Building with AI (SEAI) 2026", "pdf_url": "https://arxiv.org/pdf/2511.23408v1", "published": "2025-11-28", "update_time": "2025-11-28", "download_time": "2025-12-02 00:57:39"}
{"id": "2512.01939", "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks", "abstract": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.", "arxiv_url": "https://arxiv.org/abs/2512.01939", "authors": ["Yanlin Wang", "Xinyi Xu", "Jiachi Chen", "Tingting Bi", "Wenchao Gu", "Zibin Zheng"], "first_author": "Yanlin Wang", "primary_category": "cs.SE", "tag": ["Agent Frameworks & Developer Practices"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01939v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-03 00:57:19"}
{"id": "2512.02953", "title": "The Evolutionary Ecology of Software: Constraints, Innovation, and the AI Disruption", "abstract": "This chapter investigates the evolutionary ecology of software, focusing on the symbiotic relationship between software and innovation. An interplay between constraints, tinkering, and frequency-dependent selection drives the complex evolutionary trajectories of these socio-technological systems. Our approach integrates agent-based modeling and case studies, drawing on complex network analysis and evolutionary theory to explore how software evolves under the competing forces of novelty generation and imitation. By examining the evolution of programming languages and their impact on developer practices, we illustrate how technological artifacts co-evolve with and shape societal norms, cultural dynamics, and human interactions. This ecological perspective also informs our analysis of the emerging role of AI-driven development tools in software evolution. While large language models (LLMs) provide unprecedented access to information, their widespread adoption introduces new evolutionary pressures that may contribute to cultural stagnation, much like the decline of diversity in past software ecosystems. Understanding the evolutionary pressures introduced by AI-mediated software production is critical for anticipating broader patterns of cultural change, technological adaptation, and the future of software innovation.", "arxiv_url": "https://arxiv.org/abs/2512.02953", "authors": ["Sergi Valverde", "Blai Vidiella", "Salva Duran-Nebreda"], "first_author": "Sergi Valverde", "primary_category": "cs.SE", "tag": ["AI-mediated Software Evolution"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.02953v1", "published": "2025-12-02", "update_time": "2025-12-02", "download_time": "2025-12-04 00:57:36"}
{"id": "2512.03421", "title": "Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization", "abstract": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.", "arxiv_url": "https://arxiv.org/abs/2512.03421", "authors": ["Hexiang Xu", "Hengyuan Liu", "Yonghao Wu", "Xiaolan Kang", "Xiang Chen", "Yong Liu"], "first_author": "Hexiang Xu", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": true, "conference": "publication in The Journal of Systems & Software", "pdf_url": "https://arxiv.org/pdf/2512.03421v1", "published": "2025-12-03", "update_time": "2025-12-03", "download_time": "2025-12-05 00:58:23"}
{"id": "2512.05073", "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?", "abstract": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.", "arxiv_url": "https://arxiv.org/abs/2512.05073", "authors": ["Shashwat Shankar", "Subhranshu Pandey", "Innocent Dengkhw Mochahari", "Bhabesh Mali", "Animesh Basak Chowdhury", "Sukanta Bhattacharjee", "Chandan Karfa"], "first_author": "Shashwat Shankar", "primary_category": "cs.LG", "tag": ["Code Prompting"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.05073v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-06 00:55:11"}
{"id": "2512.04680", "title": "Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap", "abstract": "Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.", "arxiv_url": "https://arxiv.org/abs/2512.04680", "authors": ["Jialong Li", "Mingyue Zhang", "Nianyu Li", "Danny Weyns", "Zhi Jin", "Kenji Tei"], "first_author": "Jialong Li", "primary_category": "cs.SE", "tag": ["GenAI for Self-Adaptive Systems"], "benchmark": false, "conference": "ACM Transactions on Autonomous and Adaptive Systems", "pdf_url": "https://arxiv.org/pdf/2512.04680v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-07 01:03:43"}
{"id": "2512.04673", "title": "Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models", "abstract": "Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.", "arxiv_url": "https://arxiv.org/abs/2512.04673", "authors": ["Gunjan Das", "Paheli Bhattacharya", "Rishabh Gupta"], "first_author": "Gunjan Das", "primary_category": "cs.SE", "tag": ["Cross-Task Benchmarking and Evaluation"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04673v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-08 00:58:57"}
{"id": "2512.05908", "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures", "abstract": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.", "arxiv_url": "https://arxiv.org/abs/2512.05908", "authors": ["Amirkia Rafiei Oskooei", "S. Selcan Yukcu", "Mehmet Cevheri Bozoglan", "Mehmet S. Aktas"], "first_author": "Amirkia Rafiei Oskooei", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": false, "conference": "LLM4Code Workshop", "pdf_url": "https://arxiv.org/pdf/2512.05908v1", "published": "2025-12-05", "update_time": "2025-12-05", "download_time": "2025-12-09 00:58:03"}
{"id": "2512.07814", "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach", "abstract": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.", "arxiv_url": "https://arxiv.org/abs/2512.07814", "authors": ["Hua Yang", "Alejandro Velasco", "Sen Fang", "Bowen Xu", "Denys Poshyvanyk"], "first_author": "Hua Yang", "primary_category": "cs.SE", "tag": ["Code Pre-Training"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.07814v1", "published": "2025-12-08", "update_time": "2025-12-08", "download_time": "2025-12-10 00:59:23"}
{"id": "2512.08867", "title": "SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA", "abstract": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.", "arxiv_url": "https://arxiv.org/abs/2512.08867", "authors": ["Jing Zhang", "Lianghong Guo", "Yanlin Wang", "Mingwei Liu", "Jiachi Chen", "Yuchi Ma", "Ensheng Shi", "Terry Yue Zhuo", "Hongyu Zhang", "Zibin Zheng"], "first_author": "Jing Zhang", "primary_category": "cs.SE", "tag": ["Development Knowledge QA"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.08867v1", "published": "2025-12-09", "update_time": "2025-12-09", "download_time": "2025-12-11 00:59:49"}
{"id": "2512.09679", "title": "Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis", "abstract": "Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \\emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.", "arxiv_url": "https://arxiv.org/abs/2512.09679", "authors": ["Naizhu Jin", "Zhong Li", "Guang Yang", "Tian Zhang", "Qingkai Zeng"], "first_author": "Naizhu Jin", "primary_category": "cs.SE", "tag": ["Code Prompting"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.09679v1", "published": "2025-12-10", "update_time": "2025-12-10", "download_time": "2025-12-12 00:59:30"}
{"id": "2512.10713", "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code", "abstract": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.", "arxiv_url": "https://arxiv.org/abs/2512.10713", "authors": ["Itay Dreyfuss", "Antonio Abu Nassar", "Samuel Ackerman", "Axel Ben David", "Rami Katan", "Orna Raz", "Marcel Zalmanovici"], "first_author": "Itay Dreyfuss", "primary_category": "cs.SE", "tag": ["Code Evaluation/Benchmarking"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10713v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-13 00:56:08"}
{"id": "2512.10493", "title": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild", "abstract": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.", "arxiv_url": "https://arxiv.org/abs/2512.10493", "authors": ["Binquan Zhang", "Li Zhang", "Haoyuan Zhang", "Fang Liu", "Song Wang", "Bo Shen", "An Fu", "Lin Shi"], "first_author": "Binquan Zhang", "primary_category": "cs.SE", "tag": ["Code Prompting"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10493v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-14 01:03:59"}
{"id": "2512.10485", "title": "From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection", "abstract": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.", "arxiv_url": "https://arxiv.org/abs/2512.10485", "authors": ["Chaomeng Lu", "Bert Lagaisse"], "first_author": "Chaomeng Lu", "primary_category": "cs.CR", "tag": ["Code Testing"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10485v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-15 01:02:58"}
{"id": "2512.11589", "title": "A Study of Library Usage in Agent-Authored Pull Requests", "abstract": "Coding agents are becoming increasingly capable of completing end-to-end software engineering workflows that previously required a human developer, including raising pull requests (PRs) to propose their changes. However, we still know little about how these agents use libraries when generating code, a core part of real-world software development. To fill this gap, we study 26,760 agent-authored PRs from the AIDev dataset to examine three questions: how often do agents import libraries, how often do they introduce new dependencies (and with what versioning), and which specific libraries do they choose? We find that agents often import libraries (29.5% of PRs) but rarely add new dependencies (1.3% of PRs); and when they do, they follow strong versioning practices (75.0% specify a version), an improvement on direct LLM usage where versions are rarely mentioned. Generally, agents draw from a surprisingly diverse set of external libraries, contrasting with the limited \"library preferences\" seen in prior non-agentic LLM studies. Our results offer an early empirical view into how AI coding agents interact with today's software ecosystems.", "arxiv_url": "https://arxiv.org/abs/2512.11589", "authors": ["Lukas Twist"], "first_author": "Lukas Twist", "primary_category": "cs.SE", "tag": ["Agentic Code Generation"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.11589v1", "published": "2025-12-12", "update_time": "2025-12-12", "download_time": "2025-12-16 01:00:01"}
{"id": "2512.13515", "title": "Fine-tuned LLM-based Code Migration Framework", "abstract": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.", "arxiv_url": "https://arxiv.org/abs/2512.13515", "authors": ["Oleg Grynets", "Vasyl Lyashkevych", "Dmytro Baran", "Maksym Orliansky", "Taras Zelenyy", "Markiian Leshchyshyn"], "first_author": "Oleg Grynets", "primary_category": "cs.SE", "tag": ["Code Translation"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.13515v1", "published": "2025-12-15", "update_time": "2025-12-15", "download_time": "2025-12-17 00:55:40"}
{"id": "2512.14475", "title": "Teralizer: Semantics-Based Test Generalization from Conventional Unit Tests to Property-Based Tests", "abstract": "Conventional unit tests validate single input-output pairs, leaving most inputs of an execution path untested. Property-based testing addresses this shortcoming by generating multiple inputs satisfying properties but requires significant manual effort to define properties and their constraints. We propose a semantics-based approach that automatically transforms unit tests into property-based tests by extracting specifications from implementations via single-path symbolic analysis. We demonstrate this approach through Teralizer, a prototype for Java that transforms JUnit tests into property-based jqwik tests. Unlike prior work that generalizes from input-output examples, Teralizer derives specifications from program semantics.   We evaluated Teralizer on three progressively challenging datasets. On EvoSuite-generated tests for EqBench and Apache Commons utilities, Teralizer improved mutation scores by 1-4 percentage points. Generalization of mature developer-written tests from Apache Commons utilities showed only 0.05-0.07 percentage points improvement. Analysis of 632 real-world Java projects from RepoReapers highlights applicability barriers: only 1.7% of projects completed the generalization pipeline, with failures primarily due to type support limitations in symbolic analysis and static analysis limitations in our prototype. Based on the results, we provide a roadmap for future work, identifying research and engineering challenges that need to be tackled to advance the field of test generalization.   Artifacts available at: https://doi.org/10.5281/zenodo.17950381", "arxiv_url": "https://arxiv.org/abs/2512.14475", "authors": ["Johann Glock", "Clemens Bauer", "Martin Pinzger"], "first_author": "Johann Glock", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.14475v1", "published": "2025-12-16", "update_time": "2025-12-16", "download_time": "2025-12-18 00:56:41"}
{"id": "2512.15699", "title": "FrontierCS: Evolving Challenges for Evolving Intelligence", "abstract": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.", "arxiv_url": "https://arxiv.org/abs/2512.15699", "authors": ["Qiuyang Mang", "Wenhao Chai", "Zhifei Li", "Huanzhi Mao", "Shang Zhou", "Alexander Du", "Hanchen Li", "Shu Liu", "Edwin Chen", "Yichuan Wang", "Xieting Chu", "Zerui Cheng", "Yuan Xu", "Tian Xia", "Zirui Wang", "Tianneng Shi", "Jianzhu Yao", "Yilong Zhao", "Qizheng Zhang", "Charlie Ruan", "Zeyu Shen", "Kaiyuan Liu", "Runyuan He", "Dong Xing", "Zerui Li", "Zirong Zeng", "Yige Jiang", "Lufeng Cheng", "Ziyi Zhao", "Youran Sun", "Wesley Zheng", "Meiyuwang Zhang", "Ruyi Ji", "Xuechang Tu", "Zihan Zheng", "Zexing Chen", "Kangyang Zhou", "Zhaozi Wang", "Jingbang Chen", "Aleksandra Korolova", "Peter Henderson", "Pramod Viswanath", "Vijay Ganesh", "Saining Xie", "Zhuang Liu", "Dawn Song", "Sewon Min", "Ion Stoica", "Joseph E. Gonzalez", "Jingbo Shang", "Alvin Cheung"], "first_author": "Qiuyang Mang", "primary_category": "cs.LG", "tag": ["Code Benchmarking"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.15699v1", "published": "2025-12-17", "update_time": "2025-12-17", "download_time": "2025-12-19 00:59:23"}
{"id": "2512.16790", "title": "Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse", "abstract": "While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.", "arxiv_url": "https://arxiv.org/abs/2512.16790", "authors": ["Aaron Imani", "Mohammad Moshirpour", "Iftekhar Ahmed"], "first_author": "Aaron Imani", "primary_category": "cs.SE", "tag": ["Code Interpretability"], "benchmark": false, "conference": "ICSE", "pdf_url": "https://arxiv.org/pdf/2512.16790v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-20 00:56:12"}
{"id": "2512.16741", "title": "An Empirical Study of the Realism of Mutants in Deep Learning", "abstract": "Mutation analysis is a well-established technique for assessing test quality in the traditional software development paradigm by injecting artificial faults into programs. Its application to deep learning (DL) has expanded beyond classical testing to support tasks such as fault localization, repair, data generation, and model robustness evaluation. The core assumption is that mutants behave similarly to real faults, an assumption well established in traditional software systems but largely unverified for DL.   This study presents the first empirical comparison of pre-training and post-training mutation approaches in DL with respect to realism. We introduce a statistical framework to quantify their coupling strength and behavioral similarity to real faults using publicly available bugs datasets: CleanML, DeepFD, DeepLocalize, and defect4ML. Mutants are generated using state-of-the-art tools representing both approaches.   Results show that pre-training mutants exhibit consistently stronger coupling and higher behavioral similarity to real faults than post-training mutants, indicating greater realism. However, the substantial computational cost of pre-training mutation underscores the need for more effective post-training operators that match or exceed the realism demonstrated by pre-training mutants.", "arxiv_url": "https://arxiv.org/abs/2512.16741", "authors": ["Zaheed Ahmed", "Philip Makedonski", "Jens Grabowski"], "first_author": "Zaheed Ahmed", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.16741v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-21 01:03:33"}
{"id": "2512.16272", "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls", "abstract": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.   To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.   Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.", "arxiv_url": "https://arxiv.org/abs/2512.16272", "authors": ["Ora Nova Fandina", "Eitan Farchi", "Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Rami Katan", "Alice Podolsky"], "first_author": "Ora Nova Fandina", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.16272v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-22 01:02:53"}
{"id": "2512.17814", "title": "LLM-based Behaviour Driven Development for Hardware Design", "abstract": "Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.   Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.", "arxiv_url": "https://arxiv.org/abs/2512.17814", "authors": ["Rolf Drechsler", "Qian Liu"], "first_author": "Rolf Drechsler", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.17814v1", "published": "2025-12-19", "update_time": "2025-12-19", "download_time": "2025-12-23 00:59:13"}
{"id": "2512.19644", "title": "More code, less validation: Risk factors for over-reliance on AI coding tools among scientists", "abstract": "Programming is essential to modern scientific research, yet most scientists report inadequate training for the software development their work demands. Generative AI tools capable of code generation may support scientific programmers, but user studies indicate risks of over-reliance, particularly among inexperienced users. We surveyed 868 scientists who program, examining adoption patterns, tool preferences, and factors associated with perceived productivity. Adoption is highest among students and less experienced programmers, with variation across fields. Scientific programmers overwhelmingly prefer general-purpose conversational interfaces like ChatGPT over developer-specific tools. Both inexperience and limited use of development practices (like testing, code review, and version control) are associated with greater perceived productivity-but these factors interact, suggesting formal practices may partially compensate for inexperience. The strongest predictor of perceived productivity is the number of lines of generated code typically accepted at once. These findings suggest scientific programmers using generative AI may gauge productivity by code generation rather than validation, raising concerns about research code integrity.", "arxiv_url": "https://arxiv.org/abs/2512.19644", "authors": ["Gabrielle O'Brien", "Alexis Parker", "Nasir Eisty", "Jeffrey Carver"], "first_author": "Gabrielle O'Brien", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.19644v1", "published": "2025-12-22", "update_time": "2025-12-22", "download_time": "2025-12-24 00:58:43"}
{"id": "2512.20482", "title": "SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization", "abstract": "Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.", "arxiv_url": "https://arxiv.org/abs/2512.20482", "authors": ["Revanth Gangi Reddy", "Ye Liu", "Wenting Zhao", "JaeHyeok Doo", "Tarun Suresh", "Daniel Lee", "Caiming Xiong", "Yingbo Zhou", "Semih Yavuz", "Shafiq Joty"], "first_author": "Revanth Gangi Reddy", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.20482v1", "published": "2025-12-23", "update_time": "2025-12-23", "download_time": "2025-12-25 00:59:23"}
{"id": "2512.21238", "title": "Assessing the Software Security Comprehension of Large Language Models", "abstract": "Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.", "arxiv_url": "https://arxiv.org/abs/2512.21238", "authors": ["Mohammed Latif Siddiq", "Natalie Sekerak", "Antonio Karam", "Maria Leal", "Arvin Islam-Gomes", "Joanna C. S. Santos"], "first_author": "Mohammed Latif Siddiq", "primary_category": "cs.SE", "tag": ["Code Security Evaluation"], "benchmark": false, "conference": "Submitted to Empirical Software Engineering (EMSE) journal", "pdf_url": "https://arxiv.org/pdf/2512.21238v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-26 00:59:49"}
{"id": "2512.21236", "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking", "abstract": "Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.", "arxiv_url": "https://arxiv.org/abs/2512.21236", "authors": ["Yifan Huang", "Xiaojun Jia", "Wenbo Guo", "Yuqiang Sun", "Yihao Huang", "Chong Wang", "Yang Liu"], "first_author": "Yifan Huang", "primary_category": "cs.CR", "tag": ["Code Alignment"], "benchmark": true, "conference": "FSE 2026", "pdf_url": "https://arxiv.org/pdf/2512.21236v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-27 00:58:08"}
{"id": "2512.21028", "title": "Artificial or Just Artful? Do LLMs Bend the Rules in Programming?", "abstract": "Large Language Models (LLMs) are widely used for automated code generation, yet their apparent successes often mask a tension between pretraining objectives and alignment choices. While pretraining encourages models to exploit all available signals to maximize success, alignment, whether through fine-tuning or prompting, may restrict their use. This conflict is especially salient in agentic AI settings, for instance when an agent has access to unit tests that, although intended for validation, act as strong contextual signals that can be leveraged regardless of explicit prohibitions. In this paper, we investigate how LLMs adapt their code generation strategies when exposed to test cases under different prompting conditions. Using the BigCodeBench (Hard) dataset, we design five prompting conditions that manipulate test visibility and impose explicit or implicit restrictions on their use. We evaluate five LLMs (four open-source and one closed-source) across correctness, code similarity, program size, and code churn, and analyze cross-model consistency to identify recurring adaptation strategies. Our results show that test visibility dramatically alters performance, correctness nearly doubles for some models, while explicit restrictions or partial exposure only partially mitigate this effect. Beyond raw performance, we identify four recurring adaptation strategies, with test-driven refinement emerging as the most frequent. These results highlight how LLMs adapt their behavior when exposed to contextual signals that conflict with explicit instructions, providing useful insight into how models reconcile pretraining objectives with alignment constraints.", "arxiv_url": "https://arxiv.org/abs/2512.21028", "authors": ["Oussama Ben Sghaier", "Kevin Delcourt", "Houari Sahraoui"], "first_author": "Oussama Ben Sghaier", "primary_category": "cs.SE", "tag": ["Code Prompting"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.21028v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-28 01:07:08"}
{"id": "2512.20957", "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents", "abstract": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.", "arxiv_url": "https://arxiv.org/abs/2512.20957", "authors": ["Zhaoxi Zhang", "Yitong Duan", "Yanzhi Zhang", "Yiming Xu", "Jiyan He", "Yunfang Wu"], "first_author": "Zhaoxi Zhang", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.20957v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-29 01:04:48"}
{"id": "2512.22113", "title": "Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications", "abstract": "Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.", "arxiv_url": "https://arxiv.org/abs/2512.22113", "authors": ["Shengkun Cui", "Rahul Krishna", "Saurabh Jha", "Ravishankar K. Iyer"], "first_author": "Shengkun Cui", "primary_category": "cs.DC", "tag": ["Code Debug"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.22113v1", "published": "2025-12-26", "update_time": "2025-12-26", "download_time": "2025-12-30 00:59:44"}
{"id": "2512.23327", "title": "An Empirical Study of Generative AI Adoption in Software Engineering", "abstract": "Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.", "arxiv_url": "https://arxiv.org/abs/2512.23327", "authors": ["G√∂rkem Giray", "Onur Demir√∂rs", "Marcos Kalinowski", "Daniel Mendez"], "first_author": "G√∂rkem Giray", "primary_category": "cs.SE", "tag": ["Human Factors in LLM4SE"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.23327v1", "published": "2025-12-29", "update_time": "2025-12-29", "download_time": "2025-12-31 01:07:00"}
{"id": "2512.23214", "title": "Anka: A Domain-Specific Language for Reliable LLM Code Generation", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.", "arxiv_url": "https://arxiv.org/abs/2512.23214", "authors": ["Saif Khalfan Saif Al Mazrouei"], "first_author": "Saif Khalfan Saif Al Mazrouei", "primary_category": "cs.CL", "tag": ["Code Prompting"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.23214v1", "published": "2025-12-29", "update_time": "2025-12-29", "download_time": "2026-01-01 01:08:37"}
{"id": "2512.24858", "title": "Feature Slice Matching for Precise Bug Detection", "abstract": "Measuring the function similarity to detect bugs is effective, but the statements unrelated to the bugs can impede the performance due to the noise interference. Suppressing the noise interference in existing works does not manage the tough job, i.e., eliminating the noise in the targets. In this paper, we propose MATUS to mitigate the target noise for precise bug detection based on similarity measurement. Feature slices are extracted from both the buggy query and the targets to represent the semantic feature of (potential) bug logics. In particular, MATUS guides the target slicing with the prior knowledge from the buggy code, in an end-to-end way to pinpoint the slicing criterion in the targets. All feature slices are embedded and compared based on the vector similarity. Buggy candidates are audited to confirm unknown bugs in the targets. Experiments show that MATUS holds advantages in bug detection for real-world projects with acceptable efficiency. In total, MATUS has spotted 31 unknown bugs in the Linux kernel. All of them have been confirmed by the kernel developers, and 11 have been assigned CVEs.", "arxiv_url": "https://arxiv.org/abs/2512.24858", "authors": ["Ke Ma", "Jianjun Huang", "Wei You", "Bin Liang", "Jingzheng Wu", "Yanjun Wu", "Yuanjun Gong"], "first_author": "Ke Ma", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": false, "conference": "FSE2026", "pdf_url": "https://arxiv.org/pdf/2512.24858v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-02 01:01:14"}
{"id": "2512.24636", "title": "How Do Agentic AI Systems Deal With Software Energy Concerns? A Pull Request-Based Study", "abstract": "As Software Engineering enters its new era (SE 3.0), AI coding agents increasingly automate software development workflows. However, it remains unclear how exactly these agents recognize and address software energy concerns-an issue growing in importance due to large-scale data centers, energy-hungry language models, and battery-constrained devices. In this paper, we examined the energy awareness of agent-authored pull requests (PRs) using a publicly available dataset. We identified 216 energy-explicit PRs and conducted a thematic analysis, deriving a taxonomy of energy-aware work. Our further analysis of the applied optimization techniques shows that most align with established research recommendations. Although building and running these agents is highly energy intensive, encouragingly, the results indicate that they exhibit energy awareness when generating software artifacts. However, optimization-related PRs are accepted less frequently than others, largely due to their negative impact on maintainability.", "arxiv_url": "https://arxiv.org/abs/2512.24636", "authors": ["Tanjum Motin Mitul", "Md. Masud Mazumder", "Md Nahidul Islam Opu", "Shaiful Chowdhury"], "first_author": "Tanjum Motin Mitul", "primary_category": "cs.SE", "tag": ["Code Alignment"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24636v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-03 00:58:30"}
{"id": "2512.24656", "title": "Characterizing Bugs and Quality Attributes in Quantum Software: A Large-Scale Empirical Study", "abstract": "Quantum Software Engineering (QSE) is essential for ensuring the reliability and maintainability of hybrid quantum-classical systems, yet empirical evidence on how bugs emerge and affect quality in real-world quantum projects remains limited. This study presents the first ecosystem-scale longitudinal analysis of software defects across 123 open source quantum repositories from 2012 to 2024, spanning eight functional categories, including full-stack libraries, simulators, annealing, algorithms, compilers, assembly, cryptography, and experimental computing. Using a mixed method approach combining repository mining, static code analysis, issue metadata extraction, and a validated rule-based classification framework, we analyze 32,296 verified bug reports. Results show that full-stack libraries and compilers are the most defect-prone categories due to circuit, gate, and transpilation-related issues, while simulators are mainly affected by measurement and noise modeling errors. Classical bugs primarily impact usability and interoperability, whereas quantum-specific bugs disproportionately degrade performance, maintainability, and reliability. Longitudinal analysis indicates ecosystem maturation, with defect densities peaking between 2017 and 2021 and declining thereafter. High-severity defects cluster in cryptography, experimental computing, and compiler toolchains. Repositories employing automated testing detect more defects and resolve issues faster. A negative binomial regression further shows that automated testing is associated with an approximate 60 percent reduction in expected defect incidence. Overall, this work provides the first large-scale data-driven characterization of quantum software defects and offers empirical guidance for improving testing, documentation, and maintainability practices in QSE.", "arxiv_url": "https://arxiv.org/abs/2512.24656", "authors": ["Mir Mohammad Yousuf", "Shabir Ahmad Sofi"], "first_author": "Mir Mohammad Yousuf", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24656v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-04 01:07:26"}
{"id": "2512.24635", "title": "DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information", "abstract": "Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair.   To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs.", "arxiv_url": "https://arxiv.org/abs/2512.24635", "authors": ["Zhili Huang", "Ling Xu", "Chao Liu", "Weifeng Sun", "Xu Zhang", "Yan Lei", "Meng Yan", "Hongyu Zhang"], "first_author": "Zhili Huang", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24635v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-05 01:07:04"}
{"id": "2601.00753", "title": "Early-Stage Prediction of Review Effort in AI-Generated Pull Requests", "abstract": "As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?   Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).   We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.   Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.", "arxiv_url": "https://arxiv.org/abs/2601.00753", "authors": ["Dao Sy Duy Minh", "Huynh Trung Kiet", "Tran Chi Nguyen", "Nguyen Lam Phu Quy", "Phu Hoa Pham", "Nguyen Dinh Ha Duong", "Truong Bao Tran"], "first_author": "Dao Sy Duy Minh", "primary_category": "cs.SE", "tag": ["Code Review Triage"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.00753v1", "published": "2026-01-02", "update_time": "2026-01-02", "download_time": "2026-01-06 01:00:51"}
{"id": "2601.02345", "title": "Question Answering for Multi-Release Systems: A Case Study at Ciena", "abstract": "Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.", "arxiv_url": "https://arxiv.org/abs/2601.02345", "authors": ["Parham Khamsepour", "Mark Cole", "Ish Ashraf", "Sandeep Puri", "Mehrdad Sabetzadeh", "Shiva Nejati"], "first_author": "Parham Khamsepour", "primary_category": "cs.SE", "tag": ["Code Prompting"], "benchmark": true, "conference": "publication in SANER 2026", "pdf_url": "https://arxiv.org/pdf/2601.02345v1", "published": "2026-01-05", "update_time": "2026-01-05", "download_time": "2026-01-07 01:01:26"}
{"id": "2601.02971", "title": "Few-shot learning for security bug report identification", "abstract": "Security bug reports require prompt identification to minimize the window of vulnerability in software systems. Traditional machine learning (ML) techniques for classifying bug reports to identify security bug reports rely heavily on large amounts of labeled data. However, datasets for security bug reports are often scarce in practice, leading to poor model performance and limited applicability in real-world settings. In this study, we propose a few-shot learning-based technique to effectively identify security bug reports using limited labeled data. We employ SetFit, a state-of-the-art few-shot learning framework that combines sentence transformers with contrastive learning and parameter-efficient fine-tuning. The model is trained on a small labeled dataset of bug reports and is evaluated on its ability to classify these reports as either security-related or non-security-related. Our approach achieves an AUC of 0.865, at best, outperforming traditional ML techniques (baselines) for all of the evaluated datasets. This highlights the potential of SetFit to effectively identify security bug reports. SetFit-based few-shot learning offers a promising alternative to traditional ML techniques to identify security bug reports. The approach enables efficient model development with minimal annotation effort, making it highly suitable for scenarios where labeled data is scarce.", "arxiv_url": "https://arxiv.org/abs/2601.02971", "authors": ["Muhammad Laiq"], "first_author": "Muhammad Laiq", "primary_category": "cs.SE", "tag": ["Bug Report Classification"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.02971v1", "published": "2026-01-06", "update_time": "2026-01-06", "download_time": "2026-01-08 01:01:06"}
{"id": "2601.03988", "title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures", "abstract": "Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.   Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.   Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.", "arxiv_url": "https://arxiv.org/abs/2601.03988", "authors": ["Nicolas Lacroix", "Mireille Blay-Fornarino", "S√©bastien Mosser", "Frederic Precioso"], "first_author": "Nicolas Lacroix", "primary_category": "cs.SE", "tag": ["Code Summarization"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.03988v1", "published": "2026-01-07", "update_time": "2026-01-07", "download_time": "2026-01-09 01:02:06"}
{"id": "2601.04886", "title": "Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests", "abstract": "Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that descriptions claiming unimplemented changes was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5x longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.", "arxiv_url": "https://arxiv.org/abs/2601.04886", "authors": ["Jingzhi Gong", "Giovanni Pinna", "Yixin Bian", "Jie M. Zhang"], "first_author": "Jingzhi Gong", "primary_category": "cs.SE", "tag": ["Code Alignment"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.04886v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-10 00:59:31"}
{"id": "2601.04540", "title": "AdaptEval: A Benchmark for Evaluating Large Language Models on Code Snippet Adaptation", "abstract": "Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMs' performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation. Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, Practical Context. Tasks in AdaptEval are derived from developers' practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, Multi-granularity Annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, Fine-grained Evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs' performance across various individual adaptations. Based on AdaptEval, we conduct the first empirical study to evaluate six instruction-tuned LLMs and especially three reasoning LLMs on code snippet adaptation. Experimental results demonstrate that AdaptEval enables the assessment of LLMs' adaptation capabilities from various perspectives. It also provides critical insights into their current limitations, particularly their struggle to follow explicit instructions. We hope AdaptEval can facilitate further investigation and enhancement of LLMs' capabilities in code snippet adaptation, supporting their real-world applications.", "arxiv_url": "https://arxiv.org/abs/2601.04540", "authors": ["Tanghaoran Zhang", "Xinjun Mao", "Shangwen Wang", "Yuxin Zhao", "Yao Lu", "Jin Zhang", "Zhang Zhang", "Kang Yang", "Yue Yu"], "first_author": "Tanghaoran Zhang", "primary_category": "cs.SE", "tag": ["Code Editing"], "benchmark": true, "conference": "ASE 2025", "pdf_url": "https://arxiv.org/pdf/2601.04540v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-11 01:07:14"}
{"id": "2601.04556", "title": "4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering", "abstract": "We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked \"Why is completion rate 80%?\", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.", "arxiv_url": "https://arxiv.org/abs/2601.04556", "authors": ["Bo Yu", "Lei Zhao"], "first_author": "Bo Yu", "primary_category": "cs.SE", "tag": ["Code Prompting"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.04556v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-12 01:04:09"}
{"id": "2601.05827", "title": "SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking", "abstract": "Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.", "arxiv_url": "https://arxiv.org/abs/2601.05827", "authors": ["Zewei Lin", "Jiachi Chen", "Jingwen Zhang", "Zexu Wang", "Yuming Feng", "Weizhe Zhang", "Zibin Zheng"], "first_author": "Zewei Lin", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.05827v1", "published": "2026-01-09", "update_time": "2026-01-09", "download_time": "2026-01-13 00:57:09"}
{"id": "2601.07786", "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt", "abstract": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.", "arxiv_url": "https://arxiv.org/abs/2601.07786", "authors": ["Abdullah Al Mujahid", "Mia Mohammad Imran"], "first_author": "Abdullah Al Mujahid", "primary_category": "cs.SE", "tag": ["AI-Augmented Software Maintenance"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.07786v1", "published": "2026-01-12", "update_time": "2026-01-12", "download_time": "2026-01-14 01:03:04"}
{"id": "2601.08806", "title": "APEX-SWE", "abstract": "We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).", "arxiv_url": "https://arxiv.org/abs/2601.08806", "authors": ["Abhi Kottamasu", "Akul Datta", "Aakash Barthwal", "Chirag Mahapatra", "Ajay Arun", "Adarsh Hiremath", "Brendan Foody", "Bertie Vidgen"], "first_author": "Abhi Kottamasu", "primary_category": "cs.SE", "tag": ["Software Engineering Benchmarking"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.08806v1", "published": "2026-01-13", "update_time": "2026-01-13", "download_time": "2026-01-15 00:59:39"}
{"id": "2601.09703", "title": "ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation", "abstract": "Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.", "arxiv_url": "https://arxiv.org/abs/2601.09703", "authors": ["Sicong Liu", "Yanxian Huang", "Mingwei Liu", "Jiachi Chen", "Ensheng Shi", "Yuchi Ma", "Hongyu Zhang", "Yin Zhang", "Yanlin Wang"], "first_author": "Sicong Liu", "primary_category": "cs.SE", "tag": ["Code Editing"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.09703v1", "published": "2026-01-14", "update_time": "2026-01-14", "download_time": "2026-01-16 01:01:39"}
{"id": "2601.10338", "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale", "abstract": "The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.", "arxiv_url": "https://arxiv.org/abs/2601.10338", "authors": ["Yi Liu", "Weizhe Wang", "Ruitao Feng", "Yao Zhang", "Guangquan Xu", "Gelei Deng", "Yuekang Li", "Leo Zhang"], "first_author": "Yi Liu", "primary_category": "cs.CR", "tag": ["Code Vulnerability Detection"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10338v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-17 00:59:22"}
{"id": "2601.10496", "title": "Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs", "abstract": "Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it's been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.", "arxiv_url": "https://arxiv.org/abs/2601.10496", "authors": ["Ali Al-Kaswan", "Claudio Spiess", "Prem Devanbu", "Arie van Deursen", "Maliheh Izadi"], "first_author": "Ali Al-Kaswan", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10496v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-18 01:06:06"}
{"id": "2601.10253", "title": "Developer Interaction Patterns with Proactive AI: A Five-Day Field Study", "abstract": "Current in-IDE AI coding tools typically rely on time-consuming manual prompting and context management, whereas proactive alternatives that anticipate developer needs without explicit invocation remain underexplored. Understanding when humans are receptive to such proactive AI assistance during their daily work remains an open question in human-AI interaction research. We address this gap through a field study of proactive AI assistance in professional developer workflows. We present a five-day in-the-wild study with 15 developers who interacted with a proactive feature of an AI assistant integrated into a production-grade IDE that offers code quality suggestions based on in-IDE developer activity. We examined 229 AI interventions across 5,732 interaction points to understand how proactive suggestions are received across workflow stages, how developers experience them, and their perceived impact. Our findings reveal systematic patterns in human receptivity to proactive suggestions: interventions at workflow boundaries (e.g., post-commit) achieved 52% engagement rates, while mid-task interventions (e.g., on declined edit) were dismissed 62% of the time. Notably, well-timed proactive suggestions required significantly less interpretation time than reactive suggestions (45.4s versus 101.4s, W = 109.00, r = 0.533, p = 0.0016), indicating enhanced cognitive alignment. This study provides actionable implications for designing proactive coding assistants, including how to time interventions, align them with developer context, and strike a balance between AI agency and user control in production IDEs.", "arxiv_url": "https://arxiv.org/abs/2601.10253", "authors": ["Nadine Kuo", "Agnia Sergeyuk", "Valerie Chen", "Maliheh Izadi"], "first_author": "Nadine Kuo", "primary_category": "cs.HC", "tag": ["Code Alignment"], "benchmark": false, "conference": "IUI'26", "pdf_url": "https://arxiv.org/pdf/2601.10253v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-19 01:05:46"}
{"id": "2601.11362", "title": "RITA: A Tool for Automated Requirements Classification and Specification from Online User Feedback", "abstract": "Context and motivation. Online user feedback is a valuable resource for requirements engineering, but its volume and noise make analysis difficult. Existing tools support individual feedback analysis tasks, but their capabilities are rarely integrated into end-to-end support. Problem. The lack of end-to-end integration limits the practical adoption of existing RE tools and makes it difficult to assess their real-world usefulness. Solution. To address this challenge, we present RITA, a tool that integrates lightweight open-source large language models into a unified workflow for feedback-driven RE. RITA supports automated request classification, non-functional requirement identification, and natural-language requirements specification generation from online feedback via a user-friendly interface, and integrates with Jira for seamless transfer of requirements specifications to development tools. Results and conclusions. RITA exploits previously evaluated LLM-based RE techniques to efficiently transform raw user feedback into requirements artefacts, helping bridge the gap between research and practice. A demonstration is available at: https://youtu.be/8meCLpwQWV8.", "arxiv_url": "https://arxiv.org/abs/2601.11362", "authors": ["Manjeshwar Aniruddh Mallya", "Alessio Ferrari", "Mohammad Amin Zadenoori", "Jacek DƒÖbrowski"], "first_author": "Manjeshwar Aniruddh Mallya", "primary_category": "cs.SE", "tag": ["Requirements Extraction & Specification"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.11362v1", "published": "2026-01-16", "update_time": "2026-01-16", "download_time": "2026-01-20 01:00:38"}
{"id": "2601.11077", "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development", "abstract": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.", "arxiv_url": "https://arxiv.org/abs/2601.11077", "authors": ["Jie Yang", "Honglin Guo", "Li Ji", "Jiazheng Zhou", "Rui Zheng", "Zhikai Lei", "Shuo Zhang", "Zhiheng Xi", "Shichun Liu", "Yuxin Wang", "Bo Wang", "Yining Zheng", "Tao Gui", "Xipeng Qiu"], "first_author": "Jie Yang", "primary_category": "cs.SE", "tag": ["Code Testing"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.11077v1", "published": "2026-01-16", "update_time": "2026-01-16", "download_time": "2026-01-21 01:03:12"}
{"id": "2601.13943", "title": "RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme to Repository", "abstract": "Large language models and agents have achieved remarkable progress in code generation. However, existing benchmarks focus on isolated function/class-level generation (e.g., ClassEval) or modifications to existing codebases (e.g., SWE-Bench), neglecting complete microservice repository generation that reflects real-world 0-to-1 development workflows. To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and 2,335 test cases verified through a \"review-rebuttal\" quality assurance process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%), the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on Java, exposing deficiencies in architectural coherence, dependency management, and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis (train), achieves performance comparable to GPT-5 mini, demonstrating the quality of RepoGenesis for advancing microservice generation. We release our benchmark at https://github.com/pzy2000/RepoGenesis.", "arxiv_url": "https://arxiv.org/abs/2601.13943", "authors": ["Zhiyuan Peng", "Xin Yin", "Pu Zhao", "Fangkai Yang", "Lu Wang", "Ran Jia", "Xu Chen", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "first_author": "Zhiyuan Peng", "primary_category": "cs.SE", "tag": ["Code Completion"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.13943v1", "published": "2026-01-20", "update_time": "2026-01-20", "download_time": "2026-01-22 01:02:40"}
{"id": "2601.15232", "title": "When Agents Fail: A Comprehensive Study of Bugs in LLM Agents with Automated Labeling", "abstract": "Large Language Models (LLMs) have revolutionized intelligent application development. While standalone LLMs cannot perform any actions, LLM agents address the limitation by integrating tools. However, debugging LLM agents is difficult and costly as the field is still in it's early stage and the community is underdeveloped. To understand the bugs encountered during agent development, we present the first comprehensive study of bug types, root causes, and effects in LLM agent-based software. We collected and analyzed 1,187 bug-related posts and code snippets from Stack Overflow, GitHub, and Hugging Face forums, focused on LLM agents built with seven widely used LLM frameworks as well as custom implementations. For a deeper analysis, we have also studied the component where the bug occurred, along with the programming language and framework. This study also investigates the feasibility of automating bug identification. For that, we have built a ReAct agent named BugReAct, equipped with adequate external tools to determine whether it can detect and annotate the bugs in our dataset. According to our study, we found that BugReAct equipped with Gemini 2.5 Flash achieved a remarkable performance in annotating bug characteristics with an average cost of 0.01 USD per post/code snippet.", "arxiv_url": "https://arxiv.org/abs/2601.15232", "authors": ["Niful Islam", "Ragib Shahriar Ayon", "Deepak George Thomas", "Shibbir Ahmed", "Mohammad Wardat"], "first_author": "Niful Islam", "primary_category": "cs.SE", "tag": ["Code Debug"], "benchmark": true, "conference": "A version of this paper has been submitted to ACM Transactions on Software Engin...", "pdf_url": "https://arxiv.org/pdf/2601.15232v1", "published": "2026-01-21", "update_time": "2026-01-21", "download_time": "2026-01-23 01:02:39"}
{"id": "2601.15879", "title": "Evaluating and Achieving Controllable Code Completion in Code LLM", "abstract": "Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.", "arxiv_url": "https://arxiv.org/abs/2601.15879", "authors": ["Jiajun Zhang", "Zeyu Cui", "Lei Zhang", "Jian Yang", "Jiaxi Yang", "Qiang Liu", "Zilei Wang", "Binyuan Hui", "Liang Wang", "Junyang Lin"], "first_author": "Jiajun Zhang", "primary_category": "cs.SE", "tag": ["Code Completion"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.15879v1", "published": "2026-01-22", "update_time": "2026-01-22", "download_time": "2026-01-24 00:59:56"}
{"id": "2601.15195", "title": "Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub", "abstract": "AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project's CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.", "arxiv_url": "https://arxiv.org/abs/2601.15195", "authors": ["Ramtin Ehsani", "Sakshi Pathak", "Shriya Rawal", "Abdullah Al Mujahid", "Mia Mohammad Imran", "Preetha Chatterjee"], "first_author": "Ramtin Ehsani", "primary_category": "cs.SE", "tag": ["Agentic Code Collaboration"], "benchmark": false, "conference": "International Mining Software Repositories Conference (MSR 2026)", "pdf_url": "https://arxiv.org/pdf/2601.15195v1", "published": "2026-01-21", "update_time": "2026-01-21", "download_time": "2026-01-25 01:10:34"}
{"id": "2601.15728", "title": "Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity", "abstract": "While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.", "arxiv_url": "https://arxiv.org/abs/2601.15728", "authors": ["Hangle Hu", "Chenyu Hou", "Bin Cao", "Ruizhe Li"], "first_author": "Hangle Hu", "primary_category": "cs.AI", "tag": ["Code Prompting"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.15728v1", "published": "2026-01-22", "update_time": "2026-01-22", "download_time": "2026-01-26 01:06:51"}
{"id": "2601.16839", "title": "AI builds, We Analyze: An Empirical Study of AI-Generated Build Code Quality", "abstract": "The rapid adoption of AI coding agents for software development has raised important questions about the quality and maintainability of the code they produce. While prior studies have examined AI-generated source code, the impact of AI coding agents on build systems-a critical yet understudied component of the software lifecycle-remains largely unexplored. This data mining challenge focuses on AIDev, the first large-scale, openly available dataset capturing agent-authored pull requests (Agentic-PRs) from real-world GitHub repositories. Our paper leverages this dataset to investigate (RQ1) whether AI coding agents generate build code with quality issues (e.g., code smells), (RQ2) to what extent AI agents can eliminate code smells from build code, and (RQ3) to what extent Agentic-PRs are accepted by developers. We identified 364 maintainability and security-related build smells across varying severity levels, indicating that AI-generated build code can introduce quality issues-such as lack of error handling, and hardcoded paths or URLs-while also, in some cases, removing existing smells through refactorings (e.g., Pull Up Module and Externalize Properties). Notably, more than 61\\% of Agentic-PRs are approved and merged with minimal human intervention. This dual impact underscores the need for future research on AI-aware build code quality assessment to systematically evaluate, guide, and govern AI-generated build systems code.", "arxiv_url": "https://arxiv.org/abs/2601.16839", "authors": ["Anwar Ghammam", "Mohamed Almukhtar"], "first_author": "Anwar Ghammam", "primary_category": "cs.SE", "tag": ["Code Quality Assessment"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.16839v1", "published": "2026-01-23", "update_time": "2026-01-23", "download_time": "2026-01-27 01:05:16"}
{"id": "2601.18749", "title": "Let's Make Every Pull Request Meaningful: An Empirical Analysis of Developer and Agentic Pull Requests", "abstract": "The automatic generation of pull requests (PRs) using AI agents has become increasingly common. Although AI-generated PRs are fast and easy to create, their merge rates have been reported to be lower than those created by humans. In this study, we conduct a large-scale empirical analysis of 40,214 PRs collected from the AIDev dataset. We extract 64 features across six families and fit statistical regression models to compare PR merge outcomes for human and agentic PRs, as well as across three AI agents. Our results show that submitter attributes dominate merge outcomes for both groups, while review-related features exhibit contrasting effects between human and agentic PRs. The findings of this study provide insights into improving PR quality through human-AI collaboration.", "arxiv_url": "https://arxiv.org/abs/2601.18749", "authors": ["Haruhiko Yoshioka", "Takahiro Monno", "Haruka Tokumasu", "Taiki Wakamatsu", "Yuki Ota", "Nimmi Weeraddana", "Kenichi Matsumoto"], "first_author": "Haruhiko Yoshioka", "primary_category": "cs.SE", "tag": ["Code Review and Human-AI Collaboration"], "benchmark": false, "conference": "publication in the 23rd International Conference on Mining Software Repositories (MSR '26) :", "pdf_url": "https://arxiv.org/pdf/2601.18749v1", "published": "2026-01-26", "update_time": "2026-01-26", "download_time": "2026-01-28 01:01:56"}
{"id": "2601.19747", "title": "Veri-Sure: A Contract-Aware Multi-Agent Framework with Temporal Tracing and Formal Verification for Correct RTL Code Generation", "abstract": "In the rapidly evolving field of Electronic Design Automation (EDA), the deployment of Large Language Models (LLMs) for Register-Transfer Level (RTL) design has emerged as a promising direction. However, silicon-grade correctness remains bottlenecked by: (i) limited test coverage and reliability of simulation-centric evaluation, (ii) regressions and repair hallucinations introduced by iterative debugging, and (iii) semantic drift as intent is reinterpreted across agent handoffs. In this work, we propose Veri-Sure, a multi-agent framework that establishes a design contract to align agents' intent and uses a patching mechanism guided by static dependency slicing to perform precise, localized repairs. By integrating a multi-branch verification pipeline that combines trace-driven temporal analysis with formal verification consisting of assertion-based checking and boolean equivalence proofs, Veri-Sure enables functional correctness beyond pure simulations. We also introduce VerilogEval-v2-EXT, extending the original benchmark with 53 more industrial-grade design tasks and stratified difficulty levels, and show that Veri-Sure achieves state-of-the-art verified-correct RTL code generation performance, surpassing standalone LLMs and prior agentic systems.", "arxiv_url": "https://arxiv.org/abs/2601.19747", "authors": ["Jiale Liu", "Taiyu Zhou", "Tianqi Jiang"], "first_author": "Jiale Liu", "primary_category": "cs.AR", "tag": ["Code Debug"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.19747v1", "published": "2026-01-27", "update_time": "2026-01-27", "download_time": "2026-01-29 01:10:58"}
{"id": "2601.20810", "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs", "abstract": "Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph", "arxiv_url": "https://arxiv.org/abs/2601.20810", "authors": ["Shahd Seddik", "Fahd Seddik", "Iman Saberi", "Fatemeh Fard", "Minh Hieu Huynh", "Patanamon Thongtanunam"], "first_author": "Shahd Seddik", "primary_category": "cs.SE", "tag": ["Code Completion"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.20810v1", "published": "2026-01-28", "update_time": "2026-01-28", "download_time": "2026-01-30 01:10:28"}
{"id": "2601.22136", "title": "StepShield: When, Not Whether to Intervene on Rogue Agents", "abstract": "Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.", "arxiv_url": "https://arxiv.org/abs/2601.22136", "authors": ["Gloria Felicia", "Michael Eniolade", "Jinfeng He", "Zitha Sasindran", "Hemant Kumar", "Milan Hussain Angati", "Sandeep Bandarupalli"], "first_author": "Gloria Felicia", "primary_category": "cs.LG", "tag": ["Code Alignment"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.22136v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-01-31 01:07:41"}
{"id": "2601.22130", "title": "World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems", "abstract": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.", "arxiv_url": "https://arxiv.org/abs/2601.22130", "authors": ["Lakshya Gupta", "Litao Li", "Yizhe Liu", "Sriram Ganapathi Subramanian", "Kaheer Suleman", "Zichen Zhang", "Haoye Lu", "Sumit Pasupalak"], "first_author": "Lakshya Gupta", "primary_category": "cs.AI", "tag": ["World Modeling for Enterprise Agents"], "benchmark": true, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.22130v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-02-01 01:22:42"}
{"id": "2601.22129", "title": "SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents", "abstract": "Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.", "arxiv_url": "https://arxiv.org/abs/2601.22129", "authors": ["Yifeng Ding", "Lingming Zhang"], "first_author": "Yifeng Ding", "primary_category": "cs.SE", "tag": ["Code Agent Orchestration"], "benchmark": false, "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.22129v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-02-02 01:14:54"}
