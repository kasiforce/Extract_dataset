{"paper_id": "2508.21107", "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning", "abstract": "Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate test generation, yet methods for training LLMs to produce high-quality tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning framework that trains an LLM to generate high-quality unit tests given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via reinforcement learning. The unit test generator is trained to maximize a discrimination reward, which reflects its ability to produce tests that expose faults in the code generator's solutions, and the code generator is trained to maximize a code reward, which reflects its ability to produce solutions that pass the unit tests generated by the test generator. In our experiments, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on human-written ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models such as GPT-4.1 in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for this task.", "paper_url": "https://arxiv.org/abs/2508.21107", "authors": "Dongjun Lee, Changho Hwang, Kimin Lee", "first_author": "Dongjun Lee", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-08-28", "update_time": "2025-09-30", "comments": "Code is available at: https://github.com/dgjun32/UTRL", "download_time": "2025-11-07 05:32:17", "query": "ti:Learning to Generate Unit Test via Adversarial Reinforcement Learning"}
{"paper_id": "2311.00272", "title": "ChatCoder: Chat-based Refine Requirement Improves LLMs' Code Generation", "abstract": "Large language models have shown good performances in generating code to meet human requirements. However, human requirements expressed in natural languages can be vague, incomplete, and ambiguous, leading large language models to misunderstand human requirements and make mistakes. Worse, it is difficult for a human user to refine the requirement. To help human users refine their requirements and improve large language models' code generation performances, we propose ChatCoder: a method to refine the requirements via chatting with large language models. We design a chat scheme in which the large language models will guide the human users to refine their expression of requirements to be more precise, unambiguous, and complete than before. Experiments show that ChatCoder has improved existing large language models' performance by a large margin. Besides, ChatCoder has the advantage over refine-based methods and LLMs fine-tuned via human response.", "paper_url": "https://arxiv.org/abs/2311.00272", "authors": "Zejun Wang, Jia Li, Ge Li, Zhi Jin", "first_author": "Zejun Wang", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2023-11-01", "update_time": "2023-11-01", "comments": "", "download_time": "2025-11-07 05:32:17", "query": "ti:ChatCoder Chat-based Refine Requirement Improves LLMs Code Generation"}
{"paper_id": "2503.17837", "title": "A Study on the Improvement of Code Generation Quality Using Large Language Models Leveraging Product Documentation", "abstract": "Research on using Large Language Models (LLMs) in system development is expanding, especially in automated code and test generation. While E2E testing is vital for ensuring application quality, most test generation research has focused on unit tests, with limited work on E2E test code. This study proposes a method for automatically generating E2E test code from product documentation such as manuals, FAQs, and tutorials using LLMs with tailored prompts. The two step process interprets documentation intent and produces executable test code. Experiments on a web app with six key features (e.g., authentication, profile, discussion) showed that tests generated from product documentation had high compilation success and functional coverage, outperforming those based on requirement specs and user stories. These findings highlight the potential of product documentation to improve E2E test quality and, by extension, software quality.", "paper_url": "https://arxiv.org/abs/2503.17837", "authors": "Takuro Morimoto, Harumi Haraguchi", "first_author": "Takuro Morimoto", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-03-22", "update_time": "2025-03-22", "comments": "12 pages, 5 figures and 10 tables", "download_time": "2025-11-07 05:32:17", "query": "ti:A Study on the Improvement of Code Generation Quality Using Large Language Models Leveraging Product Documentation"}
{"paper_id": "2305.02176", "title": "Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity", "abstract": "Mixture-of-experts (MoE) models that employ sparse activation have demonstrated effectiveness in significantly increasing the number of parameters while maintaining low computational requirements per token. However, recent studies have established that MoE models are inherently parameter-inefficient as the improvement in performance diminishes with an increasing number of experts. We hypothesize this parameter inefficiency is a result of all experts having equal capacity, which may not adequately meet the varying complexity requirements of different tokens or tasks. In light of this, we propose Stratified Mixture of Experts (SMoE) models, which feature a stratified structure and can assign dynamic capacity to different tokens. We demonstrate the effectiveness of SMoE on three multilingual machine translation benchmarks, containing 4, 15, and 94 language pairs, respectively. We show that SMoE outperforms multiple state-of-the-art MoE models with the same or fewer parameters.", "paper_url": "https://arxiv.org/abs/2305.02176", "authors": "Haoran Xu, Maha Elbayad, Kenton Murray, Jean Maillard, Vedanuj Goswami", "first_author": "Haoran Xu", "primary_category": "cs.CL", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2023-05-03", "update_time": "2023-10-22", "comments": "Accepted at Findings of EMNLP 2023", "download_time": "2025-11-07 05:32:17", "query": "ti:Towards Being Parameter-Efficient A Stratified Sparsely Activated Transformer with Dynamic Capacity"}
{"paper_id": "2408.02479", "title": "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future", "abstract": "With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.", "paper_url": "https://arxiv.org/abs/2408.02479", "authors": "Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen", "first_author": "Haolin Jin", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2024-08-05", "update_time": "2025-04-13", "comments": "", "download_time": "2025-11-07 05:32:26", "query": "ti:From LLMs to LLM-based Agents for Software Engineering A Survey of Current, Challenges and Future"}
{"paper_id": "2501.11354", "title": "Towards Advancing Code Generation with Large Language Models: A Research Roadmap", "abstract": "Recently, we have witnessed the rapid development of large language models, which have demonstrated excellent capabilities in the downstream task of code generation. However, despite their potential, LLM-based code generation still faces numerous technical and evaluation challenges, particularly when embedded in real-world development. In this paper, we present our vision for current research directions, and provide an in-depth analysis of existing studies on this task. We propose a six-layer vision framework that categorizes code generation process into distinct phases, namely Input Phase, Orchestration Phase, Development Phase, and Validation Phase. Additionally, we outline our vision workflow, which reflects on the currently prevalent frameworks. We systematically analyse the challenges faced by large language models, including those LLM-based agent frameworks, in code generation tasks. With these, we offer various perspectives and actionable recommendations in this area. Our aim is to provide guidelines for improving the reliability, robustness and usability of LLM-based code generation systems. Ultimately, this work seeks to address persistent challenges and to provide practical suggestions for a more pragmatic LLM-based solution for future code generation endeavors.", "paper_url": "https://arxiv.org/abs/2501.11354", "authors": "Haolin Jin, Huaming Chen, Qinghua Lu, Liming Zhu", "first_author": "Haolin Jin", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-01-20", "update_time": "2025-01-20", "comments": "", "download_time": "2025-11-07 05:32:26", "query": "ti:Towards Advancing Code Generation with Large Language Models A Research Roadmap"}
{"paper_id": "2402.12219", "title": "Reformatted Alignment", "abstract": "The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical reasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy. Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment ability measured by the Alpaca dataset. This work highlights the need for further research into the science and mechanistic interpretability of LLMs. We have made the associated code and data publicly accessible to support future studies at https://github.com/GAIR-NLP/ReAlign.", "paper_url": "https://arxiv.org/abs/2402.12219", "authors": "Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, Pengfei Liu", "first_author": "Run-Ze Fan", "primary_category": "cs.CL", "topic": "LLM Coding", "pdf_path": "./papers/2402.12219.pdf", "publish_time": "2024-02-19", "update_time": "2024-04-17", "comments": "Homepage: https://gair-nlp.github.io/ReAlign/", "download_time": "2025-11-07 05:32:37", "query": "ti:Aligning Requirement for Large Language Models Code Generation"}
{"paper_id": "2503.01245", "title": "Large Language Models for Code Generation: A Comprehensive Survey of Challenges, Techniques, Evaluation, and Applications", "abstract": "Large Language Models (LLMs) have demonstrated their remarkable capabilities in numerous fields. This survey focuses on how LLMs empower users, regardless of their technical background, to use human languages to automatically generate executable code. We begin with understanding LLMs' limitations and challenges in automated code generation. Subsequently, we review various fine-tuning techniques designed to enhance both the performance and adaptability of LLMs in code generation tasks. We then review the existing metrics and benchmarks for evaluations to assess model performance based on fine-tuning techniques. Finally, we explore the applications of LLMs (e.g. CodeLlama, GitHub Copilot, ToolGen) in code generation tasks to illustrate their roles and functionalities. This survey provides a comprehensive overview of LLMs for code generation, helps researchers in diverse fields better understand the current state-of-the-art technologies, and offers the potential of effectively leveraging LLMs for code generation tasks.", "paper_url": "https://arxiv.org/abs/2503.01245", "authors": "Nam Huynh, Beiyu Lin", "first_author": "Nam Huynh", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-03-03", "update_time": "2025-04-02", "comments": "", "download_time": "2025-11-07 05:32:42", "query": "ti:Large Language Models for Code Generation A Comprehensive Survey of Challenges, Techniques, Evaluation, and Applications"}
{"paper_id": "2412.16620", "title": "A Large-scale Empirical Study on Fine-tuning Large Language Models for Unit Testing", "abstract": "Unit testing plays a pivotal role in software development, improving software quality and reliability. However, generating effective test cases manually is time-consuming, prompting interest in unit testing research. Recently, Large Language Models (LLMs) have shown potential in various unit testing tasks, including test generation, assertion generation, and test evolution, but existing studies are limited in scope and lack a systematic evaluation of the effectiveness of LLMs.   To bridge this gap, we present a large-scale empirical study on fine-tuning LLMs for unit testing. Our study involves three unit testing tasks, five benchmarks, eight evaluation metrics, and 37 popular LLMs across various architectures and sizes, consuming over 3,000 NVIDIA A100 GPU hours. We focus on three key research questions: (1) the performance of LLMs compared to state-of-the-art methods, (2) the impact of different factors on LLM performance, and (3) the effectiveness of fine-tuning versus prompt engineering. Our findings reveal that LLMs outperform existing state-of-the-art approaches on all three unit testing tasks across nearly all metrics, highlighting the potential of fine-tuning LLMs in unit testing tasks. Furthermore, large-scale, decoder-only models achieve the best results across tasks, while encoder-decoder models perform better under the same parameter scale. Additionally, the comparison of the performance between fine-tuning and prompt engineering approaches reveals the considerable potential capability of the prompt engineering approach in unit testing tasks. We then discuss the concerned issues on the test generation task, including data leakage issues, bug detection capabilities, and metrics comparisons. Finally, we further pinpoint carious practical guidelines for LLM-based approaches to unit testing tasks in the near future.", "paper_url": "https://arxiv.org/abs/2412.16620", "authors": "Ye Shang, Quanjun Zhang, Chunrong Fang, Siqi Gu, Jianyi Zhou, Zhenyu Chen", "first_author": "Ye Shang", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "./papers/2412.16620.pdf", "publish_time": "2024-12-21", "update_time": "2024-12-21", "comments": "Accepted to the ACM SIGSOFT International Symposium on Software\n  Testing and Analysis (ISSTA 2025)", "download_time": "2025-11-07 05:32:43", "query": "ti:On the Evaluation of Large Language Models in Unit Test Generation"}
{"paper_id": "2506.15227", "title": "Large Language Models for Unit Testing: A Systematic Literature Review", "abstract": "Unit testing is a fundamental practice in modern software engineering, with the aim of ensuring the correctness, maintainability, and reliability of individual software components. Very recently, with the advances in Large Language Models (LLMs), a rapidly growing body of research has leveraged LLMs to automate various unit testing tasks, demonstrating remarkable performance and significantly reducing manual effort. However, due to ongoing explorations in the LLM-based unit testing field, it is challenging for researchers to understand existing achievements, open challenges, and future opportunities. This paper presents the first systematic literature review on the application of LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant papers from the perspectives of both unit testing and LLMs. We first categorize existing unit testing tasks that benefit from LLMs, e.g., test generation and oracle generation. We then discuss several critical aspects of integrating LLMs into unit testing research, including model usage, adaptation strategies, and hybrid approaches. We further summarize key challenges that remain unresolved and outline promising directions to guide future research in this area. Overall, our paper provides a systematic overview of the research landscape to the unit testing community, helping researchers gain a comprehensive understanding of achievements and promote future research. Our artifacts are publicly available at the GitHub repository: https://github.com/iSEngLab/AwesomeLLM4UT.", "paper_url": "https://arxiv.org/abs/2506.15227", "authors": "Quanjun Zhang, Chunrong Fang, Siqi Gu, Ye Shang, Zhenyu Chen, Liang Xiao", "first_author": "Quanjun Zhang", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-06-18", "update_time": "2025-06-18", "comments": "", "download_time": "2025-11-07 05:32:51", "query": "ti:Large Language Models for Unit Testing A Systematic Literature Review"}
{"paper_id": "2410.09812", "title": "Unraveling the Potential of Large Language Models in Code Translation: How Far Are We?", "abstract": "While large language models (LLMs) exhibit state-of-the-art performance in various tasks, recent studies have revealed their struggle for code translation. This is because they haven't been extensively pre-trained with parallel multilingual code, which code translation heavily depends on. Moreover, existing benchmarks only cover a limited subset of common programming languages, and thus cannot reflect the full potential of LLMs in code translation. In this paper, we conduct a large-scale empirical study to exploit the capabilities and incapabilities of LLMs in code translation tasks. We first craft a novel benchmark called PolyHumanEval by extending HumanEval to a multilingual benchmark of 14 languages. With PolyHumanEval, we then perform over 110,000 translations with bleeding-edge code LLMs. The result shows LLMs' suboptimal performance on Python to other languages and the negligible impact of widely adopted LLM optimization techniques such as conventional pre-training and instruction tuning on code translation. To further uncover the potential of LLMs in code translation, we propose two methods: (1) intermediary translation which selects an intermediary language between the source and target ones; and (2) self-training which fine-tunes LLMs on self-generated parallel data. Evaluated with CodeLlama-13B, our approach yields an average improvement of 11.7% computation accuracy on Python-to-other translations. Notably, we interestingly find that Go can serve as a lingua franca for translating between any two studied languages.", "paper_url": "https://arxiv.org/abs/2410.09812", "authors": "Qingxiao Tao, Tingrui Yu, Xiaodong Gu, Beijun Shen", "first_author": "Qingxiao Tao", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2024-10-13", "update_time": "2024-10-13", "comments": "Accepted to APSEC 2024", "download_time": "2025-11-07 05:33:00", "query": "ti:Unraveling the Potential of Large Language Models in Code Translation How Far Are We?"}
{"paper_id": "2501.18460", "title": "ExeCoder: Empowering Large Language Models with Executability Representation for Code Translation", "abstract": "Code translation is a crucial activity in the software development and maintenance process, and researchers have recently begun to focus on using pre-trained large language models (LLMs) for code translation. However, existing LLMs only learn the contextual semantics of code during pre-training, neglecting executability information closely related to the execution state of the code, which results in unguaranteed code executability and unreliable automated code translation. To address this issue, we propose ExeCoder, an LLM specifically designed for code translation, aimed at utilizing executability representations such as functional semantics, syntax structures, and variable dependencies to enhance the capabilities of LLMs in code translation. To evaluate the effectiveness of ExeCoder, we manually enhanced the widely used benchmark TransCoder-test, resulting in a benchmark called TransCoder-test-X that serves LLMs. Evaluation of TransCoder-test-X indicates that ExeCoder achieves state-of-the-art performance in code translation, surpassing existing open-source code LLMs by over 10.88% to 38.78% and over 27.44% to 42.97% on two metrics, and even outperforms the renowned closed-source LLM GPT-4o. Code is available at https://aka.ms/execoder", "paper_url": "https://arxiv.org/abs/2501.18460", "authors": "Minghua He, Yue Chen, Fangkai Yang, Pu Zhao, Wenjie Yin, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang", "first_author": "Minghua He", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-01-30", "update_time": "2025-09-27", "comments": "EMNLP 2025 (Oral)", "download_time": "2025-11-07 05:33:05", "query": "ti:ExeCoder Empowering Large Language Models with Executability Representation for Code Translation"}
{"paper_id": "2410.15966", "title": "Self-Explained Keywords Empower Large Language Models for Code Generation", "abstract": "Large language models (LLMs) have achieved impressive performance in code generation. However, due to the long-tail distribution of LLMs' training data, low-frequency terms are typically underrepresented in the training process. Consequently, LLMs often misunderstand or overlook problem-specific, low-frequency keywords during code generation, compromising the accuracy of the generated code. To address this, we propose a novel technique named SEK(\\textbf{S}elf-\\textbf{E}xplained \\textbf{K}eywords), which empowers an LLM for better code generation by extracting and explaining the key terms in the problem description with the LLM itself and ranking them based on frequency. Comprehensive experiments across three benchmarks, i.e., HumanEval(+), MBPP(+), and APPS, with five representative LLMs, show that SEK can significantly improve LLMs in code generation, yielding substantial and consistent gains. For instance, SEK improves the Pass@1 of DeepSeek-Coder-V2-Instruct from 85.4\\% to 93.3\\% on the Humaneval benchmark. Further analysis confirms that SEK enables the LLMs to shift their attention from low-frequency keywords to their corresponding high-frequency counterparts.", "paper_url": "https://arxiv.org/abs/2410.15966", "authors": "Lishui Fan, Mouxiang Chen, Zhongxin Liu", "first_author": "Lishui Fan", "primary_category": "cs.CL", "topic": "LLM Coding", "pdf_path": "./papers/2410.15966.pdf", "publish_time": "2024-10-21", "update_time": "2024-10-21", "comments": "", "download_time": "2025-11-07 05:33:15", "query": "(llm OR \"large language model\") AND (\"code generation\")"}
{"paper_id": "2406.08731", "title": "Towards Understanding the Characteristics of Code Generation Errors Made by Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated unprecedented capabilities in code generation. However, there remains a limited understanding of code generation errors that LLMs can produce. To bridge the gap, we conducted an in-depth analysis of code generation errors across six representative LLMs on the HumanEval dataset. Specifically, we first employed open coding and thematic analysis to distill a comprehensive taxonomy of code generation errors. We analyzed two dimensions of error characteristics -- semantic characteristics and syntactic characteristics. Our analysis revealed that LLMs often made non-trivial, multi-line code generation errors in various locations and with various root causes. We further analyzed the correlation between these errors and task complexity as well as test pass rate. Our findings highlighted several challenges in locating and fixing code generation errors made by LLMs. In the end, we discussed several future directions to address these challenges.", "paper_url": "https://arxiv.org/abs/2406.08731", "authors": "Zhijie Wang, Zijie Zhou, Da Song, Yuheng Huang, Shengmai Chen, Lei Ma, Tianyi Zhang", "first_author": "Zhijie Wang", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "./papers/2406.08731.pdf", "publish_time": "2024-06-13", "update_time": "2025-02-12", "comments": "To appear in the 47th IEEE/ACM Conference on Software Engineering\n  (ICSE 2025). The first three authors contributed equally to this work", "download_time": "2025-11-07 05:33:15", "query": "(llm OR \"large language model\") AND (\"code generation\")"}
{"paper_id": "2507.17271", "title": "Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation", "abstract": "Unit tests play a vital role in the software development lifecycle. Recent advances in Large Language Model (LLM)-based approaches have significantly improved automated test generation, garnering attention from both academia and industry. We revisit LLM-based unit test generation from a novel perspective by decoupling prefix generation and assertion generation. To characterize their respective challenges, we define Initialization Complexity and adopt Cyclomatic Complexity to measure the difficulty of prefix and assertion generation, revealing that the former primarily affects compilation success, while the latter influences test coverage. To address these challenges, we propose Seed&Steer, a two-step approach that combines traditional unit testing techniques with the capabilities of large language models. Seed&Steer leverages conventional unit testing tools (e.g., EvoSuite) to generate method invocations with high compilation success rates, which serve as seeds to guide LLMs in constructing effective test contexts. It then introduces branching cues to help LLMs explore diverse execution paths (e.g., normal, boundary, and exception cases) and generate assertions with high coverage. We evaluate Seed&Steer on five real-world Java projects against state-of-the-art baselines. Results show that Seed&Steer improves the compilation pass rate by approximately 7%, successfully compiling 792 and 887 previously failing cases on two LLMs. It also achieves up to ~73% branch and line coverage across focal methods of varying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our code, dataset, and experimental scripts will be publicly released to support future research and reproducibility.", "paper_url": "https://arxiv.org/abs/2507.17271", "authors": "Shuaiyu Zhou, Zhengran Zeng, Xiaoling Zhou, Rui Xie, Shikun Zhang, Wei Ye", "first_author": "Shuaiyu Zhou", "primary_category": "cs.SE", "topic": "LLM Testing", "pdf_path": "./papers/2507.17271.pdf", "publish_time": "2025-07-23", "update_time": "2025-07-23", "comments": "", "download_time": "2025-11-07 05:33:16", "query": "(llm OR \"large language model\") AND (\"unit test\" OR \"test generation\")"}
{"paper_id": "2501.16155", "title": "CITYWALK: Enhancing LLM-Based C++ Unit Test Generation via Project-Dependency Awareness and Language-Specific Knowledge", "abstract": "Unit testing plays a pivotal role in the software development lifecycle, as it ensures code quality. However, writing high-quality unit tests remains a time-consuming task for developers in practice. More recently, the application of large language models (LLMs) in automated unit test generation has demonstrated promising results. Existing approaches primarily focus on interpreted programming languages (e.g., Java), while mature solutions tailored to compiled programming languages like C++ are yet to be explored. The intricate language features of C++, such as pointers, templates, and virtual functions, pose particular challenges for LLMs in generating both executable and high-coverage unit tests. To tackle the aforementioned problems, this paper introduces CITYWALK, a novel LLM-based framework for C++ unit test generation. CITYWALK enhances LLMs by providing a comprehensive understanding of the dependency relationships within the project under test via program analysis. Furthermore, CITYWALK incorporates language-specific knowledge about C++ derived from project documentation and empirical observations, significantly improving the correctness of the LLM-generated unit tests. We implement CITYWALK by employing the widely popular LLM GPT-4o. The experimental results show that CITYWALK outperforms current state-of-the-art approaches on a collection of ten popular C++ projects. Our findings demonstrate the effectiveness of CITYWALK in generating high-quality C++ unit tests.", "paper_url": "https://arxiv.org/abs/2501.16155", "authors": "Yuwei Zhang, Qingyuan Lu, Kai Liu, Wensheng Dou, Jiaxin Zhu, Li Qian, Chunxi Zhang, Zheng Lin, Jun Wei", "first_author": "Yuwei Zhang", "primary_category": "cs.SE", "topic": "LLM Testing", "pdf_path": "./papers/2501.16155.pdf", "publish_time": "2025-01-27", "update_time": "2025-08-11", "comments": "Preprint, to appear in the ACM Transactions on Software Engineering\n  and Methodology (TOSEM)", "download_time": "2025-11-07 05:33:17", "query": "(llm OR \"large language model\") AND (\"unit test\" OR \"test generation\")"}
{"paper_id": "2412.04590", "title": "Specification-Driven Code Translation Powered by Large Language Models: How Far Are We?", "abstract": "Large Language Models (LLMs) are increasingly being applied across various domains, including code-related tasks such as code translation. Previous studies have explored using LLMs for translating code between different programming languages. Since LLMs are more effective with natural language, using natural language as an intermediate representation in code translation tasks presents a promising approach. In this work, we investigate using NL-specification as an intermediate representation for code translation. We evaluate our method using three datasets, five popular programming languages, and 29 language pair permutations. Our results show that using NL-specification alone does not lead to performance improvements. However, when combined with source code, it provides a slight improvement over the baseline in certain language pairs. Besides analyzing the performance of code translation, we also investigate the quality of the translated code and provide insights into the issues present in the translated code.", "paper_url": "https://arxiv.org/abs/2412.04590", "authors": "Soumit Kanti Saha, Fazle Rabbi, Song Wang, Jinqiu Yang", "first_author": "Soumit Kanti Saha", "primary_category": "cs.SE", "topic": "LLM Translation", "pdf_path": "./papers/2412.04590.pdf", "publish_time": "2024-12-05", "update_time": "2024-12-05", "comments": "", "download_time": "2025-11-07 05:33:17", "query": "(llm OR \"large language model\") AND (\"code translation\")"}
{"paper_id": "2308.03109", "title": "Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code", "abstract": "Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation -- with correct translations ranging from 2.1% to 47.3% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset -- consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs -- can help drive research in this area.", "paper_url": "https://arxiv.org/abs/2308.03109", "authors": "Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, Reyhaneh Jabbarvand", "first_author": "Rangeet Pan", "primary_category": "cs.SE", "topic": "LLM Translation", "pdf_path": "./papers/2308.03109.pdf", "publish_time": "2023-08-06", "update_time": "2024-01-16", "comments": "Published in ICSE 2024", "download_time": "2025-11-07 05:33:18", "query": "(llm OR \"large language model\") AND (\"code translation\")"}
