{"paper_id": "2508.21107", "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning", "abstract": "Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate test generation, yet methods for training LLMs to produce high-quality tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning framework that trains an LLM to generate high-quality unit tests given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via reinforcement learning. The unit test generator is trained to maximize a discrimination reward, which reflects its ability to produce tests that expose faults in the code generator's solutions, and the code generator is trained to maximize a code reward, which reflects its ability to produce solutions that pass the unit tests generated by the test generator. In our experiments, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on human-written ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models such as GPT-4.1 in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for this task.", "paper_url": "https://arxiv.org/abs/2508.21107", "authors": "Dongjun Lee, Changho Hwang, Kimin Lee", "first_author": "Dongjun Lee", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-08-28", "update_time": "2025-09-30", "comments": "Code is available at: https://github.com/dgjun32/UTRL", "download_time": "2025-11-06 01:15:50", "query": "ti:Learning to Generate Unit Test via Adversarial Reinforcement Learning"}
{"paper_id": "2311.00272", "title": "ChatCoder: Chat-based Refine Requirement Improves LLMs' Code Generation", "abstract": "Large language models have shown good performances in generating code to meet human requirements. However, human requirements expressed in natural languages can be vague, incomplete, and ambiguous, leading large language models to misunderstand human requirements and make mistakes. Worse, it is difficult for a human user to refine the requirement. To help human users refine their requirements and improve large language models' code generation performances, we propose ChatCoder: a method to refine the requirements via chatting with large language models. We design a chat scheme in which the large language models will guide the human users to refine their expression of requirements to be more precise, unambiguous, and complete than before. Experiments show that ChatCoder has improved existing large language models' performance by a large margin. Besides, ChatCoder has the advantage over refine-based methods and LLMs fine-tuned via human response.", "paper_url": "https://arxiv.org/abs/2311.00272", "authors": "Zejun Wang, Jia Li, Ge Li, Zhi Jin", "first_author": "Zejun Wang", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2023-11-01", "update_time": "2023-11-01", "comments": "", "download_time": "2025-11-06 01:15:50", "query": "ti:ChatCoder Chat-based Refine Requirement Improves LLMs Code Generation"}
{"paper_id": "2503.17837", "title": "A Study on the Improvement of Code Generation Quality Using Large Language Models Leveraging Product Documentation", "abstract": "Research on using Large Language Models (LLMs) in system development is expanding, especially in automated code and test generation. While E2E testing is vital for ensuring application quality, most test generation research has focused on unit tests, with limited work on E2E test code. This study proposes a method for automatically generating E2E test code from product documentation such as manuals, FAQs, and tutorials using LLMs with tailored prompts. The two step process interprets documentation intent and produces executable test code. Experiments on a web app with six key features (e.g., authentication, profile, discussion) showed that tests generated from product documentation had high compilation success and functional coverage, outperforming those based on requirement specs and user stories. These findings highlight the potential of product documentation to improve E2E test quality and, by extension, software quality.", "paper_url": "https://arxiv.org/abs/2503.17837", "authors": "Takuro Morimoto, Harumi Haraguchi", "first_author": "Takuro Morimoto", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "./papers/2503.17837.pdf", "publish_time": "2025-03-22", "update_time": "2025-03-22", "comments": "12 pages, 5 figures and 10 tables", "download_time": "2025-11-06 01:15:52", "query": "(llm OR \"large language model\") AND (\"code generation\" OR \"test generation\")"}
{"paper_id": "2411.02462", "title": "Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study", "abstract": "The advent of large language models (LLMs) like GitHub Copilot has significantly enhanced programmers' productivity, particularly in code generation. However, these models often struggle with real-world tasks without fine-tuning. As LLMs grow larger and more performant, fine-tuning for specialized tasks becomes increasingly expensive. Parameter-efficient fine-tuning (PEFT) methods, which fine-tune only a subset of model parameters, offer a promising solution by reducing the computational costs of tuning LLMs while maintaining their performance. Existing studies have explored using PEFT and LLMs for various code-related tasks and found that the effectiveness of PEFT techniques is task-dependent. The application of PEFT techniques in unit test generation remains underexplored. The state-of-the-art is limited to using LLMs with full fine-tuning to generate unit tests. This paper investigates both full fine-tuning and various PEFT methods, including LoRA, (IA)^3, and prompt tuning, across different model architectures and sizes. We use well-established benchmark datasets to evaluate their effectiveness in unit test generation. Our findings show that PEFT methods can deliver performance comparable to full fine-tuning for unit test generation, making specialized fine-tuning more accessible and cost-effective. Notably, prompt tuning is the most effective in terms of cost and resource utilization, while LoRA approaches the effectiveness of full fine-tuning in several cases.", "paper_url": "https://arxiv.org/abs/2411.02462", "authors": "André Storhaug, Jingyue Li", "first_author": "André Storhaug", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "./papers/2411.02462.pdf", "publish_time": "2024-11-04", "update_time": "2024-11-04", "comments": "12 pages, 3 figures, 4 tables, 1 listing", "download_time": "2025-11-06 01:15:52", "query": "(llm OR \"large language model\") AND (\"code generation\" OR \"test generation\")"}
