{"paper_id": "2508.21107", "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning", "abstract": "Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate test generation, yet methods for training LLMs to produce high-quality tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning framework that trains an LLM to generate high-quality unit tests given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via reinforcement learning. The unit test generator is trained to maximize a discrimination reward, which reflects its ability to produce tests that expose faults in the code generator's solutions, and the code generator is trained to maximize a code reward, which reflects its ability to produce solutions that pass the unit tests generated by the test generator. In our experiments, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on human-written ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models such as GPT-4.1 in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for this task.", "paper_url": "https://arxiv.org/abs/2508.21107", "authors": "Dongjun Lee, Changho Hwang, Kimin Lee", "first_author": "Dongjun Lee", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-08-28", "update_time": "2025-09-30", "comments": "Code is available at: https://github.com/dgjun32/UTRL", "download_time": "2025-11-07 05:32:17", "query": "ti:Learning to Generate Unit Test via Adversarial Reinforcement Learning"}
{"paper_id": "2311.00272", "title": "ChatCoder: Chat-based Refine Requirement Improves LLMs' Code Generation", "abstract": "Large language models have shown good performances in generating code to meet human requirements. However, human requirements expressed in natural languages can be vague, incomplete, and ambiguous, leading large language models to misunderstand human requirements and make mistakes. Worse, it is difficult for a human user to refine the requirement. To help human users refine their requirements and improve large language models' code generation performances, we propose ChatCoder: a method to refine the requirements via chatting with large language models. We design a chat scheme in which the large language models will guide the human users to refine their expression of requirements to be more precise, unambiguous, and complete than before. Experiments show that ChatCoder has improved existing large language models' performance by a large margin. Besides, ChatCoder has the advantage over refine-based methods and LLMs fine-tuned via human response.", "paper_url": "https://arxiv.org/abs/2311.00272", "authors": "Zejun Wang, Jia Li, Ge Li, Zhi Jin", "first_author": "Zejun Wang", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2023-11-01", "update_time": "2023-11-01", "comments": "", "download_time": "2025-11-07 05:32:17", "query": "ti:ChatCoder Chat-based Refine Requirement Improves LLMs Code Generation"}
{"paper_id": "2503.17837", "title": "A Study on the Improvement of Code Generation Quality Using Large Language Models Leveraging Product Documentation", "abstract": "Research on using Large Language Models (LLMs) in system development is expanding, especially in automated code and test generation. While E2E testing is vital for ensuring application quality, most test generation research has focused on unit tests, with limited work on E2E test code. This study proposes a method for automatically generating E2E test code from product documentation such as manuals, FAQs, and tutorials using LLMs with tailored prompts. The two step process interprets documentation intent and produces executable test code. Experiments on a web app with six key features (e.g., authentication, profile, discussion) showed that tests generated from product documentation had high compilation success and functional coverage, outperforming those based on requirement specs and user stories. These findings highlight the potential of product documentation to improve E2E test quality and, by extension, software quality.", "paper_url": "https://arxiv.org/abs/2503.17837", "authors": "Takuro Morimoto, Harumi Haraguchi", "first_author": "Takuro Morimoto", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-03-22", "update_time": "2025-03-22", "comments": "12 pages, 5 figures and 10 tables", "download_time": "2025-11-07 05:32:17", "query": "ti:A Study on the Improvement of Code Generation Quality Using Large Language Models Leveraging Product Documentation"}
{"paper_id": "2305.02176", "title": "Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity", "abstract": "Mixture-of-experts (MoE) models that employ sparse activation have demonstrated effectiveness in significantly increasing the number of parameters while maintaining low computational requirements per token. However, recent studies have established that MoE models are inherently parameter-inefficient as the improvement in performance diminishes with an increasing number of experts. We hypothesize this parameter inefficiency is a result of all experts having equal capacity, which may not adequately meet the varying complexity requirements of different tokens or tasks. In light of this, we propose Stratified Mixture of Experts (SMoE) models, which feature a stratified structure and can assign dynamic capacity to different tokens. We demonstrate the effectiveness of SMoE on three multilingual machine translation benchmarks, containing 4, 15, and 94 language pairs, respectively. We show that SMoE outperforms multiple state-of-the-art MoE models with the same or fewer parameters.", "paper_url": "https://arxiv.org/abs/2305.02176", "authors": "Haoran Xu, Maha Elbayad, Kenton Murray, Jean Maillard, Vedanuj Goswami", "first_author": "Haoran Xu", "primary_category": "cs.CL", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2023-05-03", "update_time": "2023-10-22", "comments": "Accepted at Findings of EMNLP 2023", "download_time": "2025-11-07 05:32:17", "query": "ti:Towards Being Parameter-Efficient A Stratified Sparsely Activated Transformer with Dynamic Capacity"}
{"paper_id": "2408.02479", "title": "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future", "abstract": "With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.", "paper_url": "https://arxiv.org/abs/2408.02479", "authors": "Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen", "first_author": "Haolin Jin", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2024-08-05", "update_time": "2025-04-13", "comments": "", "download_time": "2025-11-07 05:32:26", "query": "ti:From LLMs to LLM-based Agents for Software Engineering A Survey of Current, Challenges and Future"}
{"paper_id": "2501.11354", "title": "Towards Advancing Code Generation with Large Language Models: A Research Roadmap", "abstract": "Recently, we have witnessed the rapid development of large language models, which have demonstrated excellent capabilities in the downstream task of code generation. However, despite their potential, LLM-based code generation still faces numerous technical and evaluation challenges, particularly when embedded in real-world development. In this paper, we present our vision for current research directions, and provide an in-depth analysis of existing studies on this task. We propose a six-layer vision framework that categorizes code generation process into distinct phases, namely Input Phase, Orchestration Phase, Development Phase, and Validation Phase. Additionally, we outline our vision workflow, which reflects on the currently prevalent frameworks. We systematically analyse the challenges faced by large language models, including those LLM-based agent frameworks, in code generation tasks. With these, we offer various perspectives and actionable recommendations in this area. Our aim is to provide guidelines for improving the reliability, robustness and usability of LLM-based code generation systems. Ultimately, this work seeks to address persistent challenges and to provide practical suggestions for a more pragmatic LLM-based solution for future code generation endeavors.", "paper_url": "https://arxiv.org/abs/2501.11354", "authors": "Haolin Jin, Huaming Chen, Qinghua Lu, Liming Zhu", "first_author": "Haolin Jin", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-01-20", "update_time": "2025-01-20", "comments": "", "download_time": "2025-11-07 05:32:26", "query": "ti:Towards Advancing Code Generation with Large Language Models A Research Roadmap"}
{"paper_id": "2402.12219", "title": "Reformatted Alignment", "abstract": "The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical reasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy. Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment ability measured by the Alpaca dataset. This work highlights the need for further research into the science and mechanistic interpretability of LLMs. We have made the associated code and data publicly accessible to support future studies at https://github.com/GAIR-NLP/ReAlign.", "paper_url": "https://arxiv.org/abs/2402.12219", "authors": "Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, Pengfei Liu", "first_author": "Run-Ze Fan", "primary_category": "cs.CL", "topic": "LLM Coding", "pdf_path": "./papers/2402.12219.pdf", "publish_time": "2024-02-19", "update_time": "2024-04-17", "comments": "Homepage: https://gair-nlp.github.io/ReAlign/", "download_time": "2025-11-07 05:32:37", "query": "ti:Aligning Requirement for Large Language Models Code Generation"}
{"paper_id": "2503.01245", "title": "Large Language Models for Code Generation: A Comprehensive Survey of Challenges, Techniques, Evaluation, and Applications", "abstract": "Large Language Models (LLMs) have demonstrated their remarkable capabilities in numerous fields. This survey focuses on how LLMs empower users, regardless of their technical background, to use human languages to automatically generate executable code. We begin with understanding LLMs' limitations and challenges in automated code generation. Subsequently, we review various fine-tuning techniques designed to enhance both the performance and adaptability of LLMs in code generation tasks. We then review the existing metrics and benchmarks for evaluations to assess model performance based on fine-tuning techniques. Finally, we explore the applications of LLMs (e.g. CodeLlama, GitHub Copilot, ToolGen) in code generation tasks to illustrate their roles and functionalities. This survey provides a comprehensive overview of LLMs for code generation, helps researchers in diverse fields better understand the current state-of-the-art technologies, and offers the potential of effectively leveraging LLMs for code generation tasks.", "paper_url": "https://arxiv.org/abs/2503.01245", "authors": "Nam Huynh, Beiyu Lin", "first_author": "Nam Huynh", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-03-03", "update_time": "2025-04-02", "comments": "", "download_time": "2025-11-07 05:32:42", "query": "ti:Large Language Models for Code Generation A Comprehensive Survey of Challenges, Techniques, Evaluation, and Applications"}
{"paper_id": "2412.16620", "title": "A Large-scale Empirical Study on Fine-tuning Large Language Models for Unit Testing", "abstract": "Unit testing plays a pivotal role in software development, improving software quality and reliability. However, generating effective test cases manually is time-consuming, prompting interest in unit testing research. Recently, Large Language Models (LLMs) have shown potential in various unit testing tasks, including test generation, assertion generation, and test evolution, but existing studies are limited in scope and lack a systematic evaluation of the effectiveness of LLMs.   To bridge this gap, we present a large-scale empirical study on fine-tuning LLMs for unit testing. Our study involves three unit testing tasks, five benchmarks, eight evaluation metrics, and 37 popular LLMs across various architectures and sizes, consuming over 3,000 NVIDIA A100 GPU hours. We focus on three key research questions: (1) the performance of LLMs compared to state-of-the-art methods, (2) the impact of different factors on LLM performance, and (3) the effectiveness of fine-tuning versus prompt engineering. Our findings reveal that LLMs outperform existing state-of-the-art approaches on all three unit testing tasks across nearly all metrics, highlighting the potential of fine-tuning LLMs in unit testing tasks. Furthermore, large-scale, decoder-only models achieve the best results across tasks, while encoder-decoder models perform better under the same parameter scale. Additionally, the comparison of the performance between fine-tuning and prompt engineering approaches reveals the considerable potential capability of the prompt engineering approach in unit testing tasks. We then discuss the concerned issues on the test generation task, including data leakage issues, bug detection capabilities, and metrics comparisons. Finally, we further pinpoint carious practical guidelines for LLM-based approaches to unit testing tasks in the near future.", "paper_url": "https://arxiv.org/abs/2412.16620", "authors": "Ye Shang, Quanjun Zhang, Chunrong Fang, Siqi Gu, Jianyi Zhou, Zhenyu Chen", "first_author": "Ye Shang", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "./papers/2412.16620.pdf", "publish_time": "2024-12-21", "update_time": "2024-12-21", "comments": "Accepted to the ACM SIGSOFT International Symposium on Software\n  Testing and Analysis (ISSTA 2025)", "download_time": "2025-11-07 05:32:43", "query": "ti:On the Evaluation of Large Language Models in Unit Test Generation"}
{"paper_id": "2506.15227", "title": "Large Language Models for Unit Testing: A Systematic Literature Review", "abstract": "Unit testing is a fundamental practice in modern software engineering, with the aim of ensuring the correctness, maintainability, and reliability of individual software components. Very recently, with the advances in Large Language Models (LLMs), a rapidly growing body of research has leveraged LLMs to automate various unit testing tasks, demonstrating remarkable performance and significantly reducing manual effort. However, due to ongoing explorations in the LLM-based unit testing field, it is challenging for researchers to understand existing achievements, open challenges, and future opportunities. This paper presents the first systematic literature review on the application of LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant papers from the perspectives of both unit testing and LLMs. We first categorize existing unit testing tasks that benefit from LLMs, e.g., test generation and oracle generation. We then discuss several critical aspects of integrating LLMs into unit testing research, including model usage, adaptation strategies, and hybrid approaches. We further summarize key challenges that remain unresolved and outline promising directions to guide future research in this area. Overall, our paper provides a systematic overview of the research landscape to the unit testing community, helping researchers gain a comprehensive understanding of achievements and promote future research. Our artifacts are publicly available at the GitHub repository: https://github.com/iSEngLab/AwesomeLLM4UT.", "paper_url": "https://arxiv.org/abs/2506.15227", "authors": "Quanjun Zhang, Chunrong Fang, Siqi Gu, Ye Shang, Zhenyu Chen, Liang Xiao", "first_author": "Quanjun Zhang", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-06-18", "update_time": "2025-06-18", "comments": "", "download_time": "2025-11-07 05:32:51", "query": "ti:Large Language Models for Unit Testing A Systematic Literature Review"}
{"paper_id": "2410.09812", "title": "Unraveling the Potential of Large Language Models in Code Translation: How Far Are We?", "abstract": "While large language models (LLMs) exhibit state-of-the-art performance in various tasks, recent studies have revealed their struggle for code translation. This is because they haven't been extensively pre-trained with parallel multilingual code, which code translation heavily depends on. Moreover, existing benchmarks only cover a limited subset of common programming languages, and thus cannot reflect the full potential of LLMs in code translation. In this paper, we conduct a large-scale empirical study to exploit the capabilities and incapabilities of LLMs in code translation tasks. We first craft a novel benchmark called PolyHumanEval by extending HumanEval to a multilingual benchmark of 14 languages. With PolyHumanEval, we then perform over 110,000 translations with bleeding-edge code LLMs. The result shows LLMs' suboptimal performance on Python to other languages and the negligible impact of widely adopted LLM optimization techniques such as conventional pre-training and instruction tuning on code translation. To further uncover the potential of LLMs in code translation, we propose two methods: (1) intermediary translation which selects an intermediary language between the source and target ones; and (2) self-training which fine-tunes LLMs on self-generated parallel data. Evaluated with CodeLlama-13B, our approach yields an average improvement of 11.7% computation accuracy on Python-to-other translations. Notably, we interestingly find that Go can serve as a lingua franca for translating between any two studied languages.", "paper_url": "https://arxiv.org/abs/2410.09812", "authors": "Qingxiao Tao, Tingrui Yu, Xiaodong Gu, Beijun Shen", "first_author": "Qingxiao Tao", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2024-10-13", "update_time": "2024-10-13", "comments": "Accepted to APSEC 2024", "download_time": "2025-11-07 05:33:00", "query": "ti:Unraveling the Potential of Large Language Models in Code Translation How Far Are We?"}
{"paper_id": "2501.18460", "title": "ExeCoder: Empowering Large Language Models with Executability Representation for Code Translation", "abstract": "Code translation is a crucial activity in the software development and maintenance process, and researchers have recently begun to focus on using pre-trained large language models (LLMs) for code translation. However, existing LLMs only learn the contextual semantics of code during pre-training, neglecting executability information closely related to the execution state of the code, which results in unguaranteed code executability and unreliable automated code translation. To address this issue, we propose ExeCoder, an LLM specifically designed for code translation, aimed at utilizing executability representations such as functional semantics, syntax structures, and variable dependencies to enhance the capabilities of LLMs in code translation. To evaluate the effectiveness of ExeCoder, we manually enhanced the widely used benchmark TransCoder-test, resulting in a benchmark called TransCoder-test-X that serves LLMs. Evaluation of TransCoder-test-X indicates that ExeCoder achieves state-of-the-art performance in code translation, surpassing existing open-source code LLMs by over 10.88% to 38.78% and over 27.44% to 42.97% on two metrics, and even outperforms the renowned closed-source LLM GPT-4o. Code is available at https://aka.ms/execoder", "paper_url": "https://arxiv.org/abs/2501.18460", "authors": "Minghua He, Yue Chen, Fangkai Yang, Pu Zhao, Wenjie Yin, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang", "first_author": "Minghua He", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-01-30", "update_time": "2025-09-27", "comments": "EMNLP 2025 (Oral)", "download_time": "2025-11-07 05:33:05", "query": "ti:ExeCoder Empowering Large Language Models with Executability Representation for Code Translation"}
{"paper_id": "2410.15966", "title": "Self-Explained Keywords Empower Large Language Models for Code Generation", "abstract": "Large language models (LLMs) have achieved impressive performance in code generation. However, due to the long-tail distribution of LLMs' training data, low-frequency terms are typically underrepresented in the training process. Consequently, LLMs often misunderstand or overlook problem-specific, low-frequency keywords during code generation, compromising the accuracy of the generated code. To address this, we propose a novel technique named SEK(\\textbf{S}elf-\\textbf{E}xplained \\textbf{K}eywords), which empowers an LLM for better code generation by extracting and explaining the key terms in the problem description with the LLM itself and ranking them based on frequency. Comprehensive experiments across three benchmarks, i.e., HumanEval(+), MBPP(+), and APPS, with five representative LLMs, show that SEK can significantly improve LLMs in code generation, yielding substantial and consistent gains. For instance, SEK improves the Pass@1 of DeepSeek-Coder-V2-Instruct from 85.4\\% to 93.3\\% on the Humaneval benchmark. Further analysis confirms that SEK enables the LLMs to shift their attention from low-frequency keywords to their corresponding high-frequency counterparts.", "paper_url": "https://arxiv.org/abs/2410.15966", "authors": "Lishui Fan, Mouxiang Chen, Zhongxin Liu", "first_author": "Lishui Fan", "primary_category": "cs.CL", "topic": "LLM Coding", "pdf_path": "./papers/2410.15966.pdf", "publish_time": "2024-10-21", "update_time": "2024-10-21", "comments": "", "download_time": "2025-11-07 05:33:15", "query": "(llm OR \"large language model\") AND (\"code generation\")"}
{"paper_id": "2406.08731", "title": "Towards Understanding the Characteristics of Code Generation Errors Made by Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated unprecedented capabilities in code generation. However, there remains a limited understanding of code generation errors that LLMs can produce. To bridge the gap, we conducted an in-depth analysis of code generation errors across six representative LLMs on the HumanEval dataset. Specifically, we first employed open coding and thematic analysis to distill a comprehensive taxonomy of code generation errors. We analyzed two dimensions of error characteristics -- semantic characteristics and syntactic characteristics. Our analysis revealed that LLMs often made non-trivial, multi-line code generation errors in various locations and with various root causes. We further analyzed the correlation between these errors and task complexity as well as test pass rate. Our findings highlighted several challenges in locating and fixing code generation errors made by LLMs. In the end, we discussed several future directions to address these challenges.", "paper_url": "https://arxiv.org/abs/2406.08731", "authors": "Zhijie Wang, Zijie Zhou, Da Song, Yuheng Huang, Shengmai Chen, Lei Ma, Tianyi Zhang", "first_author": "Zhijie Wang", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "./papers/2406.08731.pdf", "publish_time": "2024-06-13", "update_time": "2025-02-12", "comments": "To appear in the 47th IEEE/ACM Conference on Software Engineering\n  (ICSE 2025). The first three authors contributed equally to this work", "download_time": "2025-11-07 05:33:15", "query": "(llm OR \"large language model\") AND (\"code generation\")"}
{"paper_id": "2507.17271", "title": "Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation", "abstract": "Unit tests play a vital role in the software development lifecycle. Recent advances in Large Language Model (LLM)-based approaches have significantly improved automated test generation, garnering attention from both academia and industry. We revisit LLM-based unit test generation from a novel perspective by decoupling prefix generation and assertion generation. To characterize their respective challenges, we define Initialization Complexity and adopt Cyclomatic Complexity to measure the difficulty of prefix and assertion generation, revealing that the former primarily affects compilation success, while the latter influences test coverage. To address these challenges, we propose Seed&Steer, a two-step approach that combines traditional unit testing techniques with the capabilities of large language models. Seed&Steer leverages conventional unit testing tools (e.g., EvoSuite) to generate method invocations with high compilation success rates, which serve as seeds to guide LLMs in constructing effective test contexts. It then introduces branching cues to help LLMs explore diverse execution paths (e.g., normal, boundary, and exception cases) and generate assertions with high coverage. We evaluate Seed&Steer on five real-world Java projects against state-of-the-art baselines. Results show that Seed&Steer improves the compilation pass rate by approximately 7%, successfully compiling 792 and 887 previously failing cases on two LLMs. It also achieves up to ~73% branch and line coverage across focal methods of varying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our code, dataset, and experimental scripts will be publicly released to support future research and reproducibility.", "paper_url": "https://arxiv.org/abs/2507.17271", "authors": "Shuaiyu Zhou, Zhengran Zeng, Xiaoling Zhou, Rui Xie, Shikun Zhang, Wei Ye", "first_author": "Shuaiyu Zhou", "primary_category": "cs.SE", "topic": "LLM Testing", "pdf_path": "./papers/2507.17271.pdf", "publish_time": "2025-07-23", "update_time": "2025-07-23", "comments": "", "download_time": "2025-11-07 05:33:16", "query": "(llm OR \"large language model\") AND (\"unit test\" OR \"test generation\")"}
{"paper_id": "2501.16155", "title": "CITYWALK: Enhancing LLM-Based C++ Unit Test Generation via Project-Dependency Awareness and Language-Specific Knowledge", "abstract": "Unit testing plays a pivotal role in the software development lifecycle, as it ensures code quality. However, writing high-quality unit tests remains a time-consuming task for developers in practice. More recently, the application of large language models (LLMs) in automated unit test generation has demonstrated promising results. Existing approaches primarily focus on interpreted programming languages (e.g., Java), while mature solutions tailored to compiled programming languages like C++ are yet to be explored. The intricate language features of C++, such as pointers, templates, and virtual functions, pose particular challenges for LLMs in generating both executable and high-coverage unit tests. To tackle the aforementioned problems, this paper introduces CITYWALK, a novel LLM-based framework for C++ unit test generation. CITYWALK enhances LLMs by providing a comprehensive understanding of the dependency relationships within the project under test via program analysis. Furthermore, CITYWALK incorporates language-specific knowledge about C++ derived from project documentation and empirical observations, significantly improving the correctness of the LLM-generated unit tests. We implement CITYWALK by employing the widely popular LLM GPT-4o. The experimental results show that CITYWALK outperforms current state-of-the-art approaches on a collection of ten popular C++ projects. Our findings demonstrate the effectiveness of CITYWALK in generating high-quality C++ unit tests.", "paper_url": "https://arxiv.org/abs/2501.16155", "authors": "Yuwei Zhang, Qingyuan Lu, Kai Liu, Wensheng Dou, Jiaxin Zhu, Li Qian, Chunxi Zhang, Zheng Lin, Jun Wei", "first_author": "Yuwei Zhang", "primary_category": "cs.SE", "topic": "LLM Testing", "pdf_path": "./papers/2501.16155.pdf", "publish_time": "2025-01-27", "update_time": "2025-08-11", "comments": "Preprint, to appear in the ACM Transactions on Software Engineering\n  and Methodology (TOSEM)", "download_time": "2025-11-07 05:33:17", "query": "(llm OR \"large language model\") AND (\"unit test\" OR \"test generation\")"}
{"paper_id": "2412.04590", "title": "Specification-Driven Code Translation Powered by Large Language Models: How Far Are We?", "abstract": "Large Language Models (LLMs) are increasingly being applied across various domains, including code-related tasks such as code translation. Previous studies have explored using LLMs for translating code between different programming languages. Since LLMs are more effective with natural language, using natural language as an intermediate representation in code translation tasks presents a promising approach. In this work, we investigate using NL-specification as an intermediate representation for code translation. We evaluate our method using three datasets, five popular programming languages, and 29 language pair permutations. Our results show that using NL-specification alone does not lead to performance improvements. However, when combined with source code, it provides a slight improvement over the baseline in certain language pairs. Besides analyzing the performance of code translation, we also investigate the quality of the translated code and provide insights into the issues present in the translated code.", "paper_url": "https://arxiv.org/abs/2412.04590", "authors": "Soumit Kanti Saha, Fazle Rabbi, Song Wang, Jinqiu Yang", "first_author": "Soumit Kanti Saha", "primary_category": "cs.SE", "topic": "LLM Translation", "pdf_path": "./papers/2412.04590.pdf", "publish_time": "2024-12-05", "update_time": "2024-12-05", "comments": "", "download_time": "2025-11-07 05:33:17", "query": "(llm OR \"large language model\") AND (\"code translation\")"}
{"paper_id": "2308.03109", "title": "Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code", "abstract": "Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation -- with correct translations ranging from 2.1% to 47.3% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset -- consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,748 manually labeled bugs, and 1,365 bug-fix pairs -- can help drive research in this area.", "paper_url": "https://arxiv.org/abs/2308.03109", "authors": "Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, Reyhaneh Jabbarvand", "first_author": "Rangeet Pan", "primary_category": "cs.SE", "topic": "LLM Translation", "pdf_path": "./papers/2308.03109.pdf", "publish_time": "2023-08-06", "update_time": "2024-01-16", "comments": "Published in ICSE 2024", "download_time": "2025-11-07 05:33:18", "query": "(llm OR \"large language model\") AND (\"code translation\")"}
{"paper_id": "2503.15129", "title": "Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models", "abstract": "This paper studies how AI-assisted programming and large language models (LLM) improve software developers' ability via AI tools (LLM agents) like Github Copilot and Amazon CodeWhisperer, while integrating human feedback to enhance reinforcement learning (RLHF) with crowd-sourced computation to enhance text-to-code generation. Additionally, we demonstrate that our Bayesian optimization framework supports AI alignment in code generation by distributing the feedback collection burden, highlighting the value of collecting human feedback of good quality. Our empirical evaluations demonstrate the efficacy of this approach, showcasing how LLM agents can be effectively trained for improved text-to-code generation. Our Bayesian optimization framework can be designed for general domain-specific languages, promoting the alignment of large language model capabilities with human feedback in AI-assisted programming for code generation.", "paper_url": "https://arxiv.org/abs/2503.15129", "authors": "Man Fai Wong, Chee Wei Tan", "first_author": "Man Fai Wong", "primary_category": "cs.AI", "topic": "LLM Coding", "pdf_path": "./papers/2503.15129.pdf", "publish_time": "2025-03-19", "update_time": "2025-03-19", "comments": "", "download_time": "2025-11-08 01:11:35", "query": "(llm OR \"large language model\") AND (\"code generation\")"}
{"paper_id": "2511.00087", "title": "Adding New Capability in Existing Scientific Application with LLM Assistance", "abstract": "With the emergence and rapid evolution of large language models (LLM), automating coding tasks has become an important research topic. Many efforts are underway and literature abounds about the efficacy of models and their ability to generate code. A less explored aspect of code generation is for new algorithms, where the training dataset would not have included any previous example of similar code. In this paper we propose a new methodology for writing code from scratch for a new algorithm using LLM assistance, and describe enhancement of a previously developed code-translation tool, Code-Scribe, for new code generation.", "paper_url": "https://arxiv.org/abs/2511.00087", "authors": "Anshu Dubey, Akash Dhruv", "first_author": "Anshu Dubey", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "./papers/2511.00087.pdf", "publish_time": "2025-10-30", "update_time": "2025-10-30", "comments": "8 pages, 4 figures, submitted to The 1st International Workshop on\n  Foundational large Language Models Advances for HPC in Asia", "download_time": "2025-11-08 01:11:35", "query": "(llm OR \"large language model\") AND (\"code generation\")"}
{"paper_id": "2410.13542", "title": "LLM-based Unit Test Generation via Property Retrieval", "abstract": "Automated unit test generation has been widely studied, with Large Language Models (LLMs) recently showing significant potential. Moreover, in the context of unit test generation, these tools prioritize high code coverage, often at the expense of practical usability, correctness, and maintainability. In response, we propose Property-Based Retrieval Augmentation, a novel mechanism that extends LLM-based Retrieval-Augmented Generation (RAG) beyond basic vector, text similarity, and graph-based methods. Our approach considers task-specific context and introduces a tailored property retrieval mechanism. Specifically, in the unit test generation task, we account for the unique structure of unit tests by dividing the test generation process into Given, When, and Then phases. When generating tests for a focal method, we not only retrieve general context for the code under test but also consider task-specific context such as pre-existing tests of other methods, which can provide valuable insights for any of the Given, When, and Then phases. This forms property relationships between focal method and other methods, thereby expanding the scope of retrieval beyond traditional RAG. We implement this approach in a tool called APT, which sequentially performs preprocessing, property retrieval, and unit test generation, using an iterative strategy where newly generated tests guide the creation of subsequent ones. We evaluated APT on 12 open-source projects with 1515 methods, and the results demonstrate that APT consistently outperforms existing tools in terms of correctness, completeness, and maintainability of the generated tests. Moreover, we introduce a novel code-context-aware retrieval mechanism for LLMs beyond general context, offering valuable insights and potential applications for other code-related tasks.", "paper_url": "https://arxiv.org/abs/2410.13542", "authors": "Zhe Zhang, Xingyu Liu, Yuanzhang Lin, Xiang Gao, Hailong Sun, Yuan Yuan", "first_author": "Zhe Zhang", "primary_category": "cs.SE", "topic": "LLM Testing", "pdf_path": "./papers/2410.13542.pdf", "publish_time": "2024-10-17", "update_time": "2024-10-17", "comments": "", "download_time": "2025-11-08 01:11:37", "query": "(llm OR \"large language model\") AND (\"unit test\" OR \"test generation\")"}
{"paper_id": "2410.09414", "title": "Advancing Bug Detection in Fastjson2 with Large Language Models Driven Unit Test Generation", "abstract": "Data-serialization libraries are essential tools in software development, responsible for converting between programmable data structures and data persistence formats. Among them, JSON is the most popular choice for exchanging data between different systems and programming languages, while JSON libraries serve as the programming toolkit for this task. Despite their widespread use, bugs in JSON libraries can cause severe issues such as data inconsistencies and security vulnerabilities. Unit test generation techniques are widely adopted to identify bugs in various libraries. However, there is limited systematic testing effort specifically for exposing bugs within JSON libraries in industrial practice. In this paper, we propose JSONTestGen, an approach leveraging large language models (LLMs) to generate unit tests for fastjson2, a popular open source JSON library from Alibaba. Pre-trained on billions of open-source text and code corpora, LLMs have demonstrated remarkable abilities in programming tasks. Based on historical bug-triggering unit tests, we utilize LLMs to generate more diverse test cases by incorporating JSON domain-specific mutation rules. To systematically and efficiently identify potential bugs, we adopt differential testing on the results of the generated unit tests. Our evaluation shows that JSONTestGen outperforms existing test generation tools in unknown defect detection. With JSONTestGen, we found 34 real bugs in fastjson2, 30 of which have already been fixed, including 12 non-crashing bugs. While manual inspection reveals that LLM-generated tests can be erroneous, particularly with self-contradictory assertions, we demonstrate that LLMs have the potential for classifying false-positive test failures. This suggests a promising direction for improved test oracle automation in the future.", "paper_url": "https://arxiv.org/abs/2410.09414", "authors": "Zhiyuan Zhong, Sinan Wang, Hailong Wang, Shaojin Wen, Hao Guan, Yida Tao, Yepang Liu", "first_author": "Zhiyuan Zhong", "primary_category": "cs.SE", "topic": "LLM Testing", "pdf_path": "./papers/2410.09414.pdf", "publish_time": "2024-10-12", "update_time": "2024-10-12", "comments": "", "download_time": "2025-11-08 01:11:37", "query": "(llm OR \"large language model\") AND (\"unit test\" OR \"test generation\")"}
{"paper_id": "2508.17720", "title": "RepoTransAgent: Multi-Agent LLM Framework for Repository-Aware Code Translation", "abstract": "Repository-aware code translation is critical for modernizing legacy systems, enhancing maintainability, and enabling interoperability across diverse programming languages. While recent advances in large language models (LLMs) have improved code translation quality, existing approaches face significant challenges in practical scenarios: insufficient contextual understanding, inflexible prompt designs, and inadequate error correction mechanisms. These limitations severely hinder accurate and efficient translation of complex, real-world code repositories. To address these challenges, we propose RepoTransAgent, a novel multi-agent LLM framework for repository-aware code translation. RepoTransAgent systematically decomposes the translation process into specialized subtasks-context retrieval, dynamic prompt construction, and iterative code refinement-each handled by dedicated agents. Our approach leverages retrieval-augmented generation (RAG) for contextual information gathering, employs adaptive prompts tailored to varying repository scenarios, and introduces a reflection-based mechanism for systematic error correction. We evaluate RepoTransAgent on hundreds of Java-C# translation pairs from six popular open-source projects. Experimental results demonstrate that RepoTransAgent significantly outperforms state-of-the-art baselines in both compile and pass rates. Specifically, RepoTransAgent achieves up to 55.34% compile rate and 45.84% pass rate. Comprehensive analysis confirms the robustness and generalizability of RepoTransAgent across different LLMs, establishing its effectiveness for real-world repository-aware code translation.", "paper_url": "https://arxiv.org/abs/2508.17720", "authors": "Ziqi Guan, Xin Yin, Zhiyuan Peng, Chao Ni", "first_author": "Ziqi Guan", "primary_category": "cs.SE", "topic": "LLM Translation", "pdf_path": "./papers/2508.17720.pdf", "publish_time": "2025-08-25", "update_time": "2025-08-25", "comments": "", "download_time": "2025-11-08 01:11:38", "query": "(llm OR \"large language model\") AND (\"code translation\")"}
{"paper_id": "2503.18305", "title": "Enhancing LLM-based Code Translation in Repository Context via Triple Knowledge-Augmented", "abstract": "Large language models (LLMs) have behaved well in function-level code translation without repository-level context. However, the performance of LLMs in repository-level context code translation remains suboptimal due to complex dependencies and context, hindering their adoption in industrial settings. In this work, we propose a novel LLM-based code translation technique K-Trans, which leverages triple knowledge augmentation to enhance LLM's translation quality under repository context in real-world software development. First, K-Trans constructs a translation knowledge base by extracting relevant information from target-language codebases, the repository being translated, and prior translation results. Second, for each function to be translated, K-Trans retrieves relevant triple knowledge, including target-language code samples, dependency usage examples, and successful translation function pairs, serving as references to enhance LLM for translation. Third, K-Trans constructs a knowledge-augmented translation prompt using the retrieved triple knowledge and employs LLMs to generate the translated code while preserving repository context. It further leverages LLMs for self-debugging, enhancing translation correctness.   The experiments show that K-Trans substantially outperforms the baseline adapted from previous work by 19.4%/40.2% relative improvement in pass@1 and 0.138 in CodeBLEU. It is important to note that the results also demonstrate that each knowledge significantly contributes to K-Trans's effectiveness in handling repository-level context code translation, with dependency usage examples making the most notable contribution. Moreover, as the self-evolution process progresses, the knowledge base continuously enhances the LLM's performance across various aspects of the repository-level code translation.", "paper_url": "https://arxiv.org/abs/2503.18305", "authors": "Guangsheng Ou, Mingwei Liu, Yuxuan Chen, Xueying Du, Shengbo Wang, Zekai Zhang, Xin Peng, Zibin Zheng", "first_author": "Guangsheng Ou", "primary_category": "cs.SE", "topic": "LLM Translation", "pdf_path": "./papers/2503.18305.pdf", "publish_time": "2025-03-24", "update_time": "2025-03-27", "comments": "", "download_time": "2025-11-08 01:11:40", "query": "(llm OR \"large language model\") AND (\"code translation\")"}
{"paper_id": "2503.22688", "title": "CodeIF-Bench: Evaluating Instruction-Following Capabilities of Large Language Models in Interactive Code Generation", "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance in code generation tasks and have become indispensable programming assistants for developers. However, existing code generation benchmarks primarily assess the functional correctness of code generated by LLMs in single-turn interactions. They offer limited insight into LLMs' abilities to generate code that strictly follows users' instructions in multi-turn interaction scenarios. In this paper, we introduce CodeIF-Bench, a benchmark for evaluating the instruction-following capabilities of LLMs in interactive code generation. Specifically, CodeIF-Bench incorporates nine types of verifiable instructions aligned with the real-world software development requirements, which can be independently and objectively validated through specified test cases, facilitating the evaluation of instruction-following capability in multi-turn interactions. In both \\textit{Static Conversation} and \\textit{Dynamic Conversation} settings, we evaluate the performance of 7 state-of-the-art LLMs and summarize the important factors influencing the instruction-following ability of LLMs in multi-turn interactions, as well as potential directions for improvement.", "paper_url": "https://arxiv.org/abs/2503.22688", "authors": "Peiding Wang, Li Zhang, Fang Liu, Lin Shi, Minxiao Li, Bo Shen, An Fu", "first_author": "Peiding Wang", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "./papers/2503.22688.pdf", "publish_time": "2025-03-05", "update_time": "2025-07-31", "comments": "", "download_time": "2025-11-09 01:18:48", "query": "(llm OR \"large language model\") AND (\"code generation\")"}
{"paper_id": "2404.03114", "title": "Testing the Effect of Code Documentation on Large Language Model Code Understanding", "abstract": "Large Language Models (LLMs) have demonstrated impressive abilities in recent years with regards to code generation and understanding. However, little work has investigated how documentation and other code properties affect an LLM's ability to understand and generate code or documentation. We present an empirical analysis of how underlying properties of code or documentation can affect an LLM's capabilities. We show that providing an LLM with \"incorrect\" documentation can greatly hinder code understanding, while incomplete or missing documentation does not seem to significantly affect an LLM's ability to understand code.", "paper_url": "https://arxiv.org/abs/2404.03114", "authors": "William Macke, Michael Doyle", "first_author": "William Macke", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "./papers/2404.03114.pdf", "publish_time": "2024-04-03", "update_time": "2024-04-03", "comments": "7 pages, 5 figures, 2 tables. Accepted as a Findings paper in the\n  \"Generation\" track to NAACL 2024. MITRE Public Release Case Number 23-4132", "download_time": "2025-11-09 01:18:49", "query": "(llm OR \"large language model\") AND (\"code generation\")"}
{"paper_id": "2501.07425", "title": "Enhancing LLM's Ability to Generate More Repository-Aware Unit Tests Through Precise Contextual Information Injection", "abstract": "Though many learning-based approaches have been proposed for unit test generation and achieved remarkable performance, they still have limitations in relying on task-specific datasets. Recently, Large Language Models (LLMs) guided by prompt engineering have gained attention for their ability to handle a broad range of tasks, including unit test generation. Despite their success, LLMs may exhibit hallucinations when generating unit tests for focal methods or functions due to their lack of awareness regarding the project's global context. These hallucinations may manifest as calls to non-existent methods, as well as incorrect parameters or return values, such as mismatched parameter types or numbers. While many studies have explored the role of context, they often extract fixed patterns of context for different models and focal methods, which may not be suitable for all generation processes (e.g., excessive irrelevant context could lead to redundancy, preventing the model from focusing on essential information). To overcome this limitation, we propose RATester, which enhances the LLM's ability to generate more repository-aware unit tests through global contextual information injection. To equip LLMs with global knowledge similar to that of human testers, we integrate the language server gopls, which provides essential features (e.g., definition lookup) to assist the LLM. When RATester encounters an unfamiliar identifier (e.g., an unfamiliar struct name), it first leverages gopls to fetch relevant definitions and documentation comments, and then uses this global knowledge to guide the LLM. By utilizing gopls, RATester enriches the LLM's knowledge of the project's global context, thereby reducing hallucinations during unit test generation.", "paper_url": "https://arxiv.org/abs/2501.07425", "authors": "Xin Yin, Chao Ni, Xinrui Li, Liushan Chen, Guojun Ma, Xiaohu Yang", "first_author": "Xin Yin", "primary_category": "cs.SE", "topic": "LLM Testing", "pdf_path": "./papers/2501.07425.pdf", "publish_time": "2025-01-13", "update_time": "2025-01-13", "comments": "", "download_time": "2025-11-09 01:18:50", "query": "(llm OR \"large language model\") AND (\"unit test\" OR \"test generation\")"}
{"paper_id": "2407.05202", "title": "Harnessing the Power of LLMs: Automating Unit Test Generation for High-Performance Computing", "abstract": "Unit testing is crucial in software engineering for ensuring quality. However, it's not widely used in parallel and high-performance computing software, particularly scientific applications, due to their smaller, diverse user base and complex logic. These factors make unit testing challenging and expensive, as it requires specialized knowledge and existing automated tools are often ineffective.   To address this, we propose an automated method for generating unit tests for such software, considering their unique features like complex logic and parallel processing. Recently, large language models (LLMs) have shown promise in coding and testing. We explored the capabilities of Davinci (text-davinci-002) and ChatGPT (gpt-3.5-turbo) in creating unit tests for C++ parallel programs. Our results show that LLMs can generate mostly correct and comprehensive unit tests, although they have some limitations, such as repetitive assertions and blank test cases.", "paper_url": "https://arxiv.org/abs/2407.05202", "authors": "Rabimba Karanjai, Aftab Hussain, Md Rafiqul Islam Rabin, Lei Xu, Weidong Shi, Mohammad Amin Alipour", "first_author": "Rabimba Karanjai", "primary_category": "cs.SE", "topic": "LLM Testing", "pdf_path": "./papers/2407.05202.pdf", "publish_time": "2024-07-06", "update_time": "2024-07-06", "comments": "", "download_time": "2025-11-09 01:18:50", "query": "(llm OR \"large language model\") AND (\"unit test\" OR \"test generation\")"}
{"paper_id": "2504.02017", "title": "Enhancing LLMs in Long Code Translation through Instrumentation and Program State Alignment", "abstract": "Code translation aims to transform code between programming languages while preserving functionality, with applications in cross-platform development and software migration. Recent advances in Large Language Models (LLMs) have improved code translation, but challenges remain, particularly in inferring program functionality. These issues worsen with longer and more complex code, where current LLMs struggle to handle length and intricate semantics. To evaluate LLMs on long code translation, we introduce LongTrans, a large-scale execution-based benchmark with C++, Java, and Python programs, ranging from hundreds to thousands of tokens. Our empirical study of 12 LLMs reveals a sharp performance decline as code length increases, with even the best-performing model, GPT-4o, achieving only 57.51% computational accuracy. This highlights the need for further research in long code translation. We argue that code translation should maintain invariant functionality while transforming syntax and keywords across languages. Despite differences in appearance, program states should remain consistent throughout execution. To address this, we propose PAST (Program State Alignment augmented Translation), which integrates instrumentation to capture and align program states during translation. This approach is the first to leverage LLMs to insert instrumentation in both original and translated code, tracing program states at runtime. By prompting the LLM to correct errors based on output traces, we mitigate inconsistencies and enhance translation accuracy. Experimental results show significant improvements, with computational accuracy rising from 57.51% to 84.70% for GPT-4o, 50.68% to 69.97% for Mistral-Large-2, and 52.45% to 76.43% for DeepSeek-Coder-V2. These improvements are consistent across models and datasets, with ablation studies confirming the benefits of instrumentation and state alignment.", "paper_url": "https://arxiv.org/abs/2504.02017", "authors": "Li Xin-Ye, Du Ya-Li, Li Ming", "first_author": "Li Xin-Ye", "primary_category": "cs.SE", "topic": "LLM Translation", "pdf_path": "./papers/2504.02017.pdf", "publish_time": "2025-04-02", "update_time": "2025-04-02", "comments": "20 pages", "download_time": "2025-11-09 01:18:52", "query": "(llm OR \"large language model\") AND (\"code translation\")"}
{"paper_id": "2508.11468", "title": "TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation", "abstract": "Automatic code translation is a fundamental task in modern software development. While the advent of Large Language Models (LLMs) has significantly improved the correctness of code translation, the critical dimension of execution efficiency remains overlooked. To address this gap, we introduce TRACY, the first comprehensive benchmark designed to evaluate the execution efficiency of LLM-translated code. TRACY is constructed through an LLM-driven two-stage pipeline: an initial stage generates a suite of stress tests to amplify performance differences, followed by an efficiency-oriented task pruning stage that isolates the efficiency-distinguishing tasks. The resulting benchmark comprises 1,011 code translation tasks across C++, Java, and Python, each accompanied by an average of 22.1 verified reference translations and 10 computationally demanding tests. Our extensive evaluation of 26 representative LLMs reveals that even top-tier LLMs struggle to consistently produce efficient code translations. For instance, Claude-4-think, the leading model for correctness, ranks eighth overall when time efficiency is taken into account, surpassed by several smaller open-source models. We further pinpoint that algorithmic flaws and improper resource handling are the most detrimental, causing a median time slowdown of 5.6$\\times$ and memory increase of 12.0$\\times$, respectively. Our work underscores the necessity of jointly optimizing for correctness and efficiency in future LLM-based code translation.", "paper_url": "https://arxiv.org/abs/2508.11468", "authors": "Zhihao Gong, Zeyu Sun, Dong Huang, Qingyuan Liang, Jie M. Zhang, Dan Hao", "first_author": "Zhihao Gong", "primary_category": "cs.SE", "topic": "LLM Translation", "pdf_path": "./papers/2508.11468.pdf", "publish_time": "2025-08-15", "update_time": "2025-08-15", "comments": "", "download_time": "2025-11-09 01:18:52", "query": "(llm OR \"large language model\") AND (\"code translation\")"}
