{"paper_id": "2508.21107", "title": "Learning to Generate Unit Test via Adversarial Reinforcement Learning", "abstract": "Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate test generation, yet methods for training LLMs to produce high-quality tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning framework that trains an LLM to generate high-quality unit tests given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via reinforcement learning. The unit test generator is trained to maximize a discrimination reward, which reflects its ability to produce tests that expose faults in the code generator's solutions, and the code generator is trained to maximize a code reward, which reflects its ability to produce solutions that pass the unit tests generated by the test generator. In our experiments, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on human-written ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models such as GPT-4.1 in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for this task.", "paper_url": "https://arxiv.org/abs/2508.21107", "authors": "Dongjun Lee, Changho Hwang, Kimin Lee", "first_author": "Dongjun Lee", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-08-28", "update_time": "2025-09-30", "comments": "Code is available at: https://github.com/dgjun32/UTRL", "download_time": "2025-11-07 01:15:20", "query": "ti:Learning to Generate Unit Test via Adversarial Reinforcement Learning"}
{"paper_id": "2311.00272", "title": "ChatCoder: Chat-based Refine Requirement Improves LLMs' Code Generation", "abstract": "Large language models have shown good performances in generating code to meet human requirements. However, human requirements expressed in natural languages can be vague, incomplete, and ambiguous, leading large language models to misunderstand human requirements and make mistakes. Worse, it is difficult for a human user to refine the requirement. To help human users refine their requirements and improve large language models' code generation performances, we propose ChatCoder: a method to refine the requirements via chatting with large language models. We design a chat scheme in which the large language models will guide the human users to refine their expression of requirements to be more precise, unambiguous, and complete than before. Experiments show that ChatCoder has improved existing large language models' performance by a large margin. Besides, ChatCoder has the advantage over refine-based methods and LLMs fine-tuned via human response.", "paper_url": "https://arxiv.org/abs/2311.00272", "authors": "Zejun Wang, Jia Li, Ge Li, Zhi Jin", "first_author": "Zejun Wang", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2023-11-01", "update_time": "2023-11-01", "comments": "", "download_time": "2025-11-07 01:15:21", "query": "ti:ChatCoder Chat-based Refine Requirement Improves LLMs Code Generation"}
{"paper_id": "2503.17837", "title": "A Study on the Improvement of Code Generation Quality Using Large Language Models Leveraging Product Documentation", "abstract": "Research on using Large Language Models (LLMs) in system development is expanding, especially in automated code and test generation. While E2E testing is vital for ensuring application quality, most test generation research has focused on unit tests, with limited work on E2E test code. This study proposes a method for automatically generating E2E test code from product documentation such as manuals, FAQs, and tutorials using LLMs with tailored prompts. The two step process interprets documentation intent and produces executable test code. Experiments on a web app with six key features (e.g., authentication, profile, discussion) showed that tests generated from product documentation had high compilation success and functional coverage, outperforming those based on requirement specs and user stories. These findings highlight the potential of product documentation to improve E2E test quality and, by extension, software quality.", "paper_url": "https://arxiv.org/abs/2503.17837", "authors": "Takuro Morimoto, Harumi Haraguchi", "first_author": "Takuro Morimoto", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "", "publish_time": "2025-03-22", "update_time": "2025-03-22", "comments": "12 pages, 5 figures and 10 tables", "download_time": "2025-11-07 01:15:22", "query": "ti:A Study on the Improvement of Code Generation Quality Using Large Language Models Leveraging Product Documentation"}
{"paper_id": "2305.02176", "title": "Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity", "abstract": "Mixture-of-experts (MoE) models that employ sparse activation have demonstrated effectiveness in significantly increasing the number of parameters while maintaining low computational requirements per token. However, recent studies have established that MoE models are inherently parameter-inefficient as the improvement in performance diminishes with an increasing number of experts. We hypothesize this parameter inefficiency is a result of all experts having equal capacity, which may not adequately meet the varying complexity requirements of different tokens or tasks. In light of this, we propose Stratified Mixture of Experts (SMoE) models, which feature a stratified structure and can assign dynamic capacity to different tokens. We demonstrate the effectiveness of SMoE on three multilingual machine translation benchmarks, containing 4, 15, and 94 language pairs, respectively. We show that SMoE outperforms multiple state-of-the-art MoE models with the same or fewer parameters.", "paper_url": "https://arxiv.org/abs/2305.02176", "authors": "Haoran Xu, Maha Elbayad, Kenton Murray, Jean Maillard, Vedanuj Goswami", "first_author": "Haoran Xu", "primary_category": "cs.CL", "topic": "LLM Coding", "pdf_path": "./papers/2305.02176.pdf", "publish_time": "2023-05-03", "update_time": "2023-10-22", "comments": "Accepted at Findings of EMNLP 2023", "download_time": "2025-11-07 01:15:23", "query": "ti:Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation An Empirical Study"}
{"paper_id": "2408.02479", "title": "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future", "abstract": "With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.", "paper_url": "https://arxiv.org/abs/2408.02479", "authors": "Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, Huaming Chen", "first_author": "Haolin Jin", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "./papers/2408.02479.pdf", "publish_time": "2024-08-05", "update_time": "2025-04-13", "comments": "", "download_time": "2025-11-07 01:15:28", "query": "(llm OR \"large language model\") AND (\"code generation\" OR \"test generation\")"}
{"paper_id": "2501.11354", "title": "Towards Advancing Code Generation with Large Language Models: A Research Roadmap", "abstract": "Recently, we have witnessed the rapid development of large language models, which have demonstrated excellent capabilities in the downstream task of code generation. However, despite their potential, LLM-based code generation still faces numerous technical and evaluation challenges, particularly when embedded in real-world development. In this paper, we present our vision for current research directions, and provide an in-depth analysis of existing studies on this task. We propose a six-layer vision framework that categorizes code generation process into distinct phases, namely Input Phase, Orchestration Phase, Development Phase, and Validation Phase. Additionally, we outline our vision workflow, which reflects on the currently prevalent frameworks. We systematically analyse the challenges faced by large language models, including those LLM-based agent frameworks, in code generation tasks. With these, we offer various perspectives and actionable recommendations in this area. Our aim is to provide guidelines for improving the reliability, robustness and usability of LLM-based code generation systems. Ultimately, this work seeks to address persistent challenges and to provide practical suggestions for a more pragmatic LLM-based solution for future code generation endeavors.", "paper_url": "https://arxiv.org/abs/2501.11354", "authors": "Haolin Jin, Huaming Chen, Qinghua Lu, Liming Zhu", "first_author": "Haolin Jin", "primary_category": "cs.SE", "topic": "LLM Coding", "pdf_path": "./papers/2501.11354.pdf", "publish_time": "2025-01-20", "update_time": "2025-01-20", "comments": "", "download_time": "2025-11-07 01:15:28", "query": "(llm OR \"large language model\") AND (\"code generation\" OR \"test generation\")"}
